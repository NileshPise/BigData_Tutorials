{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "163bebe0",
   "metadata": {},
   "source": [
    "#### pyspark.sql.SparkSession.builder.appName\n",
    "Sets a name for the application, which will be shown in the Spark web UI.\n",
    "If no application name is set, a randomly generated name will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da5e8396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "sparkSession = SparkSession.builder.appName('theAppName')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2be1ba",
   "metadata": {},
   "source": [
    "#### pyspark.sql.SparkSession.builder.config\n",
    "Sets a config option. Options set using this method are automatically propagated to both SparkConf and SparkSession’s own configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0c8f9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b6619ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession.Builder at 0x7fc7a02ecc10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkSession.config(conf=SparkConf().setMaster('yarn').setAppName('theNewAppName').set('spark.executor.cores',2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461587a0",
   "metadata": {},
   "source": [
    "#### pyspark.sql.SparkSession.builder.enableHiveSupport¶\n",
    "Enables Hive support, including connectivity to a persistent Hive metastore, support for Hive SerDes, and Hive user-defined functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc50dfbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession.Builder at 0x7fc7a02ecc10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkSession.enableHiveSupport()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e152e9f5",
   "metadata": {},
   "source": [
    "#### pyspark.sql.SparkSession.builder.getOrCreate\n",
    "Gets an existing SparkSession or, if there is no existing one, creates a new one based on the options set in this builder.\n",
    "\n",
    "This method first checks whether there is a valid global default SparkSession, and if yes, return that one. If no valid global default SparkSession exists, the method creates a new SparkSession and assigns the newly created SparkSession as the global default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb5af99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://audacious:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>theNewAppName</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc7827bef20>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkSession.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c781c7",
   "metadata": {},
   "source": [
    "#### pyspark.sql.SparkSession.builder.master¶\n",
    "Sets the Spark master URL to connect to, such as “local” to run locally, “local[4]” to run locally with 4 cores, or “spark://master:7077” to run on a Spark standalone cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed7b914f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparkSession.master('yarn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bc4b52",
   "metadata": {},
   "source": [
    "#### pyspark.sql.SparkSession.createDataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "413a3aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/12/28 11:43:47 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "sparkSession = SparkSession.builder.config(conf=SparkConf() \\\n",
    "                                          .setMaster('yarn') \\\n",
    "                                          .setAppName('test')).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a83d1335",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|   _1| _2|\n",
      "+-----+---+\n",
      "|alice|  1|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sparkSession.createDataFrame([('alice', 1)])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5421e855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|alice|  1|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sparkSession.createDataFrame([('alice', 1)],['name', 'age'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7840c12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('alice', 18), ('Bob', 24)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sparkSession.sparkContext.parallelize([('alice',18), ('Bob', 24)])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59505e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|   _1| _2|\n",
      "+-----+---+\n",
      "|alice| 18|\n",
      "|  Bob| 24|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sparkSession.createDataFrame(rdd)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5150830b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|alice| 18|\n",
      "|  Bob| 24|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "row = Row('name','age')\n",
    "rdd = rdd.map(lambda data: row(*data))\n",
    "df = sparkSession.createDataFrame(rdd)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1188d3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "| 18|alice|\n",
      "| 23|  Bob|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sparkSession.createDataFrame([{'name':'alice','age':18},{'name':'Bob','age':23}])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2620562d",
   "metadata": {},
   "source": [
    "#### pyspark.sql.SparkSession.getActiveSession\n",
    "Returns the active SparkSession for the current thread, returned by the builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7c1fc7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://audacious:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb68fd71300>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkSession.getActiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7134160a",
   "metadata": {},
   "source": [
    "#### pyspark.sql.SparkSession.newSession\n",
    "\n",
    "    Returns a new SparkSession as new session, that has separate SQLConf, registered temporary views and UDFs, but shared SparkContext and table cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a99012d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession_new = sparkSession.newSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374b851b",
   "metadata": {},
   "source": [
    "#### pyspark.sql.SparkSession.range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "426b6871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0),\n",
       " Row(id=1),\n",
       " Row(id=2),\n",
       " Row(id=3),\n",
       " Row(id=4),\n",
       " Row(id=5),\n",
       " Row(id=6),\n",
       " Row(id=7),\n",
       " Row(id=8),\n",
       " Row(id=9)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sparkSession.range(start=0, end=1000, step=1, numPartitions=3)\n",
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00e581cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa017b39",
   "metadata": {},
   "source": [
    "#### pyspark.sql.SparkSession.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8788646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparkSession.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98953f0",
   "metadata": {},
   "source": [
    "#### pyspark.sql.SparkSession.udf\n",
    "\n",
    "UDF’s a.k.a User Defined Functions, If you are coming from SQL background, UDF’s are nothing new to you as most of the traditional RDBMS databases support User Defined Functions, these functions need to register in the database library and use them on SQL as regular functions.\n",
    "\n",
    "PySpark UDF’s are similar to UDF on traditional databases. In PySpark, you create a function in a Python syntax and wrap it with PySpark SQL udf() or register it as udf and use it on DataFrame and SQL respectively.\n",
    "#### Why do we need a UDF?\n",
    "\n",
    "UDF’s are used to extend the functions of the framework and re-use these functions on multiple DataFrame’s. For example, you wanted to convert every first letter of a word in a name string to a capital case; PySpark build-in features don’t have this function hence you can create it a UDF and reuse this as needed on many Data Frames. UDF’s are once created they can be re-used on several DataFrame’s and SQL expressions.\n",
    "\n",
    "Before you create any UDF, do your research to check if the similar function you wanted is already available in Spark SQL Functions. PySpark SQL provides several predefined common functions and many more new functions are added with every release. hence, It is best to check before you reinventing the wheel.\n",
    "\n",
    "When you creating UDF’s you need to design them very carefully otherwise you will come across optimization & performance issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ae1ddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7955f9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=IntegerType())\n",
    "def fun(data):\n",
    "    return data*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d84b2f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|fun(id)|\n",
      "+-------+\n",
      "|      0|\n",
      "|      2|\n",
      "|      4|\n",
      "|      6|\n",
      "|      8|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sparkSession.range(0,110)\n",
    "df.select(fun('id')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b2ff8f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fun2 = udf(lambda data: data*3, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ffa123da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|<lambda>(id)|\n",
      "+------------+\n",
      "|           0|\n",
      "|           3|\n",
      "|           6|\n",
      "|           9|\n",
      "|          12|\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(fun2('id')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4b8a787a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(data)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkSession.udf.register('FUN2', fun2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0ca17a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createTempView('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0ece165e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|FUN2(id)|\n",
      "+--------+\n",
      "|       0|\n",
      "|       3|\n",
      "|       6|\n",
      "|       9|\n",
      "|      12|\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkSession.sql('select FUN2(id) from data').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b491c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
