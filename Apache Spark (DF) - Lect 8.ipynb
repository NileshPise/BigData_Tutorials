{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a83c3b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "sparkSession = SparkSession.builder.config(conf= SparkConf() \\\n",
    "                            .setAppName('dataframe functions') \\\n",
    "                            .setMaster('local[2]')).enableHiveSupport()\\\n",
    "                            .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cff589",
   "metadata": {},
   "source": [
    "### Normal Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd162401",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/06 11:40:22 INFO HiveConf: Found configuration file file:/home/audacious/spark-3.3.1-bin-hadoop3/conf/hive-site.xml\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.metastore.wm.default.pool.size does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.llap.task.scheduler.preempt.independent does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.llap.output.format.arrow does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.tez.llap.min.reducer.per.executor does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.arrow.root.allocator.limit does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.vectorized.use.checked.expressions does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.tez.dynamic.semijoin.reduction.for.mapjoin does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.vectorized.complex.types.enabled does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.server2.wm.worker.threads does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.repl.partitions.dump.parallelism does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.metastore.uri.selection does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.strict.checks.no.partition.filter does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.tez.dynamic.semijoin.reduction.for.dpp.factor does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.stats.filter.in.min.ratio does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.metastore.client.cache.initial.capacity does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.stats.ndv.estimate.percent does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.server2.webui.cors.allowed.methods does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.optimize.joinreducededuplication does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.metastore.client.cache.enabled does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.stats.fetch.bitvector does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.disable.unsafe.external.table.operations does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.materializedview.rewriting.incremental does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.server2.materializedviews.registry.impl does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.metastore.event.db.notification.api.auth does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.exec.orc.delta.streaming.optimizations.enabled does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.stats.ndv.algo does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.spark.job.max.tasks does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.msck.repair.batch.max.retries does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.prewarm.spark.timeout does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.optimize.update.table.properties.from.serde.list does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.llap.plugin.client.num.threads does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.test.bucketcodec.version does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.query.reexecution.enabled does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.materializedview.rewriting.time.window does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.query.reexecution.stats.cache.batch.size does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.server2.webui.cors.allowed.headers does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.join.inner.residual does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.server2.active.passive.ha.enable does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.llap.io.trace.always.dump does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.query.reexecution.stats.persist.scope does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.mm.allow.originals does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.compactor.compact.insert.only does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.txn.xlock.iow does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.spark.rsc.conf.list does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.llap.cache.defaultfs.only.native.fileid does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.spark.optimize.shuffle.serde does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.testing.remove.logs does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.distcp.privileged.doAs does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.strict.checks.orderby.no.limit does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.metastore.client.cache.expiry.time does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.llap.io.allocator.defrag.headroom does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.notification.event.consumers does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.vectorized.input.format.supports.enabled does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.metastore.client.cache.max.capacity does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.repl.dumpdir.clean.freq does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.spark.use.ts.stats.for.mapjoin does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.repl.dump.include.acid.tables does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.server2.webui.use.pam does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.query.reexecution.max.count does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.llap.io.share.object.pools does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.optimize.update.table.properties.from.serde does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.service.metrics.codahale.reporter.classes does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.tez.session.events.print.summary does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.llap.io.vrb.queue.limit.base does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.mm.avoid.s3.globstatus does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.repl.replica.functions.root.dir does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.query.results.cache.max.entry.lifetime does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.server2.limit.connections.per.user does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.server2.thrift.http.compression.enabled does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.vectorized.execution.ptf.enabled does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.optimize.shared.work.extended does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.vectorized.row.identifier.enabled does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.query.reexecution.always.collect.operator.stats does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.repl.dumpdir.ttl does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.local.time.zone does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.server2.tez.wm.am.registry.timeout does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.server2.active.passive.ha.registry.namespace does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.create.as.insert.only does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.llap.mapjoin.memory.oversubscribe.factor does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.arrow.batch.size does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.notification.sequence.lock.retry.sleep.interval does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.repl.approx.max.load.tasks does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.query.results.cache.enabled does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.legacy.schema.for.all.serdes does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.tez.dag.status.check.interval does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.druid.bitmap.type does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.spark.dynamic.partition.pruning.map.join.only does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.llap.memory.oversubscription.max.executors.per.query does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.llap.io.trace.size does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.llap.plugin.rpc.num.handlers does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.server2.wm.allow.any.pool.via.jdbc does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.vectorized.groupby.complex.types.enabled does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.avro.timestamp.skip.conversion does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.query.results.cache.nontransactional.tables.enabled does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.stats.correlated.multi.key.joins does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.metastore.db.type does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.streaming.auto.flush.check.interval.size does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.zookeeper.connection.timeout does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.query.reexecution.strategies does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.server2.limit.connections.per.user.ipaddress does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.llap.mapjoin.memory.monitor.check.interval does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.optimize.shared.work does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.stats.estimate does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.llap.io.allocator.discard.method does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.tez.cartesian-product.enabled does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.notification.sequence.lock.max.retries does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.heap.memory.monitor.usage.threshold does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.privilege.synchronizer.interval does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.vectorized.adaptor.suppress.evaluate.exceptions does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.materializedview.rebuild.incremental does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.query.results.cache.max.entry.size does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.spark.stage.max.tasks does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.testing.short.logs does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.streaming.auto.flush.enabled does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.spark.explain.user does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.server2.operation.log.cleanup.delay does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.repl.dump.metadata.only does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.optimize.countdistinct does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.auto.convert.join.shuffle.max.size does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.llap.plugin.acl does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.metastore.schema.info.class does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.server2.tez.queue.access.check does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.llap.external.splits.temp.table.storage.format does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.llap.io.row.wrapper.enabled does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.constraint.notnull.enforce does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.cli.print.escape.crlf does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.trigger.validation.interval does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.server2.webui.cors.allowed.origins does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.server2.limit.connections.per.ipaddress does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.llap.external.splits.order.by.force.single.split does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.metastore.client.cache.stats.enabled does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.notification.event.poll.interval does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.transactional.concatenate.noblock does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.materializedview.rewriting.strategy does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.vectorized.if.expr.mode does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.exim.test.mode does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.query.results.cache.directory does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.query.results.cache.wait.for.pending.results does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.remove.orderby.in.subquery does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.tez.bmj.use.subcache does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.llap.io.vrb.queue.limit.min does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.server2.wm.pool.metrics does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.repl.add.raw.reserved.namespace does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.resource.use.hdfs.location does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.stats.num.nulls.estimate.percent does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.llap.io.acid does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.llap.zk.sm.session.timeout does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.vectorized.ptf.max.memory.buffering.batch.count does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.llap.task.scheduler.am.registry does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.druid.overlord.address.default does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.optimize.remove.sq_count_check does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.server2.webui.enable.cors does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.vectorized.row.serde.inputformat.excludes does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.query.reexecution.stats.cache.size does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.combine.equivalent.work.optimization does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.lock.query.string.max.length does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.llap.io.track.cache.usage does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.use.orc.codec.pool does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.query.results.cache.max.size does not exist\n",
      "23/01/06 11:40:22 WARN HiveConf: HiveConf of name hive.repl.bootstrap.dump.open.txn.timeout does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/06 11:40:23 WARN HiveConf: HiveConf of name hive.metastore.wm.default.pool.size does not exist\n",
      "23/01/06 11:40:23 WARN HiveConf: HiveConf of name hive.llap.task.scheduler.preempt.independent does not exist\n",
      "23/01/06 11:40:23 WARN HiveConf: HiveConf of name hive.llap.output.format.arrow does not exist\n",
      "23/01/06 11:40:23 WARN HiveConf: HiveConf of name hive.tez.llap.min.reducer.per.executor does not exist\n",
      "23/01/06 11:40:23 WARN HiveConf: HiveConf of name hive.arrow.root.allocator.limit does not exist\n",
      "23/01/06 11:40:23 WARN HiveConf: HiveConf of name hive.vectorized.use.checked.expressions does not exist\n",
      "23/01/06 11:40:23 WARN HiveConf: HiveConf of name hive.tez.dynamic.semijoin.reduction.for.mapjoin does not exist\n",
      "23/01/06 11:40:23 WARN HiveConf: HiveConf of name hive.vectorized.complex.types.enabled does not exist\n",
      "23/01/06 11:40:23 WARN HiveConf: HiveConf of name hive.server2.wm.worker.threads does not exist\n",
      "23/01/06 11:40:23 WARN HiveConf: HiveConf of name hive.repl.partitions.dump.parallelism does not exist\n",
      "23/01/06 11:40:23 WARN HiveConf: HiveConf of name hive.metastore.uri.selection does not exist\n",
      "23/01/06 11:40:23 WARN HiveConf: HiveConf of name hive.strict.checks.no.partition.filter does not exist\n",
      "23/01/06 11:40:23 WARN HiveConf: HiveConf of name hive.tez.dynamic.semijoin.reduction.for.dpp.factor does not exist\n",
      "23/01/06 11:40:23 WARN HiveConf: HiveConf of name hive.stats.filter.in.min.ratio does not exist\n",
      "23/01/06 11:40:23 WARN HiveConf: HiveConf of name hive.metastore.client.cache.initial.capacity does not exist\n",
      "23/01/06 11:40:23 WARN HiveConf: HiveConf of name hive.stats.ndv.estimate.percent does not exist\n",
      "23/01/06 11:40:23 WARN HiveConf: HiveConf of name hive.server2.webui.cors.allowed.methods does not exist\n",
      "23/01/06 11:40:23 WARN HiveConf: HiveConf of name hive.optimize.joinreducededuplication does not exist\n",
      "23/01/06 11:40:23 WARN HiveConf: HiveConf of name hive.metastore.client.cache.enabled does not exist\n",
      "23/01/06 11:40:23 WARN HiveConf: HiveConf of name hive.stats.fetch.bitvector does not exist\n",
      "23/01/06 11:40:23 WARN HiveConf: HiveConf of name hive.disable.unsafe.external.table.operations does not exist\n",
      "23/01/06 11:40:23 WARN HiveConf: HiveConf of name hive.materializedview.rewriting.incremental does not exist\n",
      "23/01/06 11:40:23 WARN HiveConf: HiveConf of name hive.server2.materializedviews.registry.impl does not exist\n",
      "23/01/06 11:40:23 WARN HiveConf: HiveConf of name hive.metastore.event.db.notification.api.auth does not exist\n",
      "23/01/06 11:40:23 WARN HiveConf: HiveConf of name hive.exec.orc.delta.streaming.optimizations.enabled does not exist\n",
      "23/01/06 11:40:23 WARN HiveConf: HiveConf of name hive.stats.ndv.algo does not exist\n",
      "23/01/06 11:40:23 WARN HiveConf: HiveConf of name hive.spark.job.max.tasks does not exist\n",
      "23/01/06 11:40:23 WARN HiveConf: HiveConf of name hive.msck.repair.batch.max.retries does not exist\n",
      "23/01/06 11:40:23 WARN HiveConf: HiveConf of name hive.prewarm.spark.timeout does not exist\n",
      "23/01/06 11:40:23 WARN HiveConf: HiveConf of name hive.optimize.update.table.properties.from.serde.list does not exist\n",
      "23/01/06 11:40:23 WARN HiveConf: HiveConf of name hive.llap.plugin.client.num.threads does not exist\n",
      "23/01/06 11:40:23 WARN HiveConf: HiveConf of name hive.test.bucketcodec.version does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.query.reexecution.enabled does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.materializedview.rewriting.time.window does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.query.reexecution.stats.cache.batch.size does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.server2.webui.cors.allowed.headers does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.join.inner.residual does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.server2.active.passive.ha.enable does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.llap.io.trace.always.dump does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.query.reexecution.stats.persist.scope does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.mm.allow.originals does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.compactor.compact.insert.only does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.txn.xlock.iow does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.spark.rsc.conf.list does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.llap.cache.defaultfs.only.native.fileid does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.spark.optimize.shuffle.serde does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.testing.remove.logs does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.distcp.privileged.doAs does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.strict.checks.orderby.no.limit does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.metastore.client.cache.expiry.time does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.llap.io.allocator.defrag.headroom does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.notification.event.consumers does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.vectorized.input.format.supports.enabled does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.metastore.client.cache.max.capacity does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.repl.dumpdir.clean.freq does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.spark.use.ts.stats.for.mapjoin does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.repl.dump.include.acid.tables does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.server2.webui.use.pam does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.query.reexecution.max.count does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.llap.io.share.object.pools does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.optimize.update.table.properties.from.serde does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.service.metrics.codahale.reporter.classes does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.tez.session.events.print.summary does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.llap.io.vrb.queue.limit.base does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.mm.avoid.s3.globstatus does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.repl.replica.functions.root.dir does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.query.results.cache.max.entry.lifetime does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.server2.limit.connections.per.user does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.server2.thrift.http.compression.enabled does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.vectorized.execution.ptf.enabled does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.optimize.shared.work.extended does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.vectorized.row.identifier.enabled does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.query.reexecution.always.collect.operator.stats does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.repl.dumpdir.ttl does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.local.time.zone does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.server2.tez.wm.am.registry.timeout does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.server2.active.passive.ha.registry.namespace does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.create.as.insert.only does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.llap.mapjoin.memory.oversubscribe.factor does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.arrow.batch.size does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.notification.sequence.lock.retry.sleep.interval does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.repl.approx.max.load.tasks does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.query.results.cache.enabled does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.legacy.schema.for.all.serdes does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.tez.dag.status.check.interval does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.druid.bitmap.type does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.spark.dynamic.partition.pruning.map.join.only does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.llap.memory.oversubscription.max.executors.per.query does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.llap.io.trace.size does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.llap.plugin.rpc.num.handlers does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.server2.wm.allow.any.pool.via.jdbc does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.vectorized.groupby.complex.types.enabled does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.avro.timestamp.skip.conversion does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.query.results.cache.nontransactional.tables.enabled does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.stats.correlated.multi.key.joins does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.metastore.db.type does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.streaming.auto.flush.check.interval.size does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.zookeeper.connection.timeout does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.query.reexecution.strategies does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.server2.limit.connections.per.user.ipaddress does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.llap.mapjoin.memory.monitor.check.interval does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.optimize.shared.work does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.stats.estimate does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.llap.io.allocator.discard.method does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.tez.cartesian-product.enabled does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.notification.sequence.lock.max.retries does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.heap.memory.monitor.usage.threshold does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.privilege.synchronizer.interval does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.vectorized.adaptor.suppress.evaluate.exceptions does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.materializedview.rebuild.incremental does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.query.results.cache.max.entry.size does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.spark.stage.max.tasks does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.testing.short.logs does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.streaming.auto.flush.enabled does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.spark.explain.user does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.server2.operation.log.cleanup.delay does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.repl.dump.metadata.only does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.optimize.countdistinct does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.auto.convert.join.shuffle.max.size does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.llap.plugin.acl does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.metastore.schema.info.class does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.server2.tez.queue.access.check does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.llap.external.splits.temp.table.storage.format does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.llap.io.row.wrapper.enabled does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.constraint.notnull.enforce does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.cli.print.escape.crlf does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.trigger.validation.interval does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.server2.webui.cors.allowed.origins does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.server2.limit.connections.per.ipaddress does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.llap.external.splits.order.by.force.single.split does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.metastore.client.cache.stats.enabled does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.notification.event.poll.interval does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.transactional.concatenate.noblock does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.materializedview.rewriting.strategy does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.vectorized.if.expr.mode does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.exim.test.mode does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.query.results.cache.directory does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.query.results.cache.wait.for.pending.results does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.remove.orderby.in.subquery does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.tez.bmj.use.subcache does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.llap.io.vrb.queue.limit.min does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.server2.wm.pool.metrics does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.repl.add.raw.reserved.namespace does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.resource.use.hdfs.location does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.stats.num.nulls.estimate.percent does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.llap.io.acid does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.llap.zk.sm.session.timeout does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.vectorized.ptf.max.memory.buffering.batch.count does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.llap.task.scheduler.am.registry does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.druid.overlord.address.default does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.optimize.remove.sq_count_check does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.server2.webui.enable.cors does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.vectorized.row.serde.inputformat.excludes does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.query.reexecution.stats.cache.size does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.combine.equivalent.work.optimization does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.lock.query.string.max.length does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.llap.io.track.cache.usage does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.use.orc.codec.pool does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.query.results.cache.max.size does not exist\n",
      "23/01/06 11:40:24 WARN HiveConf: HiveConf of name hive.repl.bootstrap.dump.open.txn.timeout does not exist\n",
      "23/01/06 11:40:24 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore\n",
      "23/01/06 11:40:24 INFO ObjectStore: ObjectStore, initialize called\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/06 11:40:24 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored\n",
      "23/01/06 11:40:24 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored\n",
      "Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.\n",
      "23/01/06 11:40:25 INFO HikariDataSource: HikariPool-1 - Started.\n",
      "23/01/06 11:40:25 WARN DriverDataSource: Registered driver with driverClassName=com.mysql.jdbc.Driver was not found, trying direct instantiation.\n",
      "23/01/06 11:40:25 INFO HikariDataSource: HikariPool-2 - Started.\n",
      "23/01/06 11:40:25 WARN DriverDataSource: Registered driver with driverClassName=com.mysql.jdbc.Driver was not found, trying direct instantiation.\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.metastore.wm.default.pool.size does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.llap.task.scheduler.preempt.independent does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.llap.output.format.arrow does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.tez.llap.min.reducer.per.executor does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.arrow.root.allocator.limit does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.vectorized.use.checked.expressions does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.tez.dynamic.semijoin.reduction.for.mapjoin does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.vectorized.complex.types.enabled does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.server2.wm.worker.threads does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.repl.partitions.dump.parallelism does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.metastore.uri.selection does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.strict.checks.no.partition.filter does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.tez.dynamic.semijoin.reduction.for.dpp.factor does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.stats.filter.in.min.ratio does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.metastore.client.cache.initial.capacity does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.stats.ndv.estimate.percent does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.server2.webui.cors.allowed.methods does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.optimize.joinreducededuplication does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.metastore.client.cache.enabled does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.stats.fetch.bitvector does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.disable.unsafe.external.table.operations does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.materializedview.rewriting.incremental does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.server2.materializedviews.registry.impl does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.metastore.event.db.notification.api.auth does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.exec.orc.delta.streaming.optimizations.enabled does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.stats.ndv.algo does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.spark.job.max.tasks does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.msck.repair.batch.max.retries does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.prewarm.spark.timeout does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.optimize.update.table.properties.from.serde.list does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.llap.plugin.client.num.threads does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.test.bucketcodec.version does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.query.reexecution.enabled does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.materializedview.rewriting.time.window does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.query.reexecution.stats.cache.batch.size does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.server2.webui.cors.allowed.headers does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.join.inner.residual does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.server2.active.passive.ha.enable does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.llap.io.trace.always.dump does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.query.reexecution.stats.persist.scope does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.mm.allow.originals does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.compactor.compact.insert.only does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.txn.xlock.iow does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.spark.rsc.conf.list does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.llap.cache.defaultfs.only.native.fileid does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.spark.optimize.shuffle.serde does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.testing.remove.logs does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.distcp.privileged.doAs does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.strict.checks.orderby.no.limit does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.metastore.client.cache.expiry.time does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.llap.io.allocator.defrag.headroom does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.notification.event.consumers does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.vectorized.input.format.supports.enabled does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.metastore.client.cache.max.capacity does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.repl.dumpdir.clean.freq does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.spark.use.ts.stats.for.mapjoin does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.repl.dump.include.acid.tables does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.server2.webui.use.pam does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.query.reexecution.max.count does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.llap.io.share.object.pools does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.optimize.update.table.properties.from.serde does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.service.metrics.codahale.reporter.classes does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.tez.session.events.print.summary does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.llap.io.vrb.queue.limit.base does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.mm.avoid.s3.globstatus does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.repl.replica.functions.root.dir does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.query.results.cache.max.entry.lifetime does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.server2.limit.connections.per.user does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.server2.thrift.http.compression.enabled does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.vectorized.execution.ptf.enabled does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.optimize.shared.work.extended does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.vectorized.row.identifier.enabled does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.query.reexecution.always.collect.operator.stats does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.repl.dumpdir.ttl does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.local.time.zone does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.server2.tez.wm.am.registry.timeout does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.server2.active.passive.ha.registry.namespace does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.create.as.insert.only does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.llap.mapjoin.memory.oversubscribe.factor does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.arrow.batch.size does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.notification.sequence.lock.retry.sleep.interval does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.repl.approx.max.load.tasks does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.query.results.cache.enabled does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.legacy.schema.for.all.serdes does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.tez.dag.status.check.interval does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.druid.bitmap.type does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.spark.dynamic.partition.pruning.map.join.only does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.llap.memory.oversubscription.max.executors.per.query does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.llap.io.trace.size does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.llap.plugin.rpc.num.handlers does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.server2.wm.allow.any.pool.via.jdbc does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.vectorized.groupby.complex.types.enabled does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.avro.timestamp.skip.conversion does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.query.results.cache.nontransactional.tables.enabled does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.stats.correlated.multi.key.joins does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.metastore.db.type does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.streaming.auto.flush.check.interval.size does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.zookeeper.connection.timeout does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.query.reexecution.strategies does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.server2.limit.connections.per.user.ipaddress does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.llap.mapjoin.memory.monitor.check.interval does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.optimize.shared.work does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.stats.estimate does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.llap.io.allocator.discard.method does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.tez.cartesian-product.enabled does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.notification.sequence.lock.max.retries does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.heap.memory.monitor.usage.threshold does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.privilege.synchronizer.interval does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.vectorized.adaptor.suppress.evaluate.exceptions does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.materializedview.rebuild.incremental does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.query.results.cache.max.entry.size does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.spark.stage.max.tasks does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.testing.short.logs does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.streaming.auto.flush.enabled does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.spark.explain.user does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.server2.operation.log.cleanup.delay does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.repl.dump.metadata.only does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.optimize.countdistinct does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.auto.convert.join.shuffle.max.size does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.llap.plugin.acl does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.metastore.schema.info.class does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.server2.tez.queue.access.check does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.llap.external.splits.temp.table.storage.format does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.llap.io.row.wrapper.enabled does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.constraint.notnull.enforce does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.cli.print.escape.crlf does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.trigger.validation.interval does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.server2.webui.cors.allowed.origins does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.server2.limit.connections.per.ipaddress does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.llap.external.splits.order.by.force.single.split does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.metastore.client.cache.stats.enabled does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.notification.event.poll.interval does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.transactional.concatenate.noblock does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.materializedview.rewriting.strategy does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.vectorized.if.expr.mode does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.exim.test.mode does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.query.results.cache.directory does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.query.results.cache.wait.for.pending.results does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.remove.orderby.in.subquery does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.tez.bmj.use.subcache does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.llap.io.vrb.queue.limit.min does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.server2.wm.pool.metrics does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.repl.add.raw.reserved.namespace does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.resource.use.hdfs.location does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.stats.num.nulls.estimate.percent does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.llap.io.acid does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.llap.zk.sm.session.timeout does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.vectorized.ptf.max.memory.buffering.batch.count does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.llap.task.scheduler.am.registry does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.druid.overlord.address.default does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.optimize.remove.sq_count_check does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.server2.webui.enable.cors does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.vectorized.row.serde.inputformat.excludes does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.query.reexecution.stats.cache.size does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.combine.equivalent.work.optimization does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.lock.query.string.max.length does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.llap.io.track.cache.usage does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.use.orc.codec.pool does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.query.results.cache.max.size does not exist\n",
      "23/01/06 11:40:26 WARN HiveConf: HiveConf of name hive.repl.bootstrap.dump.open.txn.timeout does not exist\n",
      "23/01/06 11:40:26 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=\"Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/06 11:40:28 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is MYSQL\n",
      "23/01/06 11:40:28 INFO ObjectStore: Initialized ObjectStore\n",
      "23/01/06 11:40:28 INFO HiveMetaStore: Added admin role in metastore\n",
      "23/01/06 11:40:29 INFO HiveMetaStore: Added public role in metastore\n",
      "23/01/06 11:40:29 INFO HiveMetaStore: No user is added in admin role, since config is empty\n",
      "23/01/06 11:40:29 INFO HiveMetaStore: 0: get_database: default\n",
      "23/01/06 11:40:29 INFO audit: ugi=audacious\tip=unknown-ip-addr\tcmd=get_database: default\t\n",
      "23/01/06 11:40:29 INFO HiveMetaStore: 0: get_database: global_temp\n",
      "23/01/06 11:40:29 INFO audit: ugi=audacious\tip=unknown-ip-addr\tcmd=get_database: global_temp\t\n",
      "23/01/06 11:40:29 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "23/01/06 11:40:29 INFO HiveMetaStore: 0: get_database: testing\n",
      "23/01/06 11:40:29 INFO audit: ugi=audacious\tip=unknown-ip-addr\tcmd=get_database: testing\t\n",
      "23/01/06 11:40:29 INFO HiveMetaStore: 0: get_table : db=testing tbl=health\n",
      "23/01/06 11:40:29 INFO audit: ugi=audacious\tip=unknown-ip-addr\tcmd=get_table : db=testing tbl=health\t\n",
      "23/01/06 11:40:30 INFO HiveMetaStore: 0: get_table : db=testing tbl=health\n",
      "23/01/06 11:40:30 INFO audit: ugi=audacious\tip=unknown-ip-addr\tcmd=get_table : db=testing tbl=health\t\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+---+------------+----------+-------+---------+---+--------+--------+----+------+-------+------------+-------------+--------------+-------+-------------+-------+--------------------+---+\n",
      "|USMER|MEDICAL_UNIT|SEX|PATIENT_TYPE| DATE_DIED|INTUBED|PNEUMONIA|AGE|PREGNANT|DIABETES|COPD|ASTHMA|INMSUPR|HIPERTENSION|OTHER_DISEASE|CARDIOVASCULAR|OBESITY|RENAL_CHRONIC|TOBACCO|CLASIFFICATION_FINAL|ICU|\n",
      "+-----+------------+---+------------+----------+-------+---------+---+--------+--------+----+------+-------+------------+-------------+--------------+-------+-------------+-------+--------------------+---+\n",
      "|    2|           1|  1|           1|03/05/2020|     97|        1| 65|       2|       2|   2|     2|      2|           1|            2|             2|      2|            2|      2|                   3| 97|\n",
      "|    2|           1|  2|           1|03/06/2020|     97|        1| 72|      97|       2|   2|     2|      2|           1|            2|             2|      1|            1|      2|                   5| 97|\n",
      "|    2|           1|  2|           2|09/06/2020|      1|        2| 55|      97|       1|   2|     2|      2|           2|            2|             2|      2|            2|      2|                   3|  2|\n",
      "|    2|           1|  1|           1|12/06/2020|     97|        2| 53|       2|       2|   2|     2|      2|           2|            2|             2|      2|            2|      2|                   7| 97|\n",
      "|    2|           1|  2|           1|21/06/2020|     97|        2| 68|      97|       1|   2|     2|      2|           1|            2|             2|      2|            2|      2|                   3| 97|\n",
      "+-----+------------+---+------------+----------+-------+---------+---+--------+--------+----+------+-------+------------+-------------+--------------+-------+-------------+-------+--------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = sparkSession.read.table('testing.health')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0484130c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|USMER|\n",
      "+-----+\n",
      "|    2|\n",
      "|    2|\n",
      "|    2|\n",
      "|    2|\n",
      "|    2|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# col\n",
    "df.select(F.col('USMER')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c248839a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|USMER|\n",
      "+-----+\n",
      "|    2|\n",
      "|    2|\n",
      "|    2|\n",
      "|    2|\n",
      "|    2|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#column\n",
    "df.select(F.column('USMER')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a590c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|  amp_col|\n",
      "+---------+\n",
      "|{65 -> 1}|\n",
      "|{72 -> 2}|\n",
      "|{55 -> 2}|\n",
      "|{53 -> 1}|\n",
      "|{68 -> 2}|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create_map\n",
    "df.select(F.create_map(F.col('AGE'),F.col('SEX')).alias('amp_col')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd00d8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "|AGE|constant|\n",
      "+---+--------+\n",
      "| 65|       1|\n",
      "| 72|       1|\n",
      "| 55|       1|\n",
      "| 53|       1|\n",
      "| 68|       1|\n",
      "+---+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#lit to create new column with some values\n",
    "df.select(F.col('AGE'), F.lit(1).alias('constant')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a26f98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|array_col|\n",
      "+---------+\n",
      "|  [65, 1]|\n",
      "|  [72, 2]|\n",
      "|  [55, 2]|\n",
      "|  [53, 1]|\n",
      "|  [68, 2]|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# array column\n",
    "df.select(F.array(F.col('AGE'), F.col('SEX')).alias('array_col')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfab3869",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|             map|\n",
      "+----------------+\n",
      "|{2 -> a, 5 -> b}|\n",
      "+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### create map on array columns\n",
    "\n",
    "df1 = sparkSession.createDataFrame([([2, 5], ['a', 'b'])], ['k', 'v'])\n",
    "df1.select(F.map_from_arrays(df1.k, df1.v).alias(\"map\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "713f844a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|isnan(SEX)|\n",
      "+----------+\n",
      "|     false|\n",
      "|     false|\n",
      "|     false|\n",
      "|     false|\n",
      "|     false|\n",
      "|     false|\n",
      "|     false|\n",
      "|     false|\n",
      "|     false|\n",
      "|     false|\n",
      "|     false|\n",
      "|     false|\n",
      "|     false|\n",
      "|     false|\n",
      "|     false|\n",
      "|     false|\n",
      "|     false|\n",
      "|     false|\n",
      "|     false|\n",
      "|     false|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.isnan(F.col('SEX'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c4e6c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|(SEX IS NULL)|\n",
      "+-------------+\n",
      "|        false|\n",
      "|        false|\n",
      "|        false|\n",
      "|        false|\n",
      "|        false|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.isnull(F.col('SEX'))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c666c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#monotonically id\n",
    "df.select(F.monotonically_increasing_id().alias('id')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a9a7910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|Struct_col|\n",
      "+----------+\n",
      "|   {1, 65}|\n",
      "|   {2, 72}|\n",
      "|   {2, 55}|\n",
      "|   {1, 53}|\n",
      "|   {2, 68}|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#struct\n",
    "df.select(F.struct(F.col('SEX'), F.col('AGE')).alias('Struct_col')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ebe28a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|new|\n",
      "+---+\n",
      "|651|\n",
      "|  0|\n",
      "|  0|\n",
      "|531|\n",
      "|  0|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# when\n",
    "df.select(F.when(F.col('SEX') == 1, F.concat(F.col('AGE'), F.lit(1))).otherwise(0).alias('new')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba67d471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|greatest(AGE, SEX)|\n",
      "+------------------+\n",
      "|                65|\n",
      "|                72|\n",
      "|                55|\n",
      "|                53|\n",
      "|                68|\n",
      "|                40|\n",
      "|                64|\n",
      "|                64|\n",
      "|                37|\n",
      "|                25|\n",
      "|                38|\n",
      "|                24|\n",
      "|                30|\n",
      "|                55|\n",
      "|                48|\n",
      "|                23|\n",
      "|                80|\n",
      "|                61|\n",
      "|                54|\n",
      "|                64|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#greatest\n",
    "df.select(F.greatest(F.col('AGE'),F.col('SEX'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e64fc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|least(AGE, SEX)|\n",
      "+---------------+\n",
      "|              1|\n",
      "|              2|\n",
      "|              2|\n",
      "|              1|\n",
      "|              2|\n",
      "|              1|\n",
      "|              1|\n",
      "|              1|\n",
      "|              1|\n",
      "|              1|\n",
      "|              1|\n",
      "|              2|\n",
      "|              2|\n",
      "|              2|\n",
      "|              1|\n",
      "|              1|\n",
      "|              1|\n",
      "|              2|\n",
      "|              2|\n",
      "|              1|\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#least\n",
    "df.select(F.least(F.col('AGE'),F.col('SEX'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6340e9fd",
   "metadata": {},
   "source": [
    "### Collection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7430f11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|array_contains(data, a)|\n",
      "+-----------------------+\n",
      "|                   true|\n",
      "|                   true|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# array_contain\n",
    "sparkSession.createDataFrame([([\"a\", \"b\", \"c\"],), ([\"a\",\"x\",\"s\"],)], ['data']) \\\n",
    "    .select(F.array_contains(F.col('data'), F.lit(\"a\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd8211fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|array_contains(data, a)|\n",
      "+-----------------------+\n",
      "|                   true|\n",
      "|                   true|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# array_contain\n",
    "sparkSession.createDataFrame([([\"a\", \"b\", \"c\"],), ([\"a\",\"x\",\"s\"],)], ['data']) \\\n",
    "    .select(F.array_contains(F.col('data'), \"a\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cad2c32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------+\n",
      "|CASE WHEN (array_contains(data, a) = false) THEN 1 ELSE 0 END|\n",
      "+-------------------------------------------------------------+\n",
      "|                                                            0|\n",
      "|                                                            0|\n",
      "+-------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# array_contain\n",
    "sparkSession.createDataFrame([([\"a\", \"b\", \"c\"],), ([\"a\",\"x\",\"s\"],)], ['data']) \\\n",
    "    .select(F.when(F.array_contains(F.col('data'), \"a\") == 'false',1).otherwise(0)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b19dabb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|slice(data, 2, 2)|\n",
      "+-----------------+\n",
      "|           [b, c]|\n",
      "|           [x, s]|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# slice array --col, index position, length\n",
    "sparkSession.createDataFrame([([\"a\", \"b\", \"c\"],), ([\"a\",\"x\",\"s\"],)], ['data']) \\\n",
    "    .select(F.slice(F.col('data'),2,2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "60531d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|joined_array|\n",
      "+------------+\n",
      "|       a|b|c|\n",
      "|       a|x|s|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# array_join\n",
    "sparkSession.createDataFrame([([\"a\", \"b\", \"c\"],), ([\"a\",\"x\",\"s\"],)], ['data']) \\\n",
    "    .select(F.array_join(F.col('data'), '|').alias('joined_array')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ec2d9cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|concat(AGE, |, SEX)|\n",
      "+-------------------+\n",
      "|               65|1|\n",
      "|               72|2|\n",
      "|               55|2|\n",
      "|               53|1|\n",
      "|               68|2|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#array concate\n",
    "df.select(F.concat(df.AGE,F.lit('|'), df.SEX)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a9a37f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|array_position(data, a)|\n",
      "+-----------------------+\n",
      "|                      3|\n",
      "|                      0|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkSession.createDataFrame([([\"c\", \"b\", \"a\"],), ([],)], ['data']) \\\n",
    "    .select(F.array_position(F.col('data'),\"a\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "80e67dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|element_at(data, 2)|\n",
      "+-------------------+\n",
      "|                  b|\n",
      "|               null|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#array element by position\n",
    "sparkSession.createDataFrame([([\"c\", \"b\", \"a\"],), ([],)], ['data']) \\\n",
    "    .select(F.element_at(F.col('data'),2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7d99abaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|              r|\n",
      "+---------------+\n",
      "|[1, 2, 3, null]|\n",
      "|            [1]|\n",
      "|             []|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkSession.createDataFrame([([2, 1, None, 3],),([1],),([],)], ['data']) \\\n",
    "    .select(F.array_sort('data').alias('r')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bd4bad66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|array_distinct(data)|\n",
      "+--------------------+\n",
      "|           [1, 2, 3]|\n",
      "|              [4, 5]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " sparkSession.createDataFrame([([1, 2, 3, 2],), ([4, 5, 5, 4],)], ['data']) \\\n",
    "        .select(F.array_distinct('data')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a8c26251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|array_intersect(c1, c2)|\n",
      "+-----------------------+\n",
      "|                 [a, c]|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "sparkSession.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])]) \\\n",
    "    .select(F.array_intersect('c1', 'c2')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "360cdbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|array_union(c1, c2)|\n",
      "+-------------------+\n",
      "|    [b, a, c, d, f]|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "sparkSession.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])]) \\\n",
    "    .select(F.array_union('c1', 'c2')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "af90cecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "|key|      values|\n",
      "+---+------------+\n",
      "|  1|[1, 2, 3, 4]|\n",
      "+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transform on array\n",
    "df2 = sparkSession.createDataFrame([(1, [1, 2, 3, 4])], (\"key\", \"values\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "802de96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|      double|\n",
      "+------------+\n",
      "|[2, 4, 6, 8]|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(F.transform('values', lambda data: data*2).alias('double')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b9eee0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|filtered|\n",
      "+--------+\n",
      "|  [3, 4]|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter element in array\n",
    "df2.select(F.filter('values', lambda data: data>2).alias('filtered')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "49f4bc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| sum|\n",
      "+----+\n",
      "|42.0|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#aggregate\n",
    "sparkSession.createDataFrame([(1, [20.0, 4.0, 2.0, 6.0, 10.0])], (\"id\", \"values\"))\\\n",
    "    .select(F.aggregate(\"values\", F.lit(0.0), lambda acc, x: acc + x).alias(\"sum\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b59600f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|powers                     |\n",
      "+---------------------------+\n",
      "|[1.0, 9.0, 625.0, 262144.0]|\n",
      "+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#zipwith\n",
    "sparkSession.createDataFrame([(1, [1, 3, 5, 8], [0, 2, 4, 6])], (\"id\", \"xs\", \"ys\")) \\\n",
    ".select(F.zip_with(\"xs\", \"ys\", lambda x, y: x ** y).alias(\"powers\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "813b9a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|powers              |\n",
      "+--------------------+\n",
      "|[1_0, 3_2, 5_4, 8_6]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#zipwith\n",
    "sparkSession.createDataFrame([(1, [1, 3, 5, 8], [0, 2, 4, 6])], (\"id\", \"xs\", \"ys\")) \\\n",
    ".select(F.zip_with(\"xs\", \"ys\", lambda x, y: F.concat_ws('_',x,y)).alias(\"powers\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e54f1e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9dde7b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|data_upper               |\n",
      "+-------------------------+\n",
      "|{BAR -> 2.0, FOO -> -2.0}|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#transform key\n",
    "sparkSession.createDataFrame([(1, {\"foo\": -2.0, \"bar\": 2.0})], (\"id\", \"data\")) \\\n",
    "    .select(F.transform_keys(\"data\", lambda k, _: F.upper(k)).alias(\"data_upper\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "94f84d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|data_upper               |\n",
      "+-------------------------+\n",
      "|{bar -> 4.0, foo -> -4.0}|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#transform values\n",
    "sparkSession.createDataFrame([(1, {\"foo\": -2.0, \"bar\": 2.0})], (\"id\", \"data\")) \\\n",
    "    .select(F.transform_values(\"data\", lambda _, values: values*2).alias(\"data_upper\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e271af01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------+\n",
      "|map_filter(data, lambdafunction((namedlambdavariable() > 10), namedlambdavariable(), namedlambdavariable()))|\n",
      "+------------------------------------------------------------------------------------------------------------+\n",
      "|{baz -> 32.0, foo -> 42.0}                                                                                  |\n",
      "+------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#map_filter filter on map values\n",
    "sparkSession.createDataFrame([(1, {\"foo\": 42.0, \"bar\": 1.0, \"baz\": 32.0})], (\"id\", \"data\")) \\\n",
    "    .select(F.map_filter(F.col('data'), lambda k, v: v>10)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "48dcff0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id| col|\n",
      "+---+----+\n",
      "|  1|20.0|\n",
      "|  1| 4.0|\n",
      "|  1| 2.0|\n",
      "|  1| 6.0|\n",
      "|  1|10.0|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#explode\n",
    "sparkSession.createDataFrame([(1, [20.0, 4.0, 2.0, 6.0, 10.0])], (\"id\", \"values\")) \\\n",
    "    .select(F.col('id'), F.explode('values')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ebc9085b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+\n",
      "| id|key|value|\n",
      "+---+---+-----+\n",
      "|  1|bar|  1.0|\n",
      "|  1|baz| 32.0|\n",
      "|  1|foo| 42.0|\n",
      "+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkSession.createDataFrame([(1, {\"foo\": 42.0, \"bar\": 1.0, \"baz\": 32.0})], (\"id\", \"data\")) \\\n",
    "    .select(F.col('id'), F.explode('data')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6b4800a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|              df5|\n",
      "+-----------------+\n",
      "|{1, nilesh, pise}|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from_json\n",
    "data = [(1,\"\"\"{'id':1, 'name':'nilesh','lname':'pise'}\"\"\")]\n",
    "schema = T.StructType([T.StructField('id', T.StringType(), True) \\\n",
    "                     ,T.StructField('name', T.StringType(), True) \\\n",
    "                     ,T.StructField('lname', T.StringType(), True)])\n",
    "sparkSession.createDataFrame(data, ['x','y']).select(F.from_json('y', schema).alias('df5')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "730f15fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|size(values)|\n",
      "+------------+\n",
      "|           5|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#size\n",
    "sparkSession.createDataFrame([(1, [20.0, 4.0, 2.0, 6.0, 10.0])], (\"id\", \"values\")) \\\n",
    "    .select(F.size('values')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "01bf0046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|sort_array(values, false)|\n",
      "+-------------------------+\n",
      "|     [20.0, 10.0, 6.0,...|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sort_array\n",
    "sparkSession.createDataFrame([(1, [20.0, 4.0, 2.0, 6.0, 10.0])], (\"id\", \"values\")) \\\n",
    "    .select(F.sort_array('values', asc=False)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "17f93a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|array_max(values)|\n",
      "+-----------------+\n",
      "|             20.0|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#array_max\n",
    "sparkSession.createDataFrame([(1, [20.0, 4.0, 2.0, 6.0, 10.0])], (\"id\", \"values\")) \\\n",
    "    .select(F.array_max('values')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8e218212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|array_min(values)|\n",
      "+-----------------+\n",
      "|              2.0|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#array_min\n",
    "sparkSession.createDataFrame([(1, [20.0, 4.0, 2.0, 6.0, 10.0])], (\"id\", \"values\")) \\\n",
    "    .select(F.array_min('values')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0465dedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|        s|\n",
      "+---------+\n",
      "|LQS krapS|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#reverse\n",
    "sparkSession.createDataFrame([('Spark SQL',)], ['data']) \\\n",
    "    .select(F.reverse('data').alias('s')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6120a8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|     reverse(values)|\n",
      "+--------------------+\n",
      "|[10.0, 6.0, 2.0, ...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkSession.createDataFrame([(1, [20.0, 4.0, 2.0, 6.0, 10.0])], (\"id\", \"values\")) \\\n",
    "    .select(F.reverse('values')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b4dcde68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|r                 |\n",
      "+------------------+\n",
      "|[1, 2, 3, 4, 5, 6]|\n",
      "|null              |\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#flatten\n",
    "sparkSession.createDataFrame([([[1, 2, 3], [4, 5], [6]],), ([None, [4, 5]],)], ['data']) \\\n",
    "    .select(F.flatten('data').alias('r')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a0d702a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  keys|\n",
      "+------+\n",
      "|[1, 2]|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#map_keys\n",
    "sparkSession.sql(\"SELECT map(1, 'a', 2, 'b') as data\") \\\n",
    "    .select(F.map_keys(\"data\").alias(\"keys\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2e9fa327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  keys|\n",
      "+------+\n",
      "|[a, b]|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#map_values\n",
    "#map_keys\n",
    "sparkSession.sql(\"SELECT map(1, 'a', 2, 'b') as data\") \\\n",
    "    .select(F.map_values(\"data\").alias(\"keys\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e55c9d",
   "metadata": {},
   "source": [
    "### Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a21f636",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|          avg(AGE)|\n",
      "+------------------+\n",
      "|41.794102472403026|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#avg\n",
    "df.select(F.avg(F.col('AGE'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0579ff95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|   collect_list(AGE)|\n",
      "+--------------------+\n",
      "|[65, 72, 55, 53, ...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#collect_list\n",
    "df.select(F.collect_list(F.col('AGE'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5d5fee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|    collect_set(AGE)|\n",
      "+--------------------+\n",
      "|[66, 120, 15, 76,...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#collect_set\n",
    "df.select(F.collect_set(F.col('AGE'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38e07fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 13:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|corr(DIABETES, HIPERTENSION)|\n",
      "+----------------------------+\n",
      "|          0.8350178504029727|\n",
      "+----------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 13:=============================>                            (1 + 1) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# corr -- pearson corelation\n",
    "df.select(F.corr(F.col('DIABETES'),F.col('HIPERTENSION'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3475b44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|count(AGE)|\n",
      "+----------+\n",
      "|   1048575|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#count\n",
    "df.select(F.count(F.col('AGE'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "356c8144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|count(DISTINCT SEX)|\n",
      "+-------------------+\n",
      "|                  2|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#count_distinct \n",
    "df.select(F.count_distinct(F.col('SEX'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd9c9d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|count(DISTINCT SEX)|\n",
      "+-------------------+\n",
      "|                  2|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.countDistinct(F.col('SEX'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c61668c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|first(AGE)|\n",
      "+----------+\n",
      "|        65|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#first\n",
    "df.select(F.first(F.col('AGE'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcfa106a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|last(AGE)|\n",
      "+---------+\n",
      "|       52|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#last\n",
    "df.select(F.last(F.col('AGE'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60f10394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|max(AGE)|\n",
      "+--------+\n",
      "|      99|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#max\n",
    "df.select(F.max(F.col('AGE'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96edbd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|min(AGE)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#min\n",
    "df.select(F.min(F.col('AGE'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a3767af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|          avg(AGE)|\n",
      "+------------------+\n",
      "|41.794102472403026|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#mean\n",
    "df.select(F.mean(F.col('AGE'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "349036c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|   sum(AGE)|\n",
      "+-----------+\n",
      "|4.3824251E7|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sum\n",
    "df.select(F.sum(F.col('AGE'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b831c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|sum(DISTINCT AGE)|\n",
      "+-----------------+\n",
      "|           7269.0|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sum_distinct\n",
    "df.select(F.sum_distinct(F.col('AGE'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f12ed2",
   "metadata": {},
   "source": [
    "### String Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8d372c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|concate_col|\n",
      "+-----------+\n",
      "|       65-1|\n",
      "|       72-2|\n",
      "|       55-2|\n",
      "|       53-1|\n",
      "|       68-2|\n",
      "+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#concat_ws\n",
    "df.select(F.concat_ws('-', F.col('AGE'), F.col('SEX')).alias('concate_col')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e108f9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|concate_col|\n",
      "+-----------+\n",
      "|        165|\n",
      "|        272|\n",
      "|        255|\n",
      "|        153|\n",
      "|        268|\n",
      "+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#concate\n",
    "df.select(F.concat(F.col('SEX'), F.col('AGE')).alias('concate_col')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ebb6716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|fomat|\n",
      "+-----+\n",
      "|65 HI|\n",
      "|72 HI|\n",
      "|55 HI|\n",
      "|53 HI|\n",
      "|68 HI|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#format_string\n",
    "df.select(F.format_string(\"%s %s\",F.col('AGE'), F.lit('HI')).alias('fomat')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a7136268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+\n",
      "|AGE|initcap(heloo fello wo)|\n",
      "+---+-----------------------+\n",
      "| 65|         Heloo Fello Wo|\n",
      "| 72|         Heloo Fello Wo|\n",
      "| 55|         Heloo Fello Wo|\n",
      "| 53|         Heloo Fello Wo|\n",
      "| 68|         Heloo Fello Wo|\n",
      "+---+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#initcap\n",
    "df.select(F.col('AGE'), F.initcap(F.lit('heloo fello wo'))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f18aadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|len|\n",
      "+---+\n",
      "|  8|\n",
      "|  8|\n",
      "|  8|\n",
      "|  8|\n",
      "|  8|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#length\n",
    "df.select(F.length(F.lit('Hi three')).alias('len')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e00f01f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|lower(HI THrere)|\n",
      "+----------------+\n",
      "|       hi threre|\n",
      "|       hi threre|\n",
      "|       hi threre|\n",
      "|       hi threre|\n",
      "|       hi threre|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#lower\n",
    "df.select(F.lower(F.lit('HI THrere'))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e8ba40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|upper(hi gsdhd)|\n",
      "+---------------+\n",
      "|       HI GSDHD|\n",
      "|       HI GSDHD|\n",
      "|       HI GSDHD|\n",
      "|       HI GSDHD|\n",
      "|       HI GSDHD|\n",
      "+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#upper\n",
    "df.select(F.upper(F.lit('hi gsdhd'))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c06c9888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|lpad(Hi, 4, 0)|\n",
      "+--------------+\n",
      "|          00Hi|\n",
      "|          00Hi|\n",
      "|          00Hi|\n",
      "+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#lpad --adding value to left side\n",
    "df.select(F.lpad(F.lit('Hi'), 4, '0')).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0eec6f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|rpad(Hi, 4, 0)|\n",
      "+--------------+\n",
      "|          Hi00|\n",
      "|          Hi00|\n",
      "|          Hi00|\n",
      "+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#rpad\n",
    "df.select(F.rpad(F.lit('Hi'), 4, '0')).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f1b8a471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|ltrim(    Hi There)|\n",
      "+-------------------+\n",
      "|           Hi There|\n",
      "|           Hi There|\n",
      "|           Hi There|\n",
      "|           Hi There|\n",
      "|           Hi There|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ltrim trim spaces from left side\n",
    "df.select(F.ltrim(F.lit('    Hi There'))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b2f40068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|rtrim(    Hi There      )|\n",
      "+-------------------------+\n",
      "|                 Hi There|\n",
      "|                 Hi There|\n",
      "|                 Hi There|\n",
      "|                 Hi There|\n",
      "|                 Hi There|\n",
      "+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#rtrim trim spaces from right side\n",
    "df.select(F.rtrim(F.lit('    Hi There      '))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1ca75132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+\n",
      "|regexp_extract(Hi ho There ho, ho, 0)|\n",
      "+-------------------------------------+\n",
      "|                                   ho|\n",
      "|                                   ho|\n",
      "|                                   ho|\n",
      "|                                   ho|\n",
      "|                                   ho|\n",
      "+-------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#regexp_extract\n",
    "df.select(F.regexp_extract(F.lit('Hi ho There ho'), 'ho',0)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9961976d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+\n",
      "|regexp_replace(Hi ho they 8, ho, 00, 1)|\n",
      "+---------------------------------------+\n",
      "|                           Hi 00 they 8|\n",
      "|                           Hi 00 they 8|\n",
      "|                           Hi 00 they 8|\n",
      "+---------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#regexp_replace\n",
    "df.select(F.regexp_replace(F.lit('Hi ho they 8'), pattern='ho', replacement='00')).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b4d0afe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|split(Hi There Hello,  , -1)|\n",
      "+----------------------------+\n",
      "|          [Hi, There, Hello]|\n",
      "|          [Hi, There, Hello]|\n",
      "|          [Hi, There, Hello]|\n",
      "|          [Hi, There, Hello]|\n",
      "|          [Hi, There, Hello]|\n",
      "+----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#split\n",
    "df.select(F.split(F.lit('Hi There Hello'), pattern=' ',limit=-1)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c3beb190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|split(Hi There Hello, r, 2)|\n",
      "+---------------------------+\n",
      "|          [Hi The, e Hello]|\n",
      "|          [Hi The, e Hello]|\n",
      "|          [Hi The, e Hello]|\n",
      "|          [Hi The, e Hello]|\n",
      "|          [Hi The, e Hello]|\n",
      "+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.split(F.lit('Hi There Hello'), pattern='r',limit=2)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "753ff281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|substring(Hi There, 3, 5)|\n",
      "+-------------------------+\n",
      "|                     Ther|\n",
      "|                     Ther|\n",
      "|                     Ther|\n",
      "+-------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#substring\n",
    "df.select(F.substring(F.lit('Hi There'),pos=3, len=5)).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56112d2e",
   "metadata": {},
   "source": [
    "### UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6aeebf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = sparkSession.createDataFrame([(1, \"John Doe\", None), (2, \"Rolls Doe\", 26), (3, \"John rcok\", 23)], (\"id\", \"name\", \"age\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "73c9aa5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+--------------+\n",
      "|<lambda>(name)|add_1(age)|to_upper(name)|\n",
      "+--------------+----------+--------------+\n",
      "|             8|      null|      JOHN DOE|\n",
      "|             9|        27|     ROLLS DOE|\n",
      "|             9|        24|     JOHN RCOK|\n",
      "+--------------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "slen = F.udf(lambda data: len(data), T.IntegerType())\n",
    "\n",
    "@F.udf(returnType=T.IntegerType())\n",
    "def add_1(data):\n",
    "    if data is not None:\n",
    "        return data + 1\n",
    "    else:\n",
    "        return None\n",
    "@F.udf()\n",
    "def to_upper(data):\n",
    "    if data is not None:\n",
    "        return data.upper()\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "df2.select(slen(F.col('name')), add_1(F.col('age')), to_upper(F.col('name'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41248482",
   "metadata": {},
   "source": [
    "### Datetime Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1ac68251",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = sparkSession.createDataFrame([('2015-04-08', 2),('2019-01-27', 1) \\\n",
    "                                    ,('2018-06-23', 6),('2013-05-08', 3)], ['dt', 'add'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6b769637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|        dt|next_month|\n",
      "+----------+----------+\n",
      "|2015-04-08|2015-05-08|\n",
      "|2019-01-27|2019-02-27|\n",
      "|2018-06-23|2018-07-23|\n",
      "|2013-05-08|2013-06-08|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#add_months\n",
    "df2.select(F.col('dt'), F.add_months(F.col('dt'),1).alias('next_month')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1df0d84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|        dt|next3months|\n",
      "+----------+-----------+\n",
      "|2015-04-08| 2015-07-08|\n",
      "|2019-01-27| 2019-04-27|\n",
      "|2018-06-23| 2018-09-23|\n",
      "|2013-05-08| 2013-08-08|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(F.col('dt'), F.add_months(F.col('dt'), 3).alias('next3months')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4db5f39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|        dt|today_date|\n",
      "+----------+----------+\n",
      "|2015-04-08|2023-01-06|\n",
      "|2019-01-27|2023-01-06|\n",
      "|2018-06-23|2023-01-06|\n",
      "|2013-05-08|2023-01-06|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#current_date\n",
    "df2.select(F.col('dt'), F.current_date().alias('today_date')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8d45469d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|timestamp              |\n",
      "+-----------------------+\n",
      "|2023-01-06 13:12:05.468|\n",
      "|2023-01-06 13:12:05.468|\n",
      "|2023-01-06 13:12:05.468|\n",
      "|2023-01-06 13:12:05.468|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#currenttimestamp\n",
    "df2.select(F.current_timestamp().alias('timestamp')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c7bf1dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|        dt|   nextday|\n",
      "+----------+----------+\n",
      "|2015-04-08|2015-04-09|\n",
      "|2019-01-27|2019-01-28|\n",
      "|2018-06-23|2018-06-24|\n",
      "|2013-05-08|2013-05-09|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#date_add -- add new days\n",
    "df2.select(F.col('dt'), F.date_add(F.col('dt'), 1).alias('nextday')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6878405d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|        dt| next5 day|\n",
      "+----------+----------+\n",
      "|2015-04-08|2015-04-13|\n",
      "|2019-01-27|2019-02-01|\n",
      "|2018-06-23|2018-06-28|\n",
      "|2013-05-08|2013-05-13|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(F.col('dt'), F.date_add(F.col('dt'), 5).alias('next5 day')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9fdca0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------------+\n",
      "|        dt|date_format(dt, dd/MM/yy)|\n",
      "+----------+-------------------------+\n",
      "|2015-04-08|                 08/04/15|\n",
      "|2019-01-27|                 27/01/19|\n",
      "|2018-06-23|                 23/06/18|\n",
      "|2013-05-08|                 08/05/13|\n",
      "+----------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#date_format\n",
    "df2.select(F.col('dt'), F.date_format(F.col('dt'), format='dd/MM/yy')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9d960f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------------+\n",
      "|        dt|date_format(dt, dd-MM-yy)|\n",
      "+----------+-------------------------+\n",
      "|2015-04-08|                 08-04-15|\n",
      "|2019-01-27|                 27-01-19|\n",
      "|2018-06-23|                 23-06-18|\n",
      "|2013-05-08|                 08-05-13|\n",
      "+----------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(F.col('dt'), F.date_format(F.col('dt'), format='dd-MM-yy')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "92ece3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------------+\n",
      "|        dt|date_format(dt, yy-dd-MM)|\n",
      "+----------+-------------------------+\n",
      "|2015-04-08|                 15-08-04|\n",
      "|2019-01-27|                 19-27-01|\n",
      "|2018-06-23|                 18-23-06|\n",
      "|2013-05-08|                 13-08-05|\n",
      "+----------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(F.col('dt'), F.date_format(F.col('dt'), format='yy-dd-MM')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c0252fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------------+\n",
      "|        dt|date_format(dt, yy/dd/MM)|\n",
      "+----------+-------------------------+\n",
      "|2015-04-08|                 15/08/04|\n",
      "|2019-01-27|                 19/27/01|\n",
      "|2018-06-23|                 18/23/06|\n",
      "|2013-05-08|                 13/08/05|\n",
      "+----------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(F.col('dt'), F.date_format(F.col('dt'), format='yy/dd/MM')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "573c036d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|        dt| prev_date|\n",
      "+----------+----------+\n",
      "|2015-04-08|2015-04-07|\n",
      "|2019-01-27|2019-01-26|\n",
      "|2018-06-23|2018-06-22|\n",
      "|2013-05-08|2013-05-07|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#date_sub\n",
    "df2.select(F.col('dt'), F.date_sub(F.col('dt'), 1).alias('prev_date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "46a81411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|        dt|prev_5_date|\n",
      "+----------+-----------+\n",
      "|2015-04-08| 2015-04-03|\n",
      "|2019-01-27| 2019-01-22|\n",
      "|2018-06-23| 2018-06-18|\n",
      "|2013-05-08| 2013-05-03|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(F.col('dt'), F.date_sub(F.col('dt'), 5).alias('prev_5_date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "92dadb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#date_trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4eddf628",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = sparkSession.createDataFrame([('1997-02-28 05:02:11',)], ['t'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8d5f1808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|                  t|date_trunc(year, t)|\n",
      "+-------------------+-------------------+\n",
      "|1997-02-28 05:02:11|1997-01-01 00:00:00|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.select(F.col('t'), F.date_trunc('year',F.col('t'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ddef706c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|                  t|date_trunc(month, t)|\n",
      "+-------------------+--------------------+\n",
      "|1997-02-28 05:02:11| 1997-02-01 00:00:00|\n",
      "+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.select(F.col('t'), F.date_trunc('month',F.col('t'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8daa2b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|        dt|date_diff|\n",
      "+----------+---------+\n",
      "|2015-04-08|     2830|\n",
      "|2019-01-27|     1440|\n",
      "|2018-06-23|     1658|\n",
      "|2013-05-08|     3530|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###datediff\n",
    "df2.select(F.col('dt'), F.datediff(F.current_date(),F.col('dt')).alias('date_diff')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bc99cbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|date_diif|\n",
      "+---------+\n",
      "|        2|\n",
      "|        2|\n",
      "|        2|\n",
      "|        2|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(F.datediff(F.date_add(F.current_date(), 2), F.current_date()).alias('date_diif')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f03d73d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|dayofmonth(dt)|\n",
      "+--------------+\n",
      "|             8|\n",
      "|            27|\n",
      "|            23|\n",
      "|             8|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dayofmonth\n",
    "df2.select(F.dayofmonth(F.col('dt'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "06bc3187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|dayofweek(dt)|\n",
      "+-------------+\n",
      "|            4|\n",
      "|            1|\n",
      "|            7|\n",
      "|            4|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dayofweek\n",
    "df2.select(F.dayofweek(F.col('dt'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "dd9f3009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|dayofyear(dt)|\n",
      "+-------------+\n",
      "|           98|\n",
      "|           27|\n",
      "|          174|\n",
      "|          128|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dayofyear\n",
    "df2.select(F.dayofyear(F.col('dt'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "00f1d168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|week|\n",
      "+----+\n",
      "|  15|\n",
      "|   4|\n",
      "|  25|\n",
      "|  19|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#weekofyear\n",
    "df2.select(F.weekofyear(F.col('dt')).alias('week')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b5ef814d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|second|\n",
      "+------+\n",
      "|    46|\n",
      "|    46|\n",
      "|    46|\n",
      "|    46|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#second\n",
    "df2.select(F.second(F.current_timestamp()).alias('second')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "dfe0c1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|year(dt)|\n",
      "+--------+\n",
      "|    2015|\n",
      "|    2019|\n",
      "|    2018|\n",
      "|    2013|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#year\n",
    "df2.select(F.year(F.col('dt'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4b5c5170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|quater|\n",
      "+------+\n",
      "|     2|\n",
      "|     1|\n",
      "|     2|\n",
      "|     2|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#quarter\n",
    "df2.select(F.quarter(F.col('dt')).alias('quater')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e7c00e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|month(dt)|\n",
      "+---------+\n",
      "|        4|\n",
      "|        1|\n",
      "|        6|\n",
      "|        5|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#months\n",
    "df2.select(F.month(F.col('dt'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5c8e0e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|        dt|last_day_month|\n",
      "+----------+--------------+\n",
      "|2015-04-08|    2015-04-30|\n",
      "|2019-01-27|    2019-01-31|\n",
      "|2018-06-23|    2018-06-30|\n",
      "|2013-05-08|    2013-05-31|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Returns the last day of the month which the given date belongs to.\n",
    "df2.select(F.col('dt'), F.last_day(F.col('dt')).alias('last_day_month')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7455d9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|minute|\n",
      "+------+\n",
      "|    42|\n",
      "|    42|\n",
      "|    42|\n",
      "|    42|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#minute\n",
    "df2.select(F.minute(F.current_timestamp()).alias('minute')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b1872d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|        dt|month_between|\n",
      "+----------+-------------+\n",
      "|2015-04-08|  92.93548387|\n",
      "|2019-01-27|  47.32258065|\n",
      "|2018-06-23|   54.4516129|\n",
      "|2013-05-08| 115.93548387|\n",
      "+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#months_between\n",
    "df2.select(F.col('dt'), F.months_between(F.current_date(), F.col('dt')).alias('month_between')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4f55701f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|hour(current_timestamp())|\n",
      "+-------------------------+\n",
      "|                       13|\n",
      "|                       13|\n",
      "|                       13|\n",
      "|                       13|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#hour\n",
    "df2.select(F.hour(F.current_timestamp())).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "54f1bc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|make_date(2020, 12, 10)|\n",
      "+-----------------------+\n",
      "|             2020-12-10|\n",
      "|             2020-12-10|\n",
      "|             2020-12-10|\n",
      "|             2020-12-10|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#make_date\n",
    "\n",
    "df2.select(F.make_date(year=F.lit('2020'), month=F.lit('12'), day=F.lit('10'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9d6b2a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from_unixtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "dbde8092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|                 ts|\n",
      "+-------------------+\n",
      "|2015-04-08 00:00:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkSession.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
    "time_df = sparkSession.createDataFrame([(1428476400,)], ['unix_time'])\n",
    "time_df.select(F.from_unixtime('unix_time').alias('ts')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050a2467",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from_unixtimestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "afdbcd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| unix_time|\n",
      "+----------+\n",
      "|1428476400|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkSession.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
    "time_df = sparkSession.createDataFrame([('2015-04-08',)], ['dt'])\n",
    "\n",
    "time_df.select(F.unix_timestamp('dt', 'yyyy-MM-dd').alias('unix_time')).show()\n",
    "\n",
    "sparkSession.conf.unset(\"spark.sql.session.timeZone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "adc0aa8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|                 dt|\n",
      "+-------------------+\n",
      "|1997-02-28 10:30:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to_timestamp\n",
    "df = sparkSession.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
    "df.select(F.to_timestamp(df.t).alias('dt')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8cb249cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|                 dt|\n",
      "+-------------------+\n",
      "|1997-02-28 10:30:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sparkSession.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
    "df.select(F.to_timestamp(df.t, 'yyyy-MM-dd HH:mm:ss').alias('dt')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5ee2193b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|1997-02-28|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to_date\n",
    "df = sparkSession.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
    "df.select(F.to_date(df.t).alias('date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fef12b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|1997-02-28|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sparkSession.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
    "df.select(F.to_date(df.t, 'yyyy-MM-dd HH:mm:ss').alias('date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c4d41430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|         local_time|\n",
      "+-------------------+\n",
      "|1997-02-28 02:30:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from_utc_timestamp\n",
    "df = sparkSession.createDataFrame([('1997-02-28 10:30:00', 'JST')], ['ts', 'tz'])\n",
    "df.select(F.from_utc_timestamp(df.ts, \"PST\").alias('local_time')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "76ab28d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|         local_time|\n",
      "+-------------------+\n",
      "|1997-02-28 19:30:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.from_utc_timestamp(df.ts, df.tz).alias('local_time')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "72ab886a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|           utc_time|\n",
      "+-------------------+\n",
      "|1997-02-28 18:30:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to_utc_timestamp\n",
    "df = sparkSession.createDataFrame([('1997-02-28 10:30:00', 'JST')], ['ts', 'tz'])\n",
    "df.select(F.to_utc_timestamp(df.ts, \"PST\").alias('utc_time')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "23b61bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|           utc_time|\n",
      "+-------------------+\n",
      "|1997-02-28 01:30:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/06 20:14:07 WARN DataStreamer: Exception for BP-241050457-127.0.0.1-1670841776195:blk_1073763003_22210\n",
      "java.net.SocketTimeoutException: 65000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/127.0.0.1:36044 remote=/127.0.0.1:9866]\n",
      "\tat org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)\n",
      "\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n",
      "\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n",
      "\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:118)\n",
      "\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n",
      "\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:519)\n",
      "\tat org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:213)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1137)\n"
     ]
    }
   ],
   "source": [
    "df.select(F.to_utc_timestamp(df.ts, df.tz).alias('utc_time')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6de431",
   "metadata": {},
   "source": [
    "### Math Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7776dae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
