{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd9da7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/01/01 14:20:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/01/01 14:20:53 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/01/01 14:20:53 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/01/01 14:20:53 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "23/01/01 14:20:53 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "23/01/01 14:20:53 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "23/01/01 14:20:53 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "sparkSession = SparkSession.builder.config(conf= SparkConf() \\\n",
    "                        .setAppName('DataFrame') \\\n",
    "                        .setMaster('local[4]')) \\\n",
    "                        .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1181d4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = sparkSession.read.csv('/Datasets/data.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "823da573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+---+------------+----------+-------+---------+---+--------+--------+----+------+-------+------------+-------------+--------------+-------+-------------+-------+--------------------+---+\n",
      "|USMER|MEDICAL_UNIT|SEX|PATIENT_TYPE| DATE_DIED|INTUBED|PNEUMONIA|AGE|PREGNANT|DIABETES|COPD|ASTHMA|INMSUPR|HIPERTENSION|OTHER_DISEASE|CARDIOVASCULAR|OBESITY|RENAL_CHRONIC|TOBACCO|CLASIFFICATION_FINAL|ICU|\n",
      "+-----+------------+---+------------+----------+-------+---------+---+--------+--------+----+------+-------+------------+-------------+--------------+-------+-------------+-------+--------------------+---+\n",
      "|    2|           1|  1|           1|03/05/2020|     97|        1| 65|       2|       2|   2|     2|      2|           1|            2|             2|      2|            2|      2|                   3| 97|\n",
      "|    2|           1|  2|           1|03/06/2020|     97|        1| 72|      97|       2|   2|     2|      2|           1|            2|             2|      1|            1|      2|                   5| 97|\n",
      "|    2|           1|  2|           2|09/06/2020|      1|        2| 55|      97|       1|   2|     2|      2|           2|            2|             2|      2|            2|      2|                   3|  2|\n",
      "|    2|           1|  1|           1|12/06/2020|     97|        2| 53|       2|       2|   2|     2|      2|           2|            2|             2|      2|            2|      2|                   7| 97|\n",
      "|    2|           1|  2|           1|21/06/2020|     97|        2| 68|      97|       1|   2|     2|      2|           1|            2|             2|      2|            2|      2|                   3| 97|\n",
      "+-----+------------+---+------------+----------+-------+---------+---+--------+--------+----+------+-------+------------+-------------+--------------+-------+-------------+-------+--------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c08cb09",
   "metadata": {},
   "source": [
    "#### pyspark.sql.Column.alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa8dfb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.select(df.AGE.alias('age2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f473bc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|age2|\n",
      "+----+\n",
      "|  65|\n",
      "|  72|\n",
      "|  55|\n",
      "|  53|\n",
      "|  68|\n",
      "+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4ddaaf",
   "metadata": {},
   "source": [
    "#### pyspark.sql.Column.asc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f2761ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|age2|\n",
      "+----+\n",
      "|   0|\n",
      "|   0|\n",
      "|   0|\n",
      "|   0|\n",
      "|   0|\n",
      "+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:=============================>                             (2 + 2) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2.orderBy(df2.age2.asc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "577cb648",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:==============>                                            (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|USMER|AGE|\n",
      "+-----+---+\n",
      "|    2|  0|\n",
      "|    1|  0|\n",
      "|    1|  0|\n",
      "|    2|  0|\n",
      "|    2|  0|\n",
      "+-----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select('USMER','AGE').orderBy(df.AGE.asc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006a4bb4",
   "metadata": {},
   "source": [
    "#### pyspark.sql.Column.asc_nulls_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "728702c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = sparkSession.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db910fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 8:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|height|\n",
      "+-----+------+\n",
      "|Alice|  null|\n",
      "| null|    60|\n",
      "|  Tom|    80|\n",
      "+-----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 8:============================================>              (3 + 1) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df5.orderBy(df5.height.asc_nulls_first()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "437b410f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|height|\n",
      "+-----+------+\n",
      "| null|    60|\n",
      "|  Tom|    80|\n",
      "|Alice|  null|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.orderBy(df5.height.asc_nulls_last()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e30144",
   "metadata": {},
   "source": [
    "#### pyspark.sql.Column.astype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b75b56e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df2.select(df2.age2.astype('String'))\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8ef40b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_84863/1128062976.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mage2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Integer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "df2 = df2.select(df2.age2.astype('Integer'))\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c0613a",
   "metadata": {},
   "source": [
    "#### pyspark.sql.Column.between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc820a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------+\n",
      "| name|((height >= 50) AND (height <= 70))|\n",
      "+-----+-----------------------------------+\n",
      "|  Tom|                              false|\n",
      "| null|                               true|\n",
      "|Alice|                               null|\n",
      "+-----+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.select(df5.name, df5.height.between(50, 70)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1c7b61",
   "metadata": {},
   "source": [
    "#### pyspark.sql.Column.cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "08c758f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions  import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47a76da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age2: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df2.select(df2.age2.cast(IntegerType()))\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "90fb76b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df2.select(df2.age2.cast(StringType()))\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "580ee54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|height|\n",
      "+-----+------+\n",
      "|Alice|  null|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.select('*').filter(df5.name.contains('A')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "919cc98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|name|height|\n",
      "+----+------+\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.select('*').filter(df5.name.contains('n')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7286ecc2",
   "metadata": {},
   "source": [
    "#### pyspark.sql.Column.desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c392f0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|height|\n",
      "+-----+------+\n",
      "|  Tom|    80|\n",
      "| null|    60|\n",
      "|Alice|  null|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.select('*').orderBy(df5.height.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cbcfc9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|height|\n",
      "+-----+------+\n",
      "|Alice|  null|\n",
      "|  Tom|    80|\n",
      "| null|    60|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.select('*').orderBy(df5.height.desc_nulls_first()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b9ed21da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|height|\n",
      "+-----+------+\n",
      "|  Tom|    80|\n",
      "| null|    60|\n",
      "|Alice|  null|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.select('*').orderBy(df5.height.desc_nulls_last()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0725a166",
   "metadata": {},
   "source": [
    "#### pyspark.sql.Column.dropFields "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "157d75ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------------+\n",
      "| name|age|         friend|\n",
      "+-----+---+---------------+\n",
      "| Alex| 20| {Bob, 30, 150}|\n",
      "|Cathy| 40|{Doge, 40, 180}|\n",
      "+-----+---+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    Row(name=\"Alex\", age=20, friend=Row(name=\"Bob\",age=30,height=150)),\n",
    "    Row(name=\"Cathy\", age=40, friend=Row(name=\"Doge\",age=40,height=180))\n",
    "]\n",
    "\n",
    "df6 = sparkSession.createDataFrame(data)\n",
    "df6.show()          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "23c4739c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------------+-----------+\n",
      "| name|age|         friend|        new|\n",
      "+-----+---+---------------+-----------+\n",
      "| Alex| 20| {Bob, 30, 150}| {Bob, 150}|\n",
      "|Cathy| 40|{Doge, 40, 180}|{Doge, 180}|\n",
      "+-----+---+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.withColumn('new', df6.friend.dropFields('age')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "accd0209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------------+-----+\n",
      "| name|age|         friend|  new|\n",
      "+-----+---+---------------+-----+\n",
      "| Alex| 20| {Bob, 30, 150}|{150}|\n",
      "|Cathy| 40|{Doge, 40, 180}|{180}|\n",
      "+-----+---+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.withColumn('new', df6.friend.dropFields('age','name')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc4a382",
   "metadata": {},
   "source": [
    "#### pyspark.sql.Column.endswith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0c9c3073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+--------------+\n",
      "|name|age|        friend|\n",
      "+----+---+--------------+\n",
      "|Alex| 20|{Bob, 30, 150}|\n",
      "+----+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.filter(df6.name.endswith('x')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "59cfd277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------------+\n",
      "| name|age|         friend|\n",
      "+-----+---+---------------+\n",
      "|Cathy| 40|{Doge, 40, 180}|\n",
      "+-----+---+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.filter(df6.name.endswith('y')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb008b8e",
   "metadata": {},
   "source": [
    "#### pyspark.sql.Column.getField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "260d1345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|friend.name|\n",
      "+-----------+\n",
      "|        Bob|\n",
      "|       Doge|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.select(df6.friend.getField('name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "da787383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|friend.name|friend.age|\n",
      "+-----------+----------+\n",
      "|        Bob|        30|\n",
      "|       Doge|        40|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.select(df6.friend.getField('name'),df6.friend.getField('age')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9cb3b81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|friend.age|\n",
      "+----------+\n",
      "|        30|\n",
      "|        40|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.select(df6.friend.age).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2988d3e4",
   "metadata": {},
   "source": [
    "#### pyspark.sql.Column.getItem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "86b822cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+\n",
      "|     l|             d|\n",
      "+------+--------------+\n",
      "|[1, 2]|{key -> value}|\n",
      "+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df7 = sparkSession.createDataFrame([([1, 2], {\"key\": \"value\"})], [\"l\", \"d\"])\n",
    "df7.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "991eb09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|l[1]|d[key]|\n",
      "+----+------+\n",
      "|   2| value|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df7.select(df7.l.getItem(1), df7.d.getItem('key')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b302d502",
   "metadata": {},
   "source": [
    "#### pyspark.sql.Column.ilike\n",
    "SQL ILIKE expression (case insensitive LIKE). Returns a boolean Column based on a case insensitive match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "db5ecadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+--------------+\n",
      "|name|age|        friend|\n",
      "+----+---+--------------+\n",
      "|Alex| 20|{Bob, 30, 150}|\n",
      "+----+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.select('*').filter(df6.name.ilike('%x')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "aa40ac96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------------+\n",
      "| name|age|         friend|\n",
      "+-----+---+---------------+\n",
      "|Cathy| 40|{Doge, 40, 180}|\n",
      "+-----+---+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.select('*').filter(df6.name.ilike('%y')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7f7774",
   "metadata": {},
   "source": [
    "#### pyspark.sql.Column.isNotNull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b161f3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|name|height|\n",
      "+----+------+\n",
      "| Tom|    80|\n",
      "|null|    60|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.select('*').filter(df5.height.isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a7d0503f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|height|\n",
      "+-----+------+\n",
      "|Alice|  null|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.select('*').filter(df5.height.isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2c0a4c",
   "metadata": {},
   "source": [
    "#### pyspark.sql.Column.isin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cb7aa233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|height|\n",
      "+-----+------+\n",
      "|Alice|  null|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.select('*').filter(df5.name.isin('Alice','Nu')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d9f68539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|height|\n",
      "+-----+------+\n",
      "|  Tom|    80|\n",
      "|Alice|  null|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.select('*').filter(df5.name.isin('Alice','Tom')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54475691",
   "metadata": {},
   "source": [
    "#### pyspark.sql.Column.like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b344775f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|height|\n",
      "+-----+------+\n",
      "|Alice|  null|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.select('*').filter(df5.name.like('Al%')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a6a5ba",
   "metadata": {},
   "source": [
    "#### pyspark.sql.Column.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5e7fbbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| Name|\n",
      "+-----+\n",
      "|  Tom|\n",
      "| null|\n",
      "|Alice|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.select(df5.name.name('Name')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b676c5",
   "metadata": {},
   "source": [
    "#### pyspark.sql.Column.otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "db4a343b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|AGE|Gender|\n",
      "+---+------+\n",
      "| 65|     M|\n",
      "| 72|     F|\n",
      "| 55|     F|\n",
      "| 53|     M|\n",
      "| 68|     F|\n",
      "+---+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.AGE, when(df.SEX == 1, 'M').otherwise('F').alias('Gender')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d037b88",
   "metadata": {},
   "source": [
    "#### pyspark.sql.Column.rlike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "44c406bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+--------------+\n",
      "|name|age|        friend|\n",
      "+----+---+--------------+\n",
      "|Alex| 20|{Bob, 30, 150}|\n",
      "+----+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.select('*').filter(df6.name.rlike('lex$')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8931df7",
   "metadata": {},
   "source": [
    "#### pyspark.sql.Column.startswith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "71705ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|height|\n",
      "+-----+------+\n",
      "|Alice|  null|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.select('*').filter(df5.name.startswith('A')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902e9266",
   "metadata": {},
   "source": [
    "#### pyspark.sql.Column.substr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "510ec158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| new|\n",
      "+----+\n",
      "| Tom|\n",
      "|null|\n",
      "| Ali|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.select(df5.name.substr(1,3).alias('new')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4ea13c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| new|\n",
      "+----+\n",
      "|  om|\n",
      "|null|\n",
      "| lic|\n",
      "+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/02 11:21:12 WARN DataStreamer: Exception for BP-241050457-127.0.0.1-1670841776195:blk_1073762840_22041\n",
      "java.net.SocketTimeoutException: 65000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/127.0.0.1:48080 remote=/127.0.0.1:9866]\n",
      "\tat org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)\n",
      "\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n",
      "\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n",
      "\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:118)\n",
      "\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n",
      "\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:519)\n",
      "\tat org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:213)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1137)\n"
     ]
    }
   ],
   "source": [
    "df5.select(df5.name.substr(2,3).alias('new')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ee547e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
