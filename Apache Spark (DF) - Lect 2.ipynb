{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4987e40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/12/28 14:19:44 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "sparkSession = SparkSession.builder.appName('DataFrame') \\\n",
    "                            .config(conf=SparkConf() \\\n",
    "                                        .setMaster('yarn')) \\\n",
    "                            .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4861895",
   "metadata": {},
   "source": [
    "#### pyspark.sql.DataFrameReader.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "469ef10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+---+------------+----------+-------+---------+---+--------+--------+----+------+-------+------------+-------------+--------------+-------+-------------+-------+--------------------+----+\n",
      "|  _c0|         _c1|_c2|         _c3|       _c4|    _c5|      _c6|_c7|     _c8|     _c9|_c10|  _c11|   _c12|        _c13|         _c14|          _c15|   _c16|         _c17|   _c18|                _c19|_c20|\n",
      "+-----+------------+---+------------+----------+-------+---------+---+--------+--------+----+------+-------+------------+-------------+--------------+-------+-------------+-------+--------------------+----+\n",
      "|USMER|MEDICAL_UNIT|SEX|PATIENT_TYPE| DATE_DIED|INTUBED|PNEUMONIA|AGE|PREGNANT|DIABETES|COPD|ASTHMA|INMSUPR|HIPERTENSION|OTHER_DISEASE|CARDIOVASCULAR|OBESITY|RENAL_CHRONIC|TOBACCO|CLASIFFICATION_FINAL| ICU|\n",
      "|    2|           1|  1|           1|03/05/2020|     97|        1| 65|       2|       2|   2|     2|      2|           1|            2|             2|      2|            2|      2|                   3|  97|\n",
      "|    2|           1|  2|           1|03/06/2020|     97|        1| 72|      97|       2|   2|     2|      2|           1|            2|             2|      1|            1|      2|                   5|  97|\n",
      "|    2|           1|  2|           2|09/06/2020|      1|        2| 55|      97|       1|   2|     2|      2|           2|            2|             2|      2|            2|      2|                   3|   2|\n",
      "|    2|           1|  1|           1|12/06/2020|     97|        2| 53|       2|       2|   2|     2|      2|           2|            2|             2|      2|            2|      2|                   7|  97|\n",
      "|    2|           1|  2|           1|21/06/2020|     97|        2| 68|      97|       1|   2|     2|      2|           1|            2|             2|      2|            2|      2|                   3|  97|\n",
      "|    2|           1|  1|           2|9999-99-99|      2|        1| 40|       2|       2|   2|     2|      2|           2|            2|             2|      2|            2|      2|                   3|   2|\n",
      "|    2|           1|  1|           1|9999-99-99|     97|        2| 64|       2|       2|   2|     2|      2|           2|            2|             2|      2|            2|      2|                   3|  97|\n",
      "|    2|           1|  1|           1|9999-99-99|     97|        1| 64|       2|       1|   2|     2|      1|           1|            2|             2|      2|            1|      2|                   3|  97|\n",
      "|    2|           1|  1|           2|9999-99-99|      2|        2| 37|       2|       1|   2|     2|      2|           1|            2|             2|      1|            2|      2|                   3|   2|\n",
      "+-----+------------+---+------------+----------+-------+---------+---+--------+--------+----+------+-------+------------+-------------+--------------+-------+-------------+-------+--------------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sparkSession.read.csv(path='/Datasets/data.csv')\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12ac843b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+---+------------+----------+-------+---------+---+--------+--------+----+------+-------+------------+-------------+--------------+-------+-------------+-------+--------------------+---+\n",
      "|USMER|MEDICAL_UNIT|SEX|PATIENT_TYPE| DATE_DIED|INTUBED|PNEUMONIA|AGE|PREGNANT|DIABETES|COPD|ASTHMA|INMSUPR|HIPERTENSION|OTHER_DISEASE|CARDIOVASCULAR|OBESITY|RENAL_CHRONIC|TOBACCO|CLASIFFICATION_FINAL|ICU|\n",
      "+-----+------------+---+------------+----------+-------+---------+---+--------+--------+----+------+-------+------------+-------------+--------------+-------+-------------+-------+--------------------+---+\n",
      "|    2|           1|  1|           1|03/05/2020|     97|        1| 65|       2|       2|   2|     2|      2|           1|            2|             2|      2|            2|      2|                   3| 97|\n",
      "|    2|           1|  2|           1|03/06/2020|     97|        1| 72|      97|       2|   2|     2|      2|           1|            2|             2|      1|            1|      2|                   5| 97|\n",
      "|    2|           1|  2|           2|09/06/2020|      1|        2| 55|      97|       1|   2|     2|      2|           2|            2|             2|      2|            2|      2|                   3|  2|\n",
      "|    2|           1|  1|           1|12/06/2020|     97|        2| 53|       2|       2|   2|     2|      2|           2|            2|             2|      2|            2|      2|                   7| 97|\n",
      "|    2|           1|  2|           1|21/06/2020|     97|        2| 68|      97|       1|   2|     2|      2|           1|            2|             2|      2|            2|      2|                   3| 97|\n",
      "|    2|           1|  1|           2|9999-99-99|      2|        1| 40|       2|       2|   2|     2|      2|           2|            2|             2|      2|            2|      2|                   3|  2|\n",
      "|    2|           1|  1|           1|9999-99-99|     97|        2| 64|       2|       2|   2|     2|      2|           2|            2|             2|      2|            2|      2|                   3| 97|\n",
      "|    2|           1|  1|           1|9999-99-99|     97|        1| 64|       2|       1|   2|     2|      1|           1|            2|             2|      2|            1|      2|                   3| 97|\n",
      "|    2|           1|  1|           2|9999-99-99|      2|        2| 37|       2|       1|   2|     2|      2|           1|            2|             2|      1|            2|      2|                   3|  2|\n",
      "|    2|           1|  1|           2|9999-99-99|      2|        2| 25|       2|       2|   2|     2|      2|           2|            2|             2|      2|            2|      2|                   3|  2|\n",
      "+-----+------------+---+------------+----------+-------+---------+---+--------+--------+----+------+-------+------------+-------------+--------------+-------+-------------+-------+--------------------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sparkSession.read.csv(path='/Datasets/data.csv', header=True, inferSchema=True)\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e4b8058",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.csv(path='/Datasets/spark_health/', mode='overwrite', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9615555d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d010a3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/28 15:20:10 WARN DataStreamer: Exception for BP-241050457-127.0.0.1-1670841776195:blk_1073745037_4223\n",
      "java.net.SocketTimeoutException: 65000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/127.0.0.1:44760 remote=/127.0.0.1:9866]\n",
      "\tat org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)\n",
      "\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n",
      "\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n",
      "\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:118)\n",
      "\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n",
      "\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:519)\n",
      "\tat org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:213)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1137)\n",
      "22/12/28 15:55:56 WARN HeartbeatReceiver: Removing executor 1 with no recent heartbeats: 121792 ms exceeds timeout 120000 ms\n",
      "22/12/28 15:57:56 ERROR Utils: Uncaught exception in thread kill-executor-thread\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [120 seconds]. This timeout is controlled by spark.rpc.askTimeout\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.killExecutors(CoarseGrainedSchedulerBackend.scala:890)\n",
      "\tat org.apache.spark.SparkContext.killAndReplaceExecutor(SparkContext.scala:1825)\n",
      "\tat org.apache.spark.HeartbeatReceiver$$anon$2.$anonfun$run$2(HeartbeatReceiver.scala:214)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1484)\n",
      "\tat org.apache.spark.HeartbeatReceiver$$anon$2.run(HeartbeatReceiver.scala:211)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [120 seconds]\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:293)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\t... 10 more\n",
      "22/12/28 15:57:56 ERROR YarnSchedulerBackend$YarnSchedulerEndpoint: Sending KillExecutors(List(1)) to AM was unsuccessful\n",
      "org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from /192.168.87.202:56892 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat scala.util.Failure.recover(Try.scala:234)\n",
      "\tat scala.concurrent.Future.$anonfun$recover$1(Future.scala:395)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.tryFailure(Promise.scala:112)\n",
      "\tat scala.concurrent.Promise.tryFailure$(Promise.scala:112)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:214)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:264)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from /192.168.87.202:56892 in 120 seconds\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:265)\n",
      "\t... 7 more\n",
      "22/12/28 15:57:56 WARN NettyRpcEnv: Ignored failure: org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from /192.168.87.202:56892 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout\n",
      "22/12/28 16:09:36 ERROR YarnClientSchedulerBackend: YARN application has exited unexpectedly with state FAILED! Check the YARN application logs for more details.\n",
      "22/12/28 16:09:36 ERROR YarnClientSchedulerBackend: Diagnostics message: Uncaught exception: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.deploy.yarn.ApplicationMaster.runExecutorLauncher(ApplicationMaster.scala:558)\n",
      "\tat org.apache.spark.deploy.yarn.ApplicationMaster.run(ApplicationMaster.scala:277)\n",
      "\tat org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3.run(ApplicationMaster.scala:926)\n",
      "\tat org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3.run(ApplicationMaster.scala:925)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n",
      "\tat org.apache.spark.deploy.yarn.ApplicationMaster$.main(ApplicationMaster.scala:925)\n",
      "\tat org.apache.spark.deploy.yarn.ExecutorLauncher$.main(ApplicationMaster.scala:957)\n",
      "\tat org.apache.spark.deploy.yarn.ExecutorLauncher.main(ApplicationMaster.scala)\n",
      "Caused by: java.io.IOException: Failed to connect to audacious/127.0.0.2:42189\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:288)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:230)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:204)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:202)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:198)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: audacious/127.0.0.2:42189\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:710)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:658)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "22/12/28 16:09:36 ERROR AsyncEventQueue: Listener EventLoggingListener threw an exception\n",
      "java.io.IOException: All datanodes [DatanodeInfoWithStorage[127.0.0.1:9866,DS-e7dcc0b2-0f10-46a2-9872-6f61756e1ab0,DISK]] are bad. Aborting...\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1609)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1543)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1529)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1305)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:668)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/28 20:12:30 WARN TransportChannelHandler: Exception in connection from /192.168.87.202:56892\n",
      "java.io.IOException: Connection timed out\n",
      "\tat sun.nio.ch.FileDispatcherImpl.read0(Native Method)\n",
      "\tat sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)\n",
      "\tat sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)\n",
      "\tat sun.nio.ch.IOUtil.read(IOUtil.java:192)\n",
      "\tat sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)\n",
      "\tat io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:258)\n",
      "\tat io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1132)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:350)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:151)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:722)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:658)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/12/28 20:12:30 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from /192.168.87.202:56892 is closed\n",
      "22/12/28 20:12:30 ERROR YarnSchedulerBackend$YarnSchedulerEndpoint: Sending RequestExecutors(Map(),Map(),Map(),Set()) to AM was unsuccessful\n",
      "java.io.IOException: Connection timed out\n",
      "\tat sun.nio.ch.FileDispatcherImpl.read0(Native Method)\n",
      "\tat sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)\n",
      "\tat sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)\n",
      "\tat sun.nio.ch.IOUtil.read(IOUtil.java:192)\n",
      "\tat sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)\n",
      "\tat io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:258)\n",
      "\tat io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1132)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:350)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:151)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:722)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:658)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/12/28 20:12:30 ERROR Utils: Uncaught exception in thread YARN application state monitor\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.requestTotalExecutors(CoarseGrainedSchedulerBackend.scala:789)\n",
      "\tat org.apache.spark.scheduler.cluster.YarnSchedulerBackend.stop(YarnSchedulerBackend.scala:114)\n",
      "\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.stop(YarnClientSchedulerBackend.scala:178)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:931)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$11(SparkContext.scala:2095)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1484)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2095)\n",
      "\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:125)\n",
      "Caused by: java.io.IOException: Connection timed out\n",
      "\tat sun.nio.ch.FileDispatcherImpl.read0(Native Method)\n",
      "\tat sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)\n",
      "\tat sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)\n",
      "\tat sun.nio.ch.IOUtil.read(IOUtil.java:192)\n",
      "\tat sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)\n",
      "\tat io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:258)\n",
      "\tat io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1132)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:350)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:151)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:722)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:658)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/12/28 20:12:30 ERROR Utils: Uncaught exception in thread YARN application state monitor\n",
      "java.lang.IllegalArgumentException: Self-suppression not permitted\n",
      "\tat java.lang.Throwable.addSuppressed(Throwable.java:1072)\n",
      "\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n",
      "\tat sun.nio.cs.StreamEncoder.implClose(StreamEncoder.java:320)\n",
      "\tat sun.nio.cs.StreamEncoder.close(StreamEncoder.java:149)\n",
      "\tat java.io.OutputStreamWriter.close(OutputStreamWriter.java:233)\n",
      "\tat java.io.PrintWriter.close(PrintWriter.java:339)\n",
      "\tat org.apache.spark.deploy.history.EventLogFileWriter.$anonfun$closeWriter$1(EventLogFileWriters.scala:127)\n",
      "\tat org.apache.spark.deploy.history.EventLogFileWriter.$anonfun$closeWriter$1$adapted(EventLogFileWriters.scala:127)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.deploy.history.EventLogFileWriter.closeWriter(EventLogFileWriters.scala:127)\n",
      "\tat org.apache.spark.deploy.history.SingleEventLogFileWriter.stop(EventLogFileWriters.scala:237)\n",
      "\tat org.apache.spark.scheduler.EventLoggingListener.stop(EventLoggingListener.scala:283)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$17(SparkContext.scala:2115)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$17$adapted(SparkContext.scala:2115)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$16(SparkContext.scala:2115)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1484)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2115)\n",
      "\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:125)\n",
      "Caused by: java.io.IOException: All datanodes [DatanodeInfoWithStorage[127.0.0.1:9866,DS-e7dcc0b2-0f10-46a2-9872-6f61756e1ab0,DISK]] are bad. Aborting...\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1609)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1543)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1529)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1305)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:668)\n"
     ]
    }
   ],
   "source": [
    "df.write.partitionBy('SEX').parquet('/Datasets/health/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a9d8cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+---+------------+----------+-------+---------+---+--------+--------+----+------+-------+------------+-------------+--------------+-------+-------------+-------+--------------------+---+\n",
      "|USMER|MEDICAL_UNIT|SEX|PATIENT_TYPE| DATE_DIED|INTUBED|PNEUMONIA|AGE|PREGNANT|DIABETES|COPD|ASTHMA|INMSUPR|HIPERTENSION|OTHER_DISEASE|CARDIOVASCULAR|OBESITY|RENAL_CHRONIC|TOBACCO|CLASIFFICATION_FINAL|ICU|\n",
      "+-----+------------+---+------------+----------+-------+---------+---+--------+--------+----+------+-------+------------+-------------+--------------+-------+-------------+-------+--------------------+---+\n",
      "|    2|           1|  1|           1|03/05/2020|     97|        1| 65|       2|       2|   2|     2|      2|           1|            2|             2|      2|            2|      2|                   3| 97|\n",
      "|    2|           1|  2|           1|03/06/2020|     97|        1| 72|      97|       2|   2|     2|      2|           1|            2|             2|      1|            1|      2|                   5| 97|\n",
      "|    2|           1|  2|           2|09/06/2020|      1|        2| 55|      97|       1|   2|     2|      2|           2|            2|             2|      2|            2|      2|                   3|  2|\n",
      "|    2|           1|  1|           1|12/06/2020|     97|        2| 53|       2|       2|   2|     2|      2|           2|            2|             2|      2|            2|      2|                   7| 97|\n",
      "|    2|           1|  2|           1|21/06/2020|     97|        2| 68|      97|       1|   2|     2|      2|           1|            2|             2|      2|            2|      2|                   3| 97|\n",
      "+-----+------------+---+------------+----------+-------+---------+---+--------+--------+----+------+-------+------------+-------------+--------------+-------+-------------+-------+--------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquet_df = sparkSession.read.parquet('/Datasets/health/')\n",
    "parquet_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45c032f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.json(path='/Datasets/Spark_health_json/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11cfe7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------+--------------------+----+----------+--------+------------+---+-------+-------+------------+-------+-------------+------------+---------+--------+-------------+---+-------+-----+\n",
      "|AGE|ASTHMA|CARDIOVASCULAR|CLASIFFICATION_FINAL|COPD| DATE_DIED|DIABETES|HIPERTENSION|ICU|INMSUPR|INTUBED|MEDICAL_UNIT|OBESITY|OTHER_DISEASE|PATIENT_TYPE|PNEUMONIA|PREGNANT|RENAL_CHRONIC|SEX|TOBACCO|USMER|\n",
      "+---+------+--------------+--------------------+----+----------+--------+------------+---+-------+-------+------------+-------+-------------+------------+---------+--------+-------------+---+-------+-----+\n",
      "| 65|     2|             2|                   3|   2|03/05/2020|       2|           1| 97|      2|     97|           1|      2|            2|           1|        1|       2|            2|  1|      2|    2|\n",
      "| 72|     2|             2|                   5|   2|03/06/2020|       2|           1| 97|      2|     97|           1|      1|            2|           1|        1|      97|            1|  2|      2|    2|\n",
      "| 55|     2|             2|                   3|   2|09/06/2020|       1|           2|  2|      2|      1|           1|      2|            2|           2|        2|      97|            2|  2|      2|    2|\n",
      "| 53|     2|             2|                   7|   2|12/06/2020|       2|           2| 97|      2|     97|           1|      2|            2|           1|        2|       2|            2|  1|      2|    2|\n",
      "| 68|     2|             2|                   3|   2|21/06/2020|       1|           1| 97|      2|     97|           1|      2|            2|           1|        2|      97|            2|  2|      2|    2|\n",
      "+---+------+--------------+--------------------+----+----------+--------+------------+---+-------+-------+------------+-------+-------------+------------+---------+--------+-------------+---+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df = sparkSession.read.json(path='/Datasets/Spark_health_json/')\n",
    "json_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadc74ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a53e505e",
   "metadata": {},
   "source": [
    "#### pyspark.sql.DataFrameReader.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5daae599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+---+------------+----------+-------+---------+---+--------+--------+----+------+-------+------------+-------------+--------------+-------+-------------+-------+--------------------+----+\n",
      "|  _c0|         _c1|_c2|         _c3|       _c4|    _c5|      _c6|_c7|     _c8|     _c9|_c10|  _c11|   _c12|        _c13|         _c14|          _c15|   _c16|         _c17|   _c18|                _c19|_c20|\n",
      "+-----+------------+---+------------+----------+-------+---------+---+--------+--------+----+------+-------+------------+-------------+--------------+-------+-------------+-------+--------------------+----+\n",
      "|USMER|MEDICAL_UNIT|SEX|PATIENT_TYPE| DATE_DIED|INTUBED|PNEUMONIA|AGE|PREGNANT|DIABETES|COPD|ASTHMA|INMSUPR|HIPERTENSION|OTHER_DISEASE|CARDIOVASCULAR|OBESITY|RENAL_CHRONIC|TOBACCO|CLASIFFICATION_FINAL| ICU|\n",
      "|    2|           1|  1|           1|03/05/2020|     97|        1| 65|       2|       2|   2|     2|      2|           1|            2|             2|      2|            2|      2|                   3|  97|\n",
      "|    2|           1|  2|           1|03/06/2020|     97|        1| 72|      97|       2|   2|     2|      2|           1|            2|             2|      1|            1|      2|                   5|  97|\n",
      "|    2|           1|  2|           2|09/06/2020|      1|        2| 55|      97|       1|   2|     2|      2|           2|            2|             2|      2|            2|      2|                   3|   2|\n",
      "|    2|           1|  1|           1|12/06/2020|     97|        2| 53|       2|       2|   2|     2|      2|           2|            2|             2|      2|            2|      2|                   7|  97|\n",
      "+-----+------------+---+------------+----------+-------+---------+---+--------+--------+----+------+-------+------------+-------------+--------------+-------+-------------+-------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sparkSession.read.format('csv').load('/Datasets/data.csv')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2aa7d787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+---+------------+----------+-------+---------+---+--------+--------+----+------+-------+------------+-------------+--------------+-------+-------------+-------+--------------------+---+\n",
      "|USMER|MEDICAL_UNIT|SEX|PATIENT_TYPE| DATE_DIED|INTUBED|PNEUMONIA|AGE|PREGNANT|DIABETES|COPD|ASTHMA|INMSUPR|HIPERTENSION|OTHER_DISEASE|CARDIOVASCULAR|OBESITY|RENAL_CHRONIC|TOBACCO|CLASIFFICATION_FINAL|ICU|\n",
      "+-----+------------+---+------------+----------+-------+---------+---+--------+--------+----+------+-------+------------+-------------+--------------+-------+-------------+-------+--------------------+---+\n",
      "|    2|           1|  1|           1|03/05/2020|     97|        1| 65|       2|       2|   2|     2|      2|           1|            2|             2|      2|            2|      2|                   3| 97|\n",
      "|    2|           1|  2|           1|03/06/2020|     97|        1| 72|      97|       2|   2|     2|      2|           1|            2|             2|      1|            1|      2|                   5| 97|\n",
      "|    2|           1|  2|           2|09/06/2020|      1|        2| 55|      97|       1|   2|     2|      2|           2|            2|             2|      2|            2|      2|                   3|  2|\n",
      "|    2|           1|  1|           1|12/06/2020|     97|        2| 53|       2|       2|   2|     2|      2|           2|            2|             2|      2|            2|      2|                   7| 97|\n",
      "|    2|           1|  2|           1|21/06/2020|     97|        2| 68|      97|       1|   2|     2|      2|           1|            2|             2|      2|            2|      2|                   3| 97|\n",
      "+-----+------------+---+------------+----------+-------+---------+---+--------+--------+----+------+-------+------------+-------------+--------------+-------+-------------+-------+--------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sparkSession.read.format('parquet').load('/Datasets/health/')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a910e47",
   "metadata": {},
   "source": [
    "#### pyspark.sql.DataFrameReader.jdbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f6dfd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 13:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+-------------------+\n",
      "|ap_id|          project_id|        project_name|          created_by|       created_date|\n",
      "+-----+--------------------+--------------------+--------------------+-------------------+\n",
      "|   20|1990881e7026a350f...|           RFPIndia1|425928e3a6d568200...|               null|\n",
      "|    1|2569c09de916f62f4...|  DigitalCertificate|190ac748f5f9310dc...|               null|\n",
      "|   26|2811af1fc14f5a257...|        RAJinsurance|27afb6922e9369be1...|2021-09-15 02:15:35|\n",
      "|    2|37745c0dbe2932523...|Datacentermanagem...|a3ea12a367c8957e9...|               null|\n",
      "|   23|39cef4d7809bc490a...|            TayoTest|ecf46c527996d04bf...|2021-08-19 01:35:54|\n",
      "+-----+--------------------+--------------------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "agile_projects = sparkSession.read.jdbc(url='jdbc:mysql://localhost:3306/agile' \\\n",
    "                                        , table='agile_projects' \\\n",
    "                                       ,numPartitions=2 \\\n",
    "                                       ,properties= {'user':'neil', \n",
    "                                                     'password':'Neil9130', \n",
    "                                                     'driver':'com.mysql.cj.jdbc.Driver',\n",
    "                                                     'zeroDateTimeBehavior':'convertToNull'})\n",
    "agile_projects.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b83a8af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+-------------------+\n",
      "|ap_id|          project_id|        project_name|          created_by|       created_date|\n",
      "+-----+--------------------+--------------------+--------------------+-------------------+\n",
      "|   20|1990881e7026a350f...|           RFPIndia1|425928e3a6d568200...|               null|\n",
      "|    1|2569c09de916f62f4...|  DigitalCertificate|190ac748f5f9310dc...|               null|\n",
      "|   26|2811af1fc14f5a257...|        RAJinsurance|27afb6922e9369be1...|2021-09-15 02:15:35|\n",
      "|    2|37745c0dbe2932523...|Datacentermanagem...|a3ea12a367c8957e9...|               null|\n",
      "|   23|39cef4d7809bc490a...|            TayoTest|ecf46c527996d04bf...|2021-08-19 01:35:54|\n",
      "+-----+--------------------+--------------------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read from MySQL Table\n",
    "agile_projects = sparkSession.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/agile\") \\\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", \"agile_projects\") \\\n",
    "    .option(\"user\", \"neil\") \\\n",
    "    .option(\"password\", \"Neil9130\") \\\n",
    "    .option(\"numPartitions\",3) \\\n",
    "    .option(\"fetchsize\", 100) \\\n",
    "    .option(\"zeroDateTimeBehavior\",\"convertToNull\").load()\n",
    "agile_projects.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32884959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|          project_id|        project_name|\n",
      "+--------------------+--------------------+\n",
      "|1990881e7026a350f...|           RFPIndia1|\n",
      "|2569c09de916f62f4...|  DigitalCertificate|\n",
      "|2811af1fc14f5a257...|        RAJinsurance|\n",
      "|37745c0dbe2932523...|Datacentermanagem...|\n",
      "|39cef4d7809bc490a...|            TayoTest|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read from MySQL Table\n",
    "agile_projects = sparkSession.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/agile\") \\\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"query\", \"select project_id, project_name from agile_projects\") \\\n",
    "    .option(\"user\", \"neil\") \\\n",
    "    .option(\"password\", \"Neil9130\") \\\n",
    "    .option(\"numPartitions\",3) \\\n",
    "    .option(\"fetchsize\", 100) \\\n",
    "    .option(\"zeroDateTimeBehavior\",\"convertToNull\").load()\n",
    "agile_projects.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e70ffde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 26:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "agile_projects.write.jdbc(url='jdbc:mysql://localhost:3306/testing' \\\n",
    "                                        , table='agile_projects2' \\\n",
    "                                       ,properties= {'user':'neil', \n",
    "                                                     'password':'Neil9130', \n",
    "                                                     'driver':'com.mysql.cj.jdbc.Driver',\n",
    "                                                     'zeroDateTimeBehavior':'convertToNull'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50b65ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "agile_projects.write.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/testing\") \\\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", \"agile_projects3\") \\\n",
    "    .option(\"user\", \"neil\") \\\n",
    "    .option(\"password\", \"Neil9130\") \\\n",
    "    .option(\"zeroDateTimeBehavior\",\"convertToNull\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddd33ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/28 14:23:24 INFO HiveMetaStore: 0: get_database: testing\n",
      "22/12/28 14:23:24 INFO audit: ugi=audacious\tip=unknown-ip-addr\tcmd=get_database: testing\t\n",
      "22/12/28 14:23:24 INFO HiveMetaStore: 0: get_table : db=testing tbl=dumy\n",
      "22/12/28 14:23:24 INFO audit: ugi=audacious\tip=unknown-ip-addr\tcmd=get_table : db=testing tbl=dumy\t\n",
      "22/12/28 14:23:25 INFO HiveMetaStore: 0: get_table : db=testing tbl=dumy\n",
      "22/12/28 14:23:25 INFO audit: ugi=audacious\tip=unknown-ip-addr\tcmd=get_table : db=testing tbl=dumy\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+\n",
      "| id|  name|age|gender|\n",
      "+---+------+---+------+\n",
      "| 10|nilesh| 26|     M|\n",
      "+---+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "attachment = sparkSession.read.table('testing.dumy')\n",
    "attachment.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "568d4846",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/28 14:26:45 INFO HiveMetaStore: 0: get_database: testing\n",
      "22/12/28 14:26:45 INFO audit: ugi=audacious\tip=unknown-ip-addr\tcmd=get_database: testing\t\n",
      "22/12/28 14:26:45 INFO HiveMetaStore: 0: get_table : db=testing tbl=agile_projects\n",
      "22/12/28 14:26:45 INFO audit: ugi=audacious\tip=unknown-ip-addr\tcmd=get_table : db=testing tbl=agile_projects\t\n",
      "22/12/28 14:26:45 INFO HiveMetaStore: 0: get_table : db=testing tbl=agile_projects\n",
      "22/12/28 14:26:45 INFO audit: ugi=audacious\tip=unknown-ip-addr\tcmd=get_table : db=testing tbl=agile_projects\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+-------------------+\n",
      "|ap_id|          project_id|        project_name|          created_by|       created_date|\n",
      "+-----+--------------------+--------------------+--------------------+-------------------+\n",
      "|   20|1990881e7026a350f...|           RFPIndia1|425928e3a6d568200...|               null|\n",
      "|    1|2569c09de916f62f4...|  DigitalCertificate|190ac748f5f9310dc...|               null|\n",
      "|   26|2811af1fc14f5a257...|        RAJinsurance|27afb6922e9369be1...|2021-09-15 02:15:35|\n",
      "|    2|37745c0dbe2932523...|Datacentermanagem...|a3ea12a367c8957e9...|               null|\n",
      "|   23|39cef4d7809bc490a...|            TayoTest|ecf46c527996d04bf...|2021-08-19 01:35:54|\n",
      "+-----+--------------------+--------------------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agile = sparkSession.read.table('testing.agile_projects')\n",
    "agile.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "538b6516",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/28 14:26:33 INFO HiveMetaStore: 0: get_table : db=testing tbl=agile_projects\n",
      "22/12/28 14:26:33 INFO audit: ugi=audacious\tip=unknown-ip-addr\tcmd=get_table : db=testing tbl=agile_projects\t\n",
      "22/12/28 14:26:33 INFO HiveMetaStore: 0: get_database: testing\n",
      "22/12/28 14:26:33 INFO audit: ugi=audacious\tip=unknown-ip-addr\tcmd=get_database: testing\t\n",
      "22/12/28 14:26:33 INFO HiveMetaStore: 0: get_table : db=testing tbl=agile_projects\n",
      "22/12/28 14:26:33 INFO audit: ugi=audacious\tip=unknown-ip-addr\tcmd=get_table : db=testing tbl=agile_projects\t\n",
      "22/12/28 14:26:33 INFO HiveMetaStore: 0: get_table : db=testing tbl=agile_projects\n",
      "22/12/28 14:26:33 INFO audit: ugi=audacious\tip=unknown-ip-addr\tcmd=get_table : db=testing tbl=agile_projects\t\n",
      "22/12/28 14:26:33 INFO HiveMetaStore: 0: get_database: testing\n",
      "22/12/28 14:26:33 INFO audit: ugi=audacious\tip=unknown-ip-addr\tcmd=get_database: testing\t\n",
      "22/12/28 14:26:33 INFO HiveMetaStore: 0: get_table : db=testing tbl=agile_projects\n",
      "22/12/28 14:26:33 INFO audit: ugi=audacious\tip=unknown-ip-addr\tcmd=get_table : db=testing tbl=agile_projects\t\n",
      "22/12/28 14:26:33 INFO HiveMetaStore: 0: get_database: testing\n",
      "22/12/28 14:26:33 INFO audit: ugi=audacious\tip=unknown-ip-addr\tcmd=get_database: testing\t\n",
      "22/12/28 14:26:33 INFO HiveMetaStore: 0: get_table : db=testing tbl=agile_projects\n",
      "22/12/28 14:26:33 INFO audit: ugi=audacious\tip=unknown-ip-addr\tcmd=get_table : db=testing tbl=agile_projects\t\n",
      "22/12/28 14:26:33 INFO HiveMetaStore: 0: drop_table : db=testing tbl=agile_projects\n",
      "22/12/28 14:26:33 INFO audit: ugi=audacious\tip=unknown-ip-addr\tcmd=drop_table : db=testing tbl=agile_projects\t\n",
      "22/12/28 14:26:34 INFO HiveMetaStore: 0: get_table : db=testing tbl=agile_projects\n",
      "22/12/28 14:26:34 INFO audit: ugi=audacious\tip=unknown-ip-addr\tcmd=get_table : db=testing tbl=agile_projects\t\n",
      "22/12/28 14:26:34 INFO HiveMetaStore: 0: get_database: testing\n",
      "22/12/28 14:26:34 INFO audit: ugi=audacious\tip=unknown-ip-addr\tcmd=get_database: testing\t\n",
      "22/12/28 14:26:34 INFO HiveMetaStore: 0: get_database: testing\n",
      "22/12/28 14:26:34 INFO audit: ugi=audacious\tip=unknown-ip-addr\tcmd=get_database: testing\t\n",
      "22/12/28 14:26:34 INFO HiveMetaStore: 0: get_database: testing\n",
      "22/12/28 14:26:34 INFO audit: ugi=audacious\tip=unknown-ip-addr\tcmd=get_database: testing\t\n",
      "22/12/28 14:26:34 INFO HiveMetaStore: 0: get_database: testing\n",
      "22/12/28 14:26:34 INFO audit: ugi=audacious\tip=unknown-ip-addr\tcmd=get_database: testing\t\n",
      "22/12/28 14:26:35 INFO HiveMetaStore: 0: get_database: testing                  \n",
      "22/12/28 14:26:35 INFO audit: ugi=audacious\tip=unknown-ip-addr\tcmd=get_database: testing\t\n",
      "22/12/28 14:26:35 INFO HiveMetaStore: 0: get_table : db=testing tbl=agile_projects\n",
      "22/12/28 14:26:35 INFO audit: ugi=audacious\tip=unknown-ip-addr\tcmd=get_table : db=testing tbl=agile_projects\t\n",
      "22/12/28 14:26:35 INFO HiveMetaStore: 0: get_database: testing\n",
      "22/12/28 14:26:35 INFO audit: ugi=audacious\tip=unknown-ip-addr\tcmd=get_database: testing\t\n",
      "22/12/28 14:26:35 INFO HiveMetaStore: 0: get_table : db=testing tbl=agile_projects\n",
      "22/12/28 14:26:35 INFO audit: ugi=audacious\tip=unknown-ip-addr\tcmd=get_table : db=testing tbl=agile_projects\t\n",
      "22/12/28 14:26:35 INFO HiveMetaStore: 0: create_table: Table(tableName:agile_projects, dbName:testing, owner:audacious, createTime:1672217794, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:ap_id, type:int, comment:null), FieldSchema(name:project_id, type:string, comment:null), FieldSchema(name:project_name, type:string, comment:null), FieldSchema(name:created_by, type:string, comment:null), FieldSchema(name:created_date, type:timestamp, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=hdfs://localhost:9000/user/hive/warehouse/testing.db/agile_projects, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema={\"type\":\"struct\",\"fields\":[{\"name\":\"ap_id\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{\"scale\":0}},{\"name\":\"project_id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{\"scale\":0}},{\"name\":\"project_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{\"scale\":0}},{\"name\":\"created_by\",\"type\":\"string\",\"nullable\":true,\"metadata\":{\"scale\":0}},{\"name\":\"created_date\",\"type\":\"timestamp\",\"nullable\":true,\"metadata\":{\"scale\":0}}]}, spark.sql.sources.provider=parquet, spark.sql.create.version=3.3.1}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{audacious=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:audacious, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:audacious, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:audacious, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:audacious, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))\n",
      "22/12/28 14:26:35 INFO audit: ugi=audacious\tip=unknown-ip-addr\tcmd=create_table: Table(tableName:agile_projects, dbName:testing, owner:audacious, createTime:1672217794, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:ap_id, type:int, comment:null), FieldSchema(name:project_id, type:string, comment:null), FieldSchema(name:project_name, type:string, comment:null), FieldSchema(name:created_by, type:string, comment:null), FieldSchema(name:created_date, type:timestamp, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=hdfs://localhost:9000/user/hive/warehouse/testing.db/agile_projects, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema={\"type\":\"struct\",\"fields\":[{\"name\":\"ap_id\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{\"scale\":0}},{\"name\":\"project_id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{\"scale\":0}},{\"name\":\"project_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{\"scale\":0}},{\"name\":\"created_by\",\"type\":\"string\",\"nullable\":true,\"metadata\":{\"scale\":0}},{\"name\":\"created_date\",\"type\":\"timestamp\",\"nullable\":true,\"metadata\":{\"scale\":0}}]}, spark.sql.sources.provider=parquet, spark.sql.create.version=3.3.1}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{audacious=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:audacious, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:audacious, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:audacious, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:audacious, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))\t\n",
      "22/12/28 14:26:35 INFO log: Updating table stats fast for agile_projects\n",
      "22/12/28 14:26:35 INFO log: Updated size of table agile_projects to 4775\n"
     ]
    }
   ],
   "source": [
    "## save as hive table\n",
    "agile_projects.write.saveAsTable('testing.agile_projects',  mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4e9eb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
