{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e8f87fd",
   "metadata": {},
   "source": [
    "### The Power of Pairs: Paired RDDs\n",
    "\n",
    "Key/value pairs are good for solving many problems efficiently in a parallel fashion. Apache Mahout, a machine-learning library that was initially developed on top of Apache Hadoop, implements many machine-learning algorithms in the areas of classification, clustering, and collaborative filtering by using the MapReduce key/value-pair architecture . In this chapter, you’ll work through recipes that develop skills for solving interesting big data problems from many disciplines.\n",
    "\n",
    "#### Create a Paired RDD\n",
    "#### Problem\n",
    "\n",
    "You want to create a paired RDD.\n",
    "#### Solution\n",
    "You have an RDD, RDD1. The elements of RDD1 are b, d, m, t, e, and u. You want to create a paired RDD, in which the keys are elements of a single RDD, and the value of a key is 0 if the element is a consonant, or1 if the element is a vowel. Figure 5-1 clearly depicts the requirements.\n",
    "\n",
    "<img src = '430628_1_En_5_Fig1_HTML.gif'>\n",
    "\n",
    "                                               Figure 5-1.\n",
    "\n",
    "#### Creating a paired RDD\n",
    "\n",
    "A paired RDD can be created in many ways. One way is to read data directly from files. We’ll explore this method in an upcoming chapter. Another way to create a paired RDD is by using the map() method, which you’ll learn about in this recipe.\n",
    "#### How It Works\n",
    "\n",
    "In this section, you’ll follow several steps to reach the solution.\n",
    "#### Creating an RDD with Single Elements\n",
    "\n",
    "Let’s start by creating an RDD out of our given data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50d20825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/23 14:43:58 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.lang.Thread.run(Thread.java:750)\n",
      "22/12/23 14:43:59 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dbc5674",
   "metadata": {},
   "outputs": [],
   "source": [
    "pythonList  =  ['b', 'd', 'm', 't', 'e', 'u']\n",
    "RDD1 = sc.parallelize(pythonList, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea076c0",
   "metadata": {},
   "source": [
    "We have created an RDD named RDD1. The elements of RDD1 are b, d, m, t, e, and u. This is an RDD of letters. It can be observed that the elements b, d, m, and t are consonants. The other elements of RDD1, e and u, are vowels .\n",
    "\n",
    "#### Writing a Python Method to Check for Consonants\n",
    "\n",
    "We are going to define a Python function named vowelCheckFunction(). This function will take a letter as input and return 1 if the input is a consonant, or 0 if it is not. Let’s implement the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2316a7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['b', 'd', 'm', 't', 'e', 'u']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5970c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vowelCheckFunction( data) :\n",
    "     if data in ['a','e','i','o','u']:\n",
    "        return 1\n",
    "     else :\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0fc5a9",
   "metadata": {},
   "source": [
    "#### Creating a Paired RDD\n",
    "\n",
    "We can create our required RDD by using the map() function. We have to create a paired RDD: the keys will be the elements of RDD1, and the value will be 0 for keys that are consonants, or 1 for keys that are vowels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97db7375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('b', 0), ('d', 0), ('m', 0), ('t', 0), ('e', 1), ('u', 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairedRdd = RDD1.map(lambda data: (data, vowelCheckFunction(data)))\n",
    "pairedRdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782d7f90",
   "metadata": {},
   "source": [
    "#### Fetching Keys from a Paired RDD\n",
    "\n",
    "The keys() function can be used to fetch all the keys:\n",
    "\n",
    "\n",
    "We can see that the keys() function performs a transformation. Therefore, keys() returns an RDD that requires the collect() function to get the data to the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83279c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b', 'd', 'm', 't', 'e', 'u']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddkeys = pairedRdd.keys()\n",
    "rddkeys.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d843e88b",
   "metadata": {},
   "source": [
    "#### Fetching Values from a Paired RDD\n",
    "\n",
    "Similar to the keys() function, the values() function will fetch all the values from a paired RDD. It also performs a transformation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "749863bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 1, 1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddvalues = pairedRdd.values()\n",
    "rddvalues.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb3dd99",
   "metadata": {},
   "source": [
    "#### Aggregate Data\n",
    "#### Problem\n",
    "\n",
    "You want to aggregate data.\n",
    "#### Solution\n",
    "You want to perform data aggregation on data from a lightbulb manufacturer, as shown in Table 5-1.\n",
    "\n",
    "<img src = '430628_1_En_5_Figa_HTML.gif'>\n",
    "                      \n",
    "                                           Table 5-1. Filament Data\n",
    "\n",
    "Company YP manufactures two types of filaments : filamentA and filamentB. 100W and 200W electric bulbs can be manufactured from both filaments. Table 5-1 indicates the expected life of each bulb.\n",
    "You want to calculate the following:\n",
    "    \n",
    "    1.Mean life in hours for bulbs of each filament type\n",
    "    2. Mean life in hours for bulbs of each power level\n",
    "    3.Mean life in hours based on both filament type and power\n",
    "\n",
    "We generally encounter aggregation of data in data-science problems. To get an aggregation of data, we can use many PySpark functions.\n",
    "\n",
    "In this recipe, we’ll use the reduceByKey() function to calculate the mean by using keys. Calculating the mean of complex keys requires creating those complex keys. Complex keys can be created by using the map() function.\n",
    "How It Works\n",
    "\n",
    "Let’s start with the creation of the RDD.\n",
    "#### Creating an RDD with Single Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6792b794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['filamentA', '100W', 605],\n",
       " ['filamentB', '100W', 683],\n",
       " ['filamentB', '100W', 691]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filDataSingle = [['filamentA','100W',605],\n",
    "                  ['filamentB','100W',683],\n",
    "                  ['filamentB','100W',691],\n",
    "                  ['filamentB','200W',561],\n",
    "                  ['filamentA','200W',530],\n",
    "                  ['filamentA','100W',619],\n",
    "                  ['filamentB','100W',686],\n",
    "                  ['filamentB','200W',600],\n",
    "                  ['filamentB','100W',696],\n",
    "                  ['filamentA','200W',579],\n",
    "                  ['filamentA','200W',520],\n",
    "                  ['filamentA','100W',622],\n",
    "                  ['filamentA','100W',668],\n",
    "                  ['filamentB','200W',569],\n",
    "                  ['filamentB','200W',555],\n",
    "                  ['filamentA','200W',541]]\n",
    "filDataSingleRDD = sc.parallelize(filDataSingle,2)\n",
    "filDataSingleRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7007ea",
   "metadata": {},
   "source": [
    "#### Creating a Paired RDD\n",
    "\n",
    "First we have to calculate the mean lifetime of bulbs, based on their filament type. Better that we are creating a paired RDD with keys for the filament type and values for the life in hours. So let’s create our required paired RDD and then investigate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d0749a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('filamentA', 605),\n",
       " ('filamentB', 683),\n",
       " ('filamentB', 691),\n",
       " ('filamentB', 561),\n",
       " ('filamentA', 530),\n",
       " ('filamentA', 619),\n",
       " ('filamentB', 686),\n",
       " ('filamentB', 600),\n",
       " ('filamentB', 696),\n",
       " ('filamentA', 579),\n",
       " ('filamentA', 520),\n",
       " ('filamentA', 622),\n",
       " ('filamentA', 668),\n",
       " ('filamentB', 569),\n",
       " ('filamentB', 555),\n",
       " ('filamentA', 541)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fildatapairedrdd1 = filDataSingleRDD.map(lambda data: (data[0], data[2]))\n",
    "fildatapairedrdd1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466ec96f",
   "metadata": {},
   "source": [
    "We have created a paired RDD, filDataPairedRDD1, by using the map() function defined on the RDD. The paired RDD filDataPairedRDD1 has the filament type as the key, and the life in hours as the value.\n",
    "#### Finding the Mean Lifetime Based on Filament Type\n",
    "\n",
    "Now we have our required paired RDD. But is this all we need? No. To calculate the mean, we need a sum and a count. We have to add an extra 1 in our paired RDD so that we can get a sum and a count. So let’s add an extra 1 now to each RDD element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7edc5d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('filamentA', [605, 1]),\n",
       " ('filamentB', [683, 1]),\n",
       " ('filamentB', [691, 1]),\n",
       " ('filamentB', [561, 1]),\n",
       " ('filamentA', [530, 1]),\n",
       " ('filamentA', [619, 1]),\n",
       " ('filamentB', [686, 1]),\n",
       " ('filamentB', [600, 1]),\n",
       " ('filamentB', [696, 1]),\n",
       " ('filamentA', [579, 1]),\n",
       " ('filamentA', [520, 1]),\n",
       " ('filamentA', [622, 1]),\n",
       " ('filamentA', [668, 1]),\n",
       " ('filamentB', [569, 1]),\n",
       " ('filamentB', [555, 1]),\n",
       " ('filamentA', [541, 1])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fildatapairedrdd2 = fildatapairedrdd1.map(lambda data: (data[0], [data[1], 1]))\n",
    "fildatapairedrdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf60681",
   "metadata": {},
   "source": [
    " filDataPairedRDD11 is a paired RDD. The values of filDataPairedRDD11 are presented as a list; the first element is the lifetime of the bulb (in hours), and the second element is just 1.\n",
    "\n",
    "Now we have to calculate the sum of the values of the lifetimes for each filament type as well as the count value, so that we can calculate the mean. Many PySpark functions could be used to do this job, but here we are going to use the reduceByKey() function for paired RDDs.\n",
    "\n",
    "The reduceByKey() function applies aggregation operators key wise. It takes an aggregation function as input and applies that function on the values of each RDD key.\n",
    "\n",
    "Let’s calculate the sum of the total life hours of bulbs based on the filament type, and the count of elements for each filament type :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "757920f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('filamentB', [5041, 8]), ('filamentA', [4684, 8])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fildatapairedrdd3 = fildatapairedrdd2.reduceByKey(lambda data, data2: [(data[0] + data2[0]), (data[1] + data2[1])])\n",
    "fildatapairedrdd3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2938303c",
   "metadata": {},
   "source": [
    "Finally, we have the summation of the life hours of bulbs and the count, based on filament type. The next step is to divide the sum by the count to get the mean value. Let’s do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50fd6af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['filamentB', 630.125], ['filamentA', 585.5]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fildatapairedrdd4 = fildatapairedrdd3.map(lambda data: [data[0], data[1][0]/ data[1][1]])\n",
    "fildatapairedrdd4.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c065f4",
   "metadata": {},
   "source": [
    "Finally, we have our required mean, based on filament type. The mean lifetime of filamentA is 585.5 hours, and the mean lifetime of filamentB is 630.125 hours. We can infer that filamentB has a longer life than filamentA.\n",
    "#### Finding the Mean Lifetime Based on Bulb Power\n",
    "\n",
    "First, we will start with creating our paired RDD. The key will be the bulb power, and the value will be the life in hours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc7a5e1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('100W', 605),\n",
       " ('100W', 683),\n",
       " ('100W', 691),\n",
       " ('200W', 561),\n",
       " ('200W', 530),\n",
       " ('100W', 619),\n",
       " ('100W', 686),\n",
       " ('200W', 600),\n",
       " ('100W', 696),\n",
       " ('200W', 579),\n",
       " ('200W', 520),\n",
       " ('100W', 622),\n",
       " ('100W', 668),\n",
       " ('200W', 569),\n",
       " ('200W', 555),\n",
       " ('200W', 541)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fildatapairedrdd_power1 = filDataSingleRDD.map(lambda data: (data[1], data[2]))\n",
    "fildatapairedrdd_power1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68c026e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('100W', [605, 1]),\n",
       " ('100W', [683, 1]),\n",
       " ('100W', [691, 1]),\n",
       " ('200W', [561, 1]),\n",
       " ('200W', [530, 1]),\n",
       " ('100W', [619, 1]),\n",
       " ('100W', [686, 1]),\n",
       " ('200W', [600, 1]),\n",
       " ('100W', [696, 1]),\n",
       " ('200W', [579, 1]),\n",
       " ('200W', [520, 1]),\n",
       " ('100W', [622, 1]),\n",
       " ('100W', [668, 1]),\n",
       " ('200W', [569, 1]),\n",
       " ('200W', [555, 1]),\n",
       " ('200W', [541, 1])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fildatapairedrdd_power2 = fildatapairedrdd_power1.map(lambda data: (data[0], [data[1], 1]))\n",
    "fildatapairedrdd_power2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fadfbe3",
   "metadata": {},
   "source": [
    "Now we have included 1 in the value part of the RDD. Therefore, each value is a list that consists of the life in hours and a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b956c9eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('100W', [5270, 8]), ('200W', [4455, 8])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fildatapairedrdd_power3 = fildatapairedrdd_power2.reduceByKey(lambda data, data1: [data[0] + data1[0], data[1] + data1[1]])\n",
    "fildatapairedrdd_power3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a414c36c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['100W', 658.75], ['200W', 556.875]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fildatapairedrdd_power4 = fildatapairedrdd_power3.map(lambda data: [data[0], data[1][0]/ data[1][1]])\n",
    "fildatapairedrdd_power4.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1486e6",
   "metadata": {},
   "source": [
    "In this last step, we have computed the mean and the count. From the result, we can infer that the mean life of 100W bulbs is longer than that of 200W bulbs.\n",
    "#### Finding the Mean Lifetime Based on Filament Type and Power\n",
    "\n",
    "To solve this part of the exercise, we need a paired RDD with keys that are complex. You might be wondering what a complex key is. Complex keys have more than one type. In our case, our complex key will have both the filament type and bulb power type. Let’s start creating our paired RDD with a complex key type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "469edfaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('filamentA', '100W'), 605],\n",
       " [('filamentB', '100W'), 683],\n",
       " [('filamentB', '100W'), 691],\n",
       " [('filamentB', '200W'), 561],\n",
       " [('filamentA', '200W'), 530],\n",
       " [('filamentA', '100W'), 619],\n",
       " [('filamentB', '100W'), 686],\n",
       " [('filamentB', '200W'), 600],\n",
       " [('filamentB', '100W'), 696],\n",
       " [('filamentA', '200W'), 579],\n",
       " [('filamentA', '200W'), 520],\n",
       " [('filamentA', '100W'), 622],\n",
       " [('filamentA', '100W'), 668],\n",
       " [('filamentB', '200W'), 569],\n",
       " [('filamentB', '200W'), 555],\n",
       " [('filamentA', '200W'), 541]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fildatacomplexkeyrdd1 = filDataSingleRDD.map(lambda data: [(data[0], data[1]), data[2]])\n",
    "fildatacomplexkeyrdd1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b35a10",
   "metadata": {},
   "source": [
    "We have created a paired RDD named filDataComplexKeyData. It can be easily observed that it has complex keys. The keys are a combination of filament type and bulb power. The rest of the exercise will move as in the previous step. In the following code, we are going to include an extra 1 in the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e95cf0d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('filamentA', '100W'), [605, 1]],\n",
       " [('filamentB', '100W'), [683, 1]],\n",
       " [('filamentB', '100W'), [691, 1]],\n",
       " [('filamentB', '200W'), [561, 1]],\n",
       " [('filamentA', '200W'), [530, 1]],\n",
       " [('filamentA', '100W'), [619, 1]],\n",
       " [('filamentB', '100W'), [686, 1]],\n",
       " [('filamentB', '200W'), [600, 1]],\n",
       " [('filamentB', '100W'), [696, 1]],\n",
       " [('filamentA', '200W'), [579, 1]],\n",
       " [('filamentA', '200W'), [520, 1]],\n",
       " [('filamentA', '100W'), [622, 1]],\n",
       " [('filamentA', '100W'), [668, 1]],\n",
       " [('filamentB', '200W'), [569, 1]],\n",
       " [('filamentB', '200W'), [555, 1]],\n",
       " [('filamentA', '200W'), [541, 1]]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fildatacomplexkeyrdd2 = fildatacomplexkeyrdd1.map(lambda data: [data[0], [data[1], 1]])\n",
    "fildatacomplexkeyrdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f895d5a",
   "metadata": {},
   "source": [
    "Our required paired RDD, filDataComplexKeyData1 , has been created. Now we can apply the reduceByKey() function to get the sum and count, based on the complex keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18583fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('filamentB', '100W'), [2756, 4]),\n",
       " (('filamentA', '200W'), [2170, 4]),\n",
       " (('filamentA', '100W'), [2514, 4]),\n",
       " (('filamentB', '200W'), [2285, 4])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fildatacomplexkeyrdd3 = fildatacomplexkeyrdd2.reduceByKey(lambda data, data1: [data[0]+ data1[0], data[1] + data1[1]])\n",
    "fildatacomplexkeyrdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8293721d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('filamentB', '100W'), 689.0],\n",
       " [('filamentA', '200W'), 542.5],\n",
       " [('filamentA', '100W'), 628.5],\n",
       " [('filamentB', '200W'), 571.25]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fildatacomplexkeyrdd4 = fildatacomplexkeyrdd3.map(lambda data: [data[0], data[1][0]/data[1][1]])\n",
    "fildatacomplexkeyrdd4.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0c0abc",
   "metadata": {},
   "source": [
    "#### Join Data\n",
    "#### Problem\n",
    "\n",
    "You want to join data.\n",
    "#### Solution\n",
    "We have been given two tables: a Students table (Table 5-2) and a Subjects table (Table 5-3).\n",
    "<img src='430628_1_En_5_Figb_HTML.gif'>\n",
    "   \n",
    "                                        Table 5-2. Students\n",
    "<img src='430628_1_En_5_Figc_HTML.gif'>\n",
    "\n",
    "                                        Table 5-3. Subjects\n",
    "    \n",
    "You want to perform the following on the Students and Subjects tables:\n",
    "\n",
    "    Inner join\n",
    "\n",
    "    Left outer join\n",
    "\n",
    "    Right outer join\n",
    "\n",
    "    Full outer join\n",
    "\n",
    "Joining data tables is an integral part of data preprocessing. We are going to perform four types of data joins in this recipe.\n",
    "\n",
    "An inner join returns all the keys that are common to both tables. It discards the key elements that are not common to both tables. In PySpark, an inner join is done by using the join() method defined on the RDD.\n",
    "\n",
    "A left outer join includes all keys in the left table and excludes uncommon keys from the right table. A left outer join can be performed by using the leftOuterJoin() function defined on the RDD in PySpark.\n",
    "\n",
    "Another important type of join is a right outer join. In a right outer join, every key of the second table is included, but from the first table, only those keys that are common to both tables are included. We can do a right outer join by using the rightOuterJoin() function in PySpark.\n",
    "\n",
    "If you want to include all keys from both tables, go for a full outer join. It can be performed by using fullOuterJoin().\n",
    "#### How It Works\n",
    "\n",
    "We’ll follow the steps in this section to work with joins.\n",
    "#### Creating Nested Lists\n",
    "\n",
    "Let’s start creating a nested list of our data from the Students table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c459cd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "studentData = [['si1','Robin','M'],\n",
    "                ['si2','Maria','F'],\n",
    "                ['si3','Julie','F'],\n",
    "                ['si4','Bob',  'M'],\n",
    "                ['si6','William','M']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fd8809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjectsData = [['si1','Python'],\n",
    "                 ['si3','Java'],\n",
    "                 ['si1','Java'],\n",
    "                 ['si2','Python'],\n",
    "                 ['si3','Ruby'],\n",
    "                 ['si4','C++'],\n",
    "                 ['si5','C'],\n",
    "                 ['si4','Python'],\n",
    "                 ['si2','Java']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211e2d9e",
   "metadata": {},
   "source": [
    "#### Creating a Paired RDD of Students and Subjects\n",
    "\n",
    "Before creating a paired RDD, we first have to create a single RDD. Let’s create studentRDD:\n",
    "\n",
    "\n",
    "We can see that, every element of the studentRDD RDD is a list, and each list has three elements. Now we have to transform it into a paired RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9946d071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['si1', 'Robin', 'M'],\n",
       " ['si2', 'Maria', 'F'],\n",
       " ['si3', 'Julie', 'F'],\n",
       " ['si4', 'Bob', 'M'],\n",
       " ['si6', 'William', 'M']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "studentDataRDD = sc.parallelize(studentData, 2)\n",
    "studentDataRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "03694d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['si1', 'Python'],\n",
       " ['si3', 'Java'],\n",
       " ['si1', 'Java'],\n",
       " ['si2', 'Python'],\n",
       " ['si3', 'Ruby'],\n",
       " ['si4', 'C++'],\n",
       " ['si5', 'C'],\n",
       " ['si4', 'Python'],\n",
       " ['si2', 'Java']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjectsDataRDD = sc.parallelize(subjectsData, 2)\n",
    "subjectsDataRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c078f1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('si1', ['Robin', 'M']),\n",
       " ('si2', ['Maria', 'F']),\n",
       " ('si3', ['Julie', 'F']),\n",
       " ('si4', ['Bob', 'M']),\n",
       " ('si6', ['William', 'M'])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "studentDataPairedRDD = studentDataRDD.map(lambda data: (data[0], [data[1], data[2]]))\n",
    "studentDataPairedRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3acaca40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('si1', 'Python'),\n",
       " ('si3', 'Java'),\n",
       " ('si1', 'Java'),\n",
       " ('si2', 'Python'),\n",
       " ('si3', 'Ruby'),\n",
       " ('si4', 'C++'),\n",
       " ('si5', 'C'),\n",
       " ('si4', 'Python'),\n",
       " ('si2', 'Java')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjectsDataPairedRDD = subjectsDataRDD.map(lambda data: (data[0],data[1]))\n",
    "subjectsDataPairedRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ad1d67",
   "metadata": {},
   "source": [
    "#### Performing an Inner Join\n",
    "\n",
    "As we know, an inner join in PySpark is done by using the join() function. We have to apply this function on the paired RDD studentPairedRDD, and provide subjectsPairedRDD as an argument to the join() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1317ea51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('si4', (['Bob', 'M'], 'C++')),\n",
       " ('si4', (['Bob', 'M'], 'Python')),\n",
       " ('si3', (['Julie', 'F'], 'Java')),\n",
       " ('si3', (['Julie', 'F'], 'Ruby')),\n",
       " ('si1', (['Robin', 'M'], 'Python')),\n",
       " ('si1', (['Robin', 'M'], 'Java')),\n",
       " ('si2', (['Maria', 'F'], 'Python')),\n",
       " ('si2', (['Maria', 'F'], 'Java'))]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "studentsubject_inner = studentDataPairedRDD.join(subjectsDataPairedRDD)\n",
    "studentsubject_inner.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90b24b6",
   "metadata": {},
   "source": [
    "Analyzing the output of this inner join reveals that the key part contains only keys that are common to the Students and Subjects tables; these appear in the joined table. The keys that are not common to both tables are not the part of joined table.\n",
    "#### Performing a Left Outer Join\n",
    "\n",
    "A left outer join can be performed by using the leftOuterJoin() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "33fb6b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('si4', (['Bob', 'M'], 'C++')),\n",
       " ('si4', (['Bob', 'M'], 'Python')),\n",
       " ('si6', (['William', 'M'], None)),\n",
       " ('si3', (['Julie', 'F'], 'Java')),\n",
       " ('si3', (['Julie', 'F'], 'Ruby')),\n",
       " ('si1', (['Robin', 'M'], 'Python')),\n",
       " ('si1', (['Robin', 'M'], 'Java')),\n",
       " ('si2', (['Maria', 'F'], 'Python')),\n",
       " ('si2', (['Maria', 'F'], 'Java'))]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "studentsubject_leftouterjoin = studentDataPairedRDD.leftOuterJoin(subjectsDataPairedRDD)\n",
    "studentsubject_leftouterjoin.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ea3c1a",
   "metadata": {},
   "source": [
    "Student ID si6 is in the Students table but not in the Subjects table. Hence, the left outer join includes si6 in the joined table. Because si6 doesn’t have its counterpart in the Subjects table, it has None in place of the subject.\n",
    "#### Performing a Right Outer Join\n",
    "\n",
    "A right outer join on the Students and Subjects tables can be performed by using the rightOuterJoin() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6993c9b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('si4', (['Bob', 'M'], 'C++')),\n",
       " ('si4', (['Bob', 'M'], 'Python')),\n",
       " ('si3', (['Julie', 'F'], 'Java')),\n",
       " ('si3', (['Julie', 'F'], 'Ruby')),\n",
       " ('si5', (None, 'C')),\n",
       " ('si1', (['Robin', 'M'], 'Python')),\n",
       " ('si1', (['Robin', 'M'], 'Java')),\n",
       " ('si2', (['Maria', 'F'], 'Python')),\n",
       " ('si2', (['Maria', 'F'], 'Java'))]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "studentsubject_rightouterjoin = studentDataPairedRDD.rightOuterJoin(subjectsDataPairedRDD)\n",
    "studentsubject_rightouterjoin.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4f27e9",
   "metadata": {},
   "source": [
    "Student ID si5 is in only the Subjects table; it is not part of the Students table. Therefore, it appears in the joined table.\n",
    "#### Performing a Full Outer Join\n",
    "\n",
    "Now let’s perform a full outer join. In a full outer join, keys from both tables will be included:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6bf33625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('si4', (['Bob', 'M'], 'C++')),\n",
       " ('si4', (['Bob', 'M'], 'Python')),\n",
       " ('si6', (['William', 'M'], None)),\n",
       " ('si3', (['Julie', 'F'], 'Java')),\n",
       " ('si3', (['Julie', 'F'], 'Ruby')),\n",
       " ('si5', (None, 'C')),\n",
       " ('si1', (['Robin', 'M'], 'Python')),\n",
       " ('si1', (['Robin', 'M'], 'Java')),\n",
       " ('si2', (['Maria', 'F'], 'Python')),\n",
       " ('si2', (['Maria', 'F'], 'Java'))]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/23 18:41:17 WARN DataStreamer: Exception for BP-241050457-127.0.0.1-1670841776195:blk_1073744908_4087\n",
      "java.net.SocketTimeoutException: 65000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/127.0.0.1:56322 remote=/127.0.0.1:9866]\n",
      "\tat org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)\n",
      "\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n",
      "\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n",
      "\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:118)\n",
      "\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n",
      "\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:519)\n",
      "\tat org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:213)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1137)\n",
      "22/12/23 18:41:45 WARN HeartbeatReceiver: Removing executor 1 with no recent heartbeats: 6089751 ms exceeds timeout 120000 ms\n",
      "22/12/23 18:43:45 ERROR Utils: Uncaught exception in thread kill-executor-thread\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [120 seconds]. This timeout is controlled by spark.rpc.askTimeout\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.killExecutors(CoarseGrainedSchedulerBackend.scala:890)\n",
      "\tat org.apache.spark.SparkContext.killAndReplaceExecutor(SparkContext.scala:1825)\n",
      "\tat org.apache.spark.HeartbeatReceiver$$anon$2.$anonfun$run$2(HeartbeatReceiver.scala:214)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1484)\n",
      "\tat org.apache.spark.HeartbeatReceiver$$anon$2.run(HeartbeatReceiver.scala:211)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [120 seconds]\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:293)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\t... 10 more\n",
      "22/12/23 18:43:45 ERROR YarnSchedulerBackend$YarnSchedulerEndpoint: Sending KillExecutors(List(1)) to AM was unsuccessful\n",
      "org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from /192.168.87.202:54442 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat scala.util.Failure.recover(Try.scala:234)\n",
      "\tat scala.concurrent.Future.$anonfun$recover$1(Future.scala:395)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.tryFailure(Promise.scala:112)\n",
      "\tat scala.concurrent.Promise.tryFailure$(Promise.scala:112)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:214)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:264)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from /192.168.87.202:54442 in 120 seconds\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:265)\n",
      "\t... 7 more\n",
      "22/12/23 18:43:45 WARN NettyRpcEnv: Ignored failure: org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from /192.168.87.202:54442 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout\n"
     ]
    }
   ],
   "source": [
    "studentsubject_fullouterjoin = studentDataPairedRDD.fullOuterJoin(subjectsDataPairedRDD)\n",
    "studentsubject_fullouterjoin.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c695297b",
   "metadata": {},
   "source": [
    "In the joined table, keys from both tables have been included. Student ID si6 is part of only the Students data, and it appears in the joined table. Similarly, student ID si5 is part of only the Subjects table, but it appears in our joined table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c965ef9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
