{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d2acc0c",
   "metadata": {},
   "source": [
    "#### PySpark Broadcast Variables\n",
    "\n",
    "In PySpark RDD and DataFrame, Broadcast variables are read-only shared variables that are cached and available on all nodes in a cluster in-order to access or use by the tasks. Instead of sending this data along with every task, PySpark distributes broadcast variables to the workers using efficient broadcast algorithms to reduce communication costs.\n",
    "\n",
    "### Use case\n",
    "\n",
    "Let me explain with an example when to use broadcast variables, assume you are getting a two-letter country state code in a file and you wanted to transform it to full state name, (for example CA to California, NY to New York e.t.c) by doing a lookup to reference mapping. In some instances, this data could be large and you may have many such lookups (like zip code e.t.c). \n",
    "\n",
    "Instead of distributing this information along with each task over the network (overhead and time consuming), we can use the broadcast variable to cache this lookup info on each machine and tasks use this cached info while executing the transformations.\n",
    "#### How does PySpark Broadcast work?\n",
    "\n",
    "Broadcast variables are used in the same way for RDD, DataFrame.\n",
    "When you run a PySpark RDD, DataFrame applications that have the Broadcast variables defined and used, PySpark does the following.\n",
    "\n",
    "1. PySpark breaks the job into stages that have distributed shuffling and actions are executed with in the stage.\n",
    "2. Later Stages are also broken into tasks\n",
    "3. Spark broadcasts the common data (reusable) needed by tasks within each stage.\n",
    "4. he broadcasted data is cache in serialized format and deserialized before executing each task.\n",
    "\n",
    "You should be creating and using broadcast variables for data that shared across multiple stages and tasks.\n",
    "\n",
    "Note that broadcast variables are not sent to executors with sc.broadcast(variable) call instead, they will be sent to executors when they are first used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85ee818b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/12/27 15:34:35 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe2ab616",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcast_variable = sc.broadcast(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0ab79e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'California'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcast_variable.value['CA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36a4bcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcast_array = sc.broadcast([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cb4505b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcast_array.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e771bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([('a',1),('b',2),('a',4),('b',3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64b13c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(val):\n",
    "    if 'a' == val[0]:\n",
    "        return broadcast_array.value\n",
    "    else: return val[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09e5b3b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', [1, 2, 3, 4]), ('b', 2), ('a', [1, 2, 3, 4]), ('b', 3)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda data: (data[0], fun(data))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2284061",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa11d3f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('James', 'Smith', 'USA', 'CA'),\n",
       " ('Michael', 'Rose', 'USA', 'NY'),\n",
       " ('Robert', 'Williams', 'USA', 'CA')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = sc.parallelize(data)\n",
    "rdd2.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a0f7181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun2(val):\n",
    "    return broadcast_variable.value[val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abc10585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('James - Smith', 'USA', 'California'),\n",
       " ('Michael - Rose', 'USA', 'New York'),\n",
       " ('Robert - Williams', 'USA', 'California'),\n",
       " ('Maria - Jones', 'USA', 'Florida')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.map(lambda data: (\"{} - {}\".format(data[0], data[1]), data[2], fun2(data[3]))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bdc472",
   "metadata": {},
   "source": [
    "#### pyspark.Broadcast.destroyÂ¶\n",
    "Destroy all data and metadata related to this broadcast variable. Use this with caution; once a broadcast variable has been destroyed, it cannot be used again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb11671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcast_variable.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4639311c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/27 15:56:35 ERROR Utils: Exception encountered\n",
      "org.apache.spark.SparkException: Attempted to use Broadcast(1) after it was destroyed (destroy at NativeMethodAccessorImpl.java:0) \n",
      "\tat org.apache.spark.broadcast.Broadcast.assertValid(Broadcast.scala:144)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$writeObject$1(TorrentBroadcast.scala:223)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1470)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.writeObject(TorrentBroadcast.scala:222)\n",
      "\tat sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1154)\n",
      "\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\n",
      "\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n",
      "\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n",
      "\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n",
      "\tat java.util.ArrayList.writeObject(ArrayList.java:768)\n",
      "\tat sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1154)\n",
      "\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\n",
      "\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n",
      "\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n",
      "\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n",
      "\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n",
      "\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n",
      "\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n",
      "\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n",
      "\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n",
      "\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n",
      "\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n",
      "\tat java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)\n",
      "\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)\n",
      "\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n",
      "\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n",
      "\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n",
      "\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n",
      "\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n",
      "\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n",
      "\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115)\n",
      "\tat org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:441)\n",
      "\tat org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:416)\n",
      "\tat org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:163)\n",
      "\tat org.apache.spark.SparkContext.clean(SparkContext.scala:2491)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Task not serializable\n\tat org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:444)\n\tat org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:416)\n\tat org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:163)\n\tat org.apache.spark.SparkContext.clean(SparkContext.scala:2491)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: org.apache.spark.SparkException: Attempted to use Broadcast(1) after it was destroyed (destroy at NativeMethodAccessorImpl.java:0) \n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1477)\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeObject(TorrentBroadcast.scala:222)\n\tat sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1154)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat java.util.ArrayList.writeObject(ArrayList.java:768)\n\tat sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1154)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115)\n\tat org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:441)\n\t... 24 more\nCaused by: org.apache.spark.SparkException: Attempted to use Broadcast(1) after it was destroyed (destroy at NativeMethodAccessorImpl.java:0) \n\tat org.apache.spark.broadcast.Broadcast.assertValid(Broadcast.scala:144)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$writeObject$1(TorrentBroadcast.scala:223)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1470)\n\t... 59 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28663/1833130917.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrdd2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"{} - {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1195\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Task not serializable\n\tat org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:444)\n\tat org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:416)\n\tat org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:163)\n\tat org.apache.spark.SparkContext.clean(SparkContext.scala:2491)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: org.apache.spark.SparkException: Attempted to use Broadcast(1) after it was destroyed (destroy at NativeMethodAccessorImpl.java:0) \n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1477)\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeObject(TorrentBroadcast.scala:222)\n\tat sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1154)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat java.util.ArrayList.writeObject(ArrayList.java:768)\n\tat sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1154)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115)\n\tat org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:441)\n\t... 24 more\nCaused by: org.apache.spark.SparkException: Attempted to use Broadcast(1) after it was destroyed (destroy at NativeMethodAccessorImpl.java:0) \n\tat org.apache.spark.broadcast.Broadcast.assertValid(Broadcast.scala:144)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$writeObject$1(TorrentBroadcast.scala:223)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1470)\n\t... 59 more\n"
     ]
    }
   ],
   "source": [
    "rdd2.map(lambda data: (\"{} - {}\".format(data[0], data[1]), data[2], fun2(data[3]))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6499df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## will get error after destroying variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44371d46",
   "metadata": {},
   "source": [
    "https://sparkbyexamples.com/spark/broadcast-join-in-spark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3812b71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "studentData = [['si1','Robin','M'],\n",
    "                ['si2','Maria','F'],\n",
    "                ['si3','Julie','F'],\n",
    "                ['si4','Bob',  'M'],\n",
    "                ['si6','William','M']]\n",
    "\n",
    "subjectsData = [['si1','Python'],\n",
    "                 ['si3','Java'],\n",
    "                 ['si1','Java'],\n",
    "                 ['si2','Python'],\n",
    "                 ['si3','Ruby'],\n",
    "                 ['si4','C++'],\n",
    "                 ['si4','Python'],\n",
    "                 ['si2','Java']]\n",
    "\n",
    "studentrdd = sc.parallelize(studentData, 2)\n",
    "subjectrdd = sc.parallelize(subjectsData, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90573792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a949315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.broadcast(studentrdd.map(lambda data:(data[0], [data[1], data[2]])).collectAsMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5308f1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'si1': ['Robin', 'M'],\n",
       " 'si2': ['Maria', 'F'],\n",
       " 'si3': ['Julie', 'F'],\n",
       " 'si4': ['Bob', 'M'],\n",
       " 'si6': ['William', 'M']}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8e9881b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Robin', 'M']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fun3(val):\n",
    "    \n",
    "    return data.value[val]\n",
    "fun3('si1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "52e127db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('si1', 'Python', ['Robin', 'M']),\n",
       " ('si3', 'Java', ['Julie', 'F']),\n",
       " ('si1', 'Java', ['Robin', 'M']),\n",
       " ('si2', 'Python', ['Maria', 'F']),\n",
       " ('si3', 'Ruby', ['Julie', 'F']),\n",
       " ('si4', 'C++', ['Bob', 'M']),\n",
       " ('si4', 'Python', ['Bob', 'M']),\n",
       " ('si2', 'Java', ['Maria', 'F'])]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjectrdd.map(lambda data: (data[0], data[1], fun3(data[0]))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e51d5f",
   "metadata": {},
   "source": [
    "#### PySpark Accumulator with Example\n",
    "\n",
    "The PySpark Accumulator is a shared variable that is used with RDD and DataFrame to perform sum and counter operations similar to Map-reduce counters. These variables are shared by all executors to update and add information through aggregation or computative operations.\n",
    "\n",
    "In this article, Iâve explained what is PySpark Accumulator, how to create, and using it on RDD and DataFrame with an example.\n",
    "#### What is PySpark Accumulator?\n",
    "\n",
    "Accumulators are write-only and initialize once variables where only tasks that are running on workers are allowed to update and updates from the workers get propagated automatically to the driver program. But, only the driver program is allowed to access the Accumulator variable using the value property.\n",
    "How to create Accumulator variable in PySpark?\n",
    "\n",
    "Using accumulator() from SparkContext class we can create an Accumulator in PySpark programming. Users can also create Accumulators for custom types using AccumulatorParam class of PySpark.\n",
    "\n",
    "Some points to note..\n",
    "\n",
    "1. sparkContext.accumulator() is used to define accumulator variables.\n",
    "2. add() function is used to add/update a value in accumulator\n",
    "3. value property on the accumulator variable is used to retrieve the value from the accumulator.\n",
    "\n",
    "We can create Accumulators in PySpark for primitive types int and float. Users can also create Accumulators for custom types using AccumulatorParam class of PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e6334640",
   "metadata": {},
   "outputs": [],
   "source": [
    "accum = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "29e80387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "951ea7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "accum.add(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "07964668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "078588a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(0,10))\n",
    "rdd.foreach(lambda data: accum.add(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c8cefbd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a2d18a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spark Context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "88a98c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'application_1672135434203_0003'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.applicationId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "214e3f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pyspark-shell'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.appName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d9c60e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9dfd776e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultMinPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e7230e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.emptyRDD()\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "803604c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PYTHONHASHSEED': '0'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "42c484bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.getCheckpointDir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "40e68811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7fd0519a6ad0>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "42d3797a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yarn'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "52a5a82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.sparkHome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "91182c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SparkContext.stop of <SparkContext master=yarn appName=pyspark-shell>>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0906aa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setLogLevel('INFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e98b1fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.1'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "82342ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "sf = SparkConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bb7bf2c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf.contains('master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3abb4dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.get('master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b562e83b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.eventLog.enabled', 'true'),\n",
       " ('spark.history.fs.logDirectory', 'hdfs://localhost:9000/spark-logs'),\n",
       " ('spark.history.ui.port', '18080'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1672135434203_0003'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-XX:+PrintGCDetails -Dkey=value -Dnumbers=\"one two three\"'),\n",
       " ('spark.app.submitTime', '1672135472013'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('spark.driver.memory', '512m'),\n",
       " ('spark.sql.warehouse.dir', 'hdfs://localhost:9000/user/hive/warehouse'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.history.provider',\n",
       "  'org.apache.spark.deploy.history.FsHistoryProvider'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.history.fs.update.interval', '10s'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.eventLog.dir', 'hdfs://localhost:9000/spark-logs')]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e4b028d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7fd0510c1060>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf.set('spark.history.fs.update.interval','11s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b93bf677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11s'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf.get('spark.history.fs.update.interval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7d252349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7fd0510c1060>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf.setAppName('okay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0e22e331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.eventLog.enabled', 'true'),\n",
       " ('spark.history.fs.logDirectory', 'hdfs://localhost:9000/spark-logs'),\n",
       " ('spark.history.ui.port', '18080'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1672135434203_0003'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-XX:+PrintGCDetails -Dkey=value -Dnumbers=\"one two three\"'),\n",
       " ('spark.app.submitTime', '1672135472013'),\n",
       " ('spark.driver.memory', '512m'),\n",
       " ('spark.sql.warehouse.dir', 'hdfs://localhost:9000/user/hive/warehouse'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.history.provider',\n",
       "  'org.apache.spark.deploy.history.FsHistoryProvider'),\n",
       " ('spark.history.fs.update.interval', '11s'),\n",
       " ('spark.app.name', 'okay'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.eventLog.dir', 'hdfs://localhost:9000/spark-logs')]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3055ae22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7fd0510c1060>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf.setAll([('spark.eventLog.enabled', 'true'),\n",
    " ('spark.history.fs.logDirectory', 'hdfs://localhost:9000/spark-logs'),\n",
    " ('spark.history.ui.port', '18080'),\n",
    " ('spark.ui.proxyBase', '/proxy/application_1672135434203_0003'),\n",
    " ('spark.executor.extraJavaOptions',\n",
    "  '-XX:+PrintGCDetails -Dkey=value -Dnumbers=\"one two three\"'),\n",
    " ('spark.app.submitTime', '1672135472013'),\n",
    " ('spark.driver.memory', '512m'),\n",
    " ('spark.sql.warehouse.dir', 'hdfs://localhost:9000/user/hive/warehouse'),\n",
    " ('spark.master', 'yarn'),\n",
    " ('spark.history.provider',\n",
    "  'org.apache.spark.deploy.history.FsHistoryProvider'),\n",
    " ('spark.history.fs.update.interval', '11s'),\n",
    " ('spark.app.name', 'okay'),\n",
    " ('spark.submit.pyFiles', ''),\n",
    " ('spark.yarn.isPython', 'true'),\n",
    " ('spark.submit.deployMode', 'client'),\n",
    " ('spark.ui.showConsoleProgress', 'true'),\n",
    " ('spark.eventLog.dir', 'hdfs://localhost:9000/spark-logs')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4d34ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052474f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
