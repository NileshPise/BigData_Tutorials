{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817d0208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "sparkSession = SparkSession.builder.config(conf=SparkConf() \\\n",
    "                                          .setAppName('Health Data Generator') \\\n",
    "                                          .setMaster('local[4]')) \\\n",
    "                                    .enableHiveSupport() \\\n",
    "                                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e49321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2946403",
   "metadata": {},
   "source": [
    "https://blog.knoldus.com/spark-structured-streaming-part-4-handling-late-data/\n",
    "\n",
    "https://medium.com/towardsdataanalytics/spark-streaming-vs-structured-streaming-ef6863d5b60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ead0533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read text from socket\n",
    "socketDF = sparkSession \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a5fbe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([StructField('Transaction_ID', StringType(), True) \\\n",
    "            , StructField('Transaction_Date', StringType(), True) \\\n",
    "            , StructField('Transaction_Value', IntegerType(), True) \\\n",
    "            , StructField('Transaction_Segment', StringType(), True) \\\n",
    "            , StructField('Credit_Card_ID', StringType(), True)])\n",
    "\n",
    "df = sparkSession.readStream.schema(schema) \\\n",
    "            .csv('/Spark_streaming/TransactionBase/')\n",
    "\n",
    "df = df.select('*').filter((df.Transaction_Value < 10000))\n",
    "\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf4ba14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([StructField('Transaction_ID', StringType(), True) \\\n",
    "            , StructField('Transaction_Date', StringType(), True) \\\n",
    "            , StructField('Transaction_Value', IntegerType(), True) \\\n",
    "            , StructField('Transaction_Segment', StringType(), True) \\\n",
    "            , StructField('Credit_Card_ID', StringType(), True)])\n",
    "\n",
    "df = sparkSession.readStream.format('csv') \\\n",
    "            .schema(schema) \\\n",
    "            .option('path','/Spark_streaming/TransactionBase/') \\\n",
    "            .load()\n",
    "\n",
    "df = df.select('*').filter((df.Transaction_Value < 10000))\n",
    "\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e9fa97",
   "metadata": {},
   "source": [
    "#### latestFirst: \n",
    "whether to process the latest new files first, useful when there is a large backlog of files (default: false) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a61b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('Transaction_ID', StringType(), True) \\\n",
    "            , StructField('Transaction_Date', StringType(), True) \\\n",
    "            , StructField('Transaction_Value', IntegerType(), True) \\\n",
    "            , StructField('Transaction_Segment', StringType(), True) \\\n",
    "            , StructField('Credit_Card_ID', StringType(), True)])\n",
    "\n",
    "df = sparkSession.readStream.format('csv') \\\n",
    "            .schema(schema) \\\n",
    "            .option('latestFirst', True) \\\n",
    "            .option('path','/Spark_streaming/TransactionBase/') \\\n",
    "            .load()\n",
    "\n",
    "df = df.select('*').filter((df.Transaction_Value < 10000))\n",
    "\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82645498",
   "metadata": {},
   "source": [
    "#### ResolveWriteToStream\n",
    "Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-d9e44aaa-c124-4532-adad-1f36ff03d8c3. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
    "\n",
    "ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
    "\n",
    "FileStreamSource: 'latestFirst' is true. New files will be processed first, which may affect the watermark\n",
    "value. In addition, 'maxFileAge' will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40b09e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([StructField('Transaction_ID', StringType(), True) \\\n",
    "            , StructField('Transaction_Date', StringType(), True) \\\n",
    "            , StructField('Transaction_Value', IntegerType(), True) \\\n",
    "            , StructField('Transaction_Segment', StringType(), True) \\\n",
    "            , StructField('Credit_Card_ID', StringType(), True)])\n",
    "\n",
    "df = sparkSession.readStream.format('csv') \\\n",
    "            .schema(schema) \\\n",
    "            .option('latestFirst', True) \\\n",
    "            .option('spark.sql.streaming.forceDeleteTempCheckpointLocation',True) \\\n",
    "            .option('path','/Spark_streaming/TransactionBase/') \\\n",
    "            .load()\n",
    "\n",
    "df = df.select('*').filter((df.Transaction_Value < 10000))\n",
    "\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ca033f",
   "metadata": {},
   "source": [
    "#### cleanSource\n",
    "cleanSource: option to clean up completed files after processing.\n",
    "\n",
    "Available options are \"archive\", \"delete\", \"off\". If the option is not provided, the default value is \"off\".\n",
    "When \"archive\" is provided, additional option sourceArchiveDir must be provided as well. The value of \"sourceArchiveDir\" must not match with source pattern in depth (the number of directories from the root directory), where the depth is minimum of depth on both paths. This will ensure archived files are never included as new source files.\n",
    "\n",
    "For example, suppose you provide '/hello?/spark/*' as source pattern, '/hello1/spark/archive/dir' cannot be used as the value of \"sourceArchiveDir\", as '/hello?/spark/*' and '/hello1/spark/archive' will be matched. '/hello1/spark' cannot be also used as the value of \"sourceArchiveDir\", as '/hello?/spark' and '/hello1/spark' will be matched. '/archived/here' would be OK as it doesn't match.\n",
    "\n",
    "Spark will move source files respecting their own path. For example, if the path of source file is /a/b/dataset.txt and the path of archive directory is /archived/here, file will be moved to /archived/here/a/b/dataset.txt.\n",
    "NOTE: Both archiving (via moving) or deleting completed files will introduce overhead (slow down, even if it's happening in separate thread) in each micro-batch, so you need to understand the cost for each operation in your file system before enabling this option. On the other hand, enabling this option will reduce the cost to list source files which can be an expensive operation.\n",
    "\n",
    "Number of threads used in completed file cleaner can be configured with spark.sql.streaming.fileSource.cleaner.numThreads (default: 1).\n",
    "\n",
    "NOTE 2: The source path should not be used from multiple sources or queries when enabling this option. Similarly, you must ensure the source path doesn't match to any files in output directory of file stream sink.\n",
    "\n",
    "NOTE 3: Both delete and move actions are best effort. Failing to delete or move files will not fail the streaming query. Spark may not clean up some source files in some circumstances - e.g. the application doesn't shut down gracefully, too many files are queued to clean up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fd8168",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([StructField('Transaction_ID', StringType(), True) \\\n",
    "            , StructField('Transaction_Date', StringType(), True) \\\n",
    "            , StructField('Transaction_Value', IntegerType(), True) \\\n",
    "            , StructField('Transaction_Segment', StringType(), True) \\\n",
    "            , StructField('Credit_Card_ID', StringType(), True)])\n",
    "\n",
    "df = sparkSession.readStream.format('csv') \\\n",
    "            .schema(schema) \\\n",
    "            .option('cleanSource','archive') \\\n",
    "            .option('sourceArchiveDir', '/Spark_archive/') \\\n",
    "            .option(\"checkpointLocation\",\"/Spark_checkpointing_dir/\") \\\n",
    "            .option('latestFirst', True) \\\n",
    "            .option('spark.sql.streaming.forceDeleteTempCheckpointLocation',True) \\\n",
    "            .option('path','/Spark_streaming/TransactionBase/') \\\n",
    "            .load()\n",
    "\n",
    "df = df.select('*').filter((df.Transaction_Value < 10000))\n",
    "\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d588a071",
   "metadata": {},
   "source": [
    "path: path to the input directory, and common to all file formats.\n",
    "maxFilesPerTrigger: maximum number of new files to be considered in every trigger (default: no max)\n",
    "latestFirst: whether to process the latest new files first, useful when there is a large backlog of files (default: false)\n",
    "\n",
    "fileNameOnly: whether to check new files based on only the filename instead of on the full path (default: false). With this set to `true`, the following files would be considered as the same file, because their filenames, \"dataset.txt\", are the same:\n",
    "\n",
    "\"file:///dataset.txt\"\n",
    "\"s3://a/dataset.txt\"\n",
    "\"s3n://a/b/dataset.txt\"\n",
    "\"s3a://a/b/c/dataset.txt\"\n",
    "\n",
    "maxFileAge: Maximum age of a file that can be found in this directory, before it is ignored. For the first batch all files will be considered valid. If latestFirst is set to `true` and maxFilesPerTrigger is set, then this parameter will be ignored, because old files that are valid, and should be processed, may be ignored. The max age is specified with respect to the timestamp of the latest file, and not the timestamp of the current system.(default: 1 week) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613d5bce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([StructField('_1', StringType(), True) \\\n",
    "                     , StructField('_2', StringType(), True), StructField('_3', StringType(), True) \\\n",
    "                     , StructField('_4', StringType(), True), StructField('_5', StringType(), True)])\n",
    "\n",
    "df = sparkSession.readStream.schema(schema) \\\n",
    "                            .parquet('/Spark_streaming/TransactionBase_parquet/') \\\n",
    "                    .toDF('Transaction_ID','Transaction_Date','Transaction_Value' \\\n",
    "                          ,'Transaction_Segment','Credit_Card_ID')\n",
    "\n",
    "df = df.select('*').filter((df.Transaction_Value > 10))\n",
    "\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090903a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('_1', StringType(), True) \\\n",
    "                     , StructField('_2', StringType(), True), StructField('_3', StringType(), True) \\\n",
    "                     , StructField('_4', StringType(), True), StructField('_5', StringType(), True)])\n",
    "\n",
    "df = sparkSession.readStream.schema(schema) \\\n",
    "                            .format('parquet') \\\n",
    "                            .option('path','/Spark_streaming/TransactionBase_parquet/') \\\n",
    "                            .load()\\\n",
    "                    .toDF('Transaction_ID','Transaction_Date','Transaction_Value' \\\n",
    "                          ,'Transaction_Segment','Credit_Card_ID')\n",
    "\n",
    "df = df.select('*').filter((df.Transaction_Value > 10))\n",
    "\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5378d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('_1', StringType(), True) \\\n",
    "                     , StructField('_2', StringType(), True), StructField('_3', StringType(), True) \\\n",
    "                     , StructField('_4', StringType(), True), StructField('_5', StringType(), True)])\n",
    "\n",
    "df = sparkSession.readStream.schema(schema) \\\n",
    "                            .format('parquet') \\\n",
    "                            .option('cleanSource','archive') \\\n",
    "                            .option('sourceArchiveDir', '/Spark_archive/') \\\n",
    "                            .option(\"checkpointLocation\",\"/Spark_checkpointing_dir/\") \\\n",
    "                            .option('latestFirst', True) \\\n",
    "                            .option('spark.sql.streaming.forceDeleteTempCheckpointLocation',True) \\\n",
    "                            .option('path','/Spark_streaming/TransactionBase_parquet/') \\\n",
    "                            .load()\\\n",
    "                    .toDF('Transaction_ID','Transaction_Date','Transaction_Value' \\\n",
    "                          ,'Transaction_Segment','Credit_Card_ID')\n",
    "\n",
    "df = df.select('*').filter((df.Transaction_Value > 10))\n",
    "\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0648707a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = sparkSession.readStream.table(\"testing.health\")\n",
    "\n",
    "df = df.select('*').filter((df.Transaction_Value < 2000))\n",
    "\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d863f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = data.writeStream.outputMode(\"append\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"checkpointLocation\",\"/Spark_checkpointing_dir/\") \\\n",
    "    .option(\"path\", \"/Spark_streaming/Trans_parquet/\") \\\n",
    "    .start()\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88399a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('Transaction_ID', StringType(), True) \\\n",
    "            , StructField('Transaction_Date', StringType(), True) \\\n",
    "            , StructField('Transaction_Value', IntegerType(), True) \\\n",
    "            , StructField('Transaction_Segment', StringType(), True) \\\n",
    "            , StructField('Credit_Card_ID', StringType(), True)])\n",
    "\n",
    "df = sparkSession.readStream.format('csv') \\\n",
    "            .schema(schema) \\\n",
    "            .option('cleanSource','archive') \\\n",
    "            .option('sourceArchiveDir', '/Spark_archive/') \\\n",
    "            .option(\"checkpointLocation\",\"/Spark_checkpointing_dir/\") \\\n",
    "            .option('latestFirst', True) \\\n",
    "            .option('spark.sql.streaming.forceDeleteTempCheckpointLocation',True) \\\n",
    "            .option('path','/Spark_streaming/TransactionBase/') \\\n",
    "            .load()\n",
    "\n",
    "df = df.select('*').filter((df.Transaction_Value < 10000))\n",
    "\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\",\"/Spark_checkpointing_dir1/\") \\\n",
    "    .format(\"csv\") \\\n",
    "    .option('path','/Spark_streaming/Trans_csv/') \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8d5209",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([StructField('Transaction_ID', StringType(), True) \\\n",
    "            , StructField('Transaction_Date', StringType(), True) \\\n",
    "            , StructField('Transaction_Value', IntegerType(), True) \\\n",
    "            , StructField('Transaction_Segment', StringType(), True) \\\n",
    "            , StructField('Credit_Card_ID', StringType(), True)])\n",
    "\n",
    "df = sparkSession.readStream.format('csv') \\\n",
    "            .schema(schema) \\\n",
    "            .option('cleanSource','archive') \\\n",
    "            .option('sourceArchiveDir', '/Spark_archive/') \\\n",
    "            .option(\"checkpointLocation\",\"/Spark_checkpointing_dir/\") \\\n",
    "            .option('latestFirst', True) \\\n",
    "            .option('spark.sql.streaming.forceDeleteTempCheckpointLocation',True) \\\n",
    "            .option('path','/Spark_streaming/TransactionBase/') \\\n",
    "            .load()\n",
    "\n",
    "df = df.select('*').filter((df.Transaction_Value < 10000))\n",
    "\n",
    "query = df.writeStream.outputMode(\"append\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"checkpointLocation\",\"/Spark_checkpointing_dir2/\") \\\n",
    "    .option(\"path\", \"/Spark_streaming/Trans_parquet/\") \\\n",
    "    .start()\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c684973",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('Transaction_ID', StringType(), True) \\\n",
    "            , StructField('Transaction_Date', StringType(), True) \\\n",
    "            , StructField('Transaction_Value', IntegerType(), True) \\\n",
    "            , StructField('Transaction_Segment', StringType(), True) \\\n",
    "            , StructField('Credit_Card_ID', StringType(), True)])\n",
    "\n",
    "df = sparkSession.readStream.format('csv') \\\n",
    "            .schema(schema) \\\n",
    "            .option('cleanSource','archive') \\\n",
    "            .option('sourceArchiveDir', '/Spark_archive/') \\\n",
    "            .option(\"checkpointLocation\",\"/Spark_checkpointing_dir/\") \\\n",
    "            .option('latestFirst', True) \\\n",
    "            .option('spark.sql.streaming.forceDeleteTempCheckpointLocation',True) \\\n",
    "            .option('path','/Spark_streaming/TransactionBase/') \\\n",
    "            .load()\n",
    "\n",
    "df = df.select('*').filter((df.Transaction_Value < 10000))\n",
    "\n",
    "query = df.writeStream.outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\",\"/Spark_checkpointing_dir3/\") \\\n",
    "    .toTable('testing.transaction') \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc134ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = data \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0c9340",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
