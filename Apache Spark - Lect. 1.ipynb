{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5769fa77",
   "metadata": {},
   "source": [
    "# Spark Architecture and the Resilient Distributed Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f74203",
   "metadata": {},
   "source": [
    "You learned Python in the preceding chapter. Now it is time to learn PySpark and utilize the power of a distributed system to solve problems related to big data. We generally distribute large amounts of data on a cluster and perform processing on that distributed data. \n",
    "\n",
    "Learning about the architecture of Spark will be very helpful to your understanding of the various components of Spark. Before delving into the recipes let’s explore this topic.\n",
    "                                 \n",
    "                                 Figure 4-1 describes the Spark architecture.\n",
    "\n",
    "<img src='430628_1_En_4_Fig1_HTML.gif'>\n",
    "\n",
    "The main components of the Spark architecture are the driver and executors. For each PySpark application, there will be one driver program and one or more executors running on the cluster slave machine. You might be wondering, what is an application in the context of PySpark? An application is a whole bunch of code used to solve a problem.\n",
    "\n",
    "The driver is the process that coordinates with many executors running on various slave machines. Spark follows a master/slave architecture. The SparkContext object is created by the driver. SparkContext is the main entry point to a PySpark application. You will learn more about SparkContext in upcoming chapters. In this chapter, we will run our PySpark commands in the PySpark shell. After starting the shell, we will find that the SparkContext object is automatically created. We will encounter the SparkContext object in the PySpark shell as the variable sc. The shell itself is working as our driver. The driver breaks our application into small tasks; a task is the smallest unit of your application. Tasks are run on different executors in parallel. The driver is also responsible for scheduling tasks to different executors.\n",
    "\n",
    "Executors are slave processes. An executor runs tasks. It also has the capability to cache data in memory by using the BlockManager process. Each executor runs in its own Java Virtual Machine (JVM).\n",
    "\n",
    "The cluster manager manages cluster resources. The driver talks to the cluster manager to negotiate resources. The cluster manager also schedules tasks on behalf of the driver on various slave executor processes. PySpark is dispatched with Standalone Cluster Manager. PySpark can also be configured on YARN and Apache Mesos. In our recipes, you are going to see how to configure PySpark on Standalone Cluster Manager and Apache Mesos. On a single machine, PySpark can be started in local mode too.\n",
    "The main celebrated component of PySpark is the resilient distributed dataset (RDD). The RDD is a data abstraction over the distributed collection. Python collections such as lists, tuples, and sets can be distributed very easily. An RDD is recomputed on node failures. Only part of the data is calculated or recalculated, as required. An RDD is created using various functions defined in the SparkContext class. One important method for creating an RDD is parallelize(), which you will encounter again and again in this chapter. \n",
    "\n",
    "                           Figure 4-2 illustrates the creation of an RDD.\n",
    "\n",
    "<img src='430628_1_En_4_Fig2_HTML.gif'>\n",
    "\n",
    "Let’s say that we have a Python collection with the elements Data1, Data2, Data3, Data4, Data5, Data6, and Data7. This collection is distributed over the cluster to create an RDD. For simplicity, we can assume that two executors are running. Our collection is divided into two parts. The first executor gets the first part of the collection, which has the elements Data1, Data2, Data3, and Data4. The second part of the collection is sent to the second executor. So, the second executor has the data elements Data5, Data6, and Data7.\n",
    "\n",
    "We can perform two types of operations on the RDD: transformation and action . Transformation on an RDD returns another RDD. We know that RDDs are immutable; therefore, changing the RDD is impossible. Hence transformations always return another RDD. Transformations are lazy, whereas actions are eagerly evaluated. I say that the transformation is lazy because whenever a transformation is applied to an RDD, that operation is not applied to the data at the same time. Instead, PySpark notes the operation request, but all the transformations are applied when the first action is called.\n",
    "Figure 4-3 illustrates a transformation operation. The transformation on RDD1 creates RDD2. RDD1 has two partitions. The first partition of RDD1 has four data elements: Data1, Data2, Data3, and Data4. The second data partition of RDD1 has three elements: Data5, Data6, and Data7. After transformation on RDD1, RDD2 is created. RDD2 has six elements. So it is clear that the daughter RDD might have a different number of data elements than the father RDD. RDD2 also has two partitions. The first partition of RDD2 has three data points: Data8, Data9, and Data10. The second partition of RDD2 also has three elements: Data11, Data12, and Data13. Don’t get confused about the daughter RDD having a different number of partitions than the father RDD. \n",
    "\n",
    "<img src= '430628_1_En_4_Fig3_HTML.gif'>\n",
    "\n",
    "                                     Figure 4-3.RDD transformations\n",
    "Figure 4-4 illustrates an action performed on an RDD. In this example, we are applying the summation action. Summed data is returned to the driver. In other cases, the result of an action can be saved to a file or to another destination.\n",
    "\n",
    "<img src = '430628_1_En_4_Fig4_HTML.gif'>\n",
    "\n",
    "You might be wondering, if Spark has been written in Scala, then how is Python contacting with Scala? You might guess that a Python wrapper of PySpark has been written using Jython, and that this Jython code is compiled to Java bytecode and run on the JVM. This guess isn’t correct.\n",
    "\n",
    "A running Python program can access Java objects in a JVM by using Py4J. A running Java program can also access Python objects by using Py4J. A gateway between Python and Java enables Python to use Java objects.\n",
    "\n",
    "Driver programs use Py4J to communicate between Python and the Java SparkContext object. PySpark uses Py4J, so that PySpark Python code can\n",
    "\n",
    "On remote cluster machines, the PythonRDD object creates Python subprocesses and communicates with them using pipes. The PythonRDD object runs in JVM and communicates with Python processes by using pipes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99927e04",
   "metadata": {},
   "source": [
    "### Create an RDD\n",
    "#### Problem\n",
    "\n",
    "You want to create an RDD.\n",
    "#### Solution\n",
    "\n",
    "As we know, an RDD is a distributed collection. You have a list with the following data:\n",
    "\n",
    "pythonList = [2.3,3.4,4.3,2.4,2.3,4.0]\n",
    "\n",
    "You want to do the following operations:\n",
    "\n",
    "    Create an RDD of the list\n",
    "\n",
    "    Get the first element\n",
    "\n",
    "    Get the first two elements\n",
    "\n",
    "    Get the number of partitions in the RDD\n",
    "\n",
    "In PySpark, an RDD can be created in many ways. One way to create an RDD out of a given collection is to use the parallelize() function. The SparkContext object is used to call the parallelize() function. You’ll read more about SparkContext in an upcoming chapter.\n",
    "\n",
    "In the case of big data, even tabular data, a table might have more than 1,000 columns. Sometimes analysts want to see what those columns of data look like. The first() function is defined on an RDD and will return the first element of the RDD.\n",
    "\n",
    "To get more than one element from a list, we can use the take() function. The number of partitions of a collection can be fetched by using getNumPartitions().\n",
    "\n",
    "#### How It Works\n",
    "\n",
    "Let’s follow the steps in this section to solve the problem.\n",
    "##### Creating an RDD of the List\n",
    "\n",
    "Let’s first create a Python list by using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "514444da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pythonList = [2.3,3.4,4.3,2.4,2.3,4.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8a9a59",
   "metadata": {},
   "source": [
    "Parallelization or distribution of data is done using the parallelize() function. This function takes two arguments. The first argument is the collection to be parallelized, and the second argument indicates the number of distributed chunks of data you want:\n",
    "\n",
    "\n",
    "Using the parallelize() function, we have distributed our data in two partitions. In order to get all the data on the driver, we can use the collect() function, as shown in the following code line. Using the collect() function is not recommended in production; rather, it should be used only in code debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0fd6231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/12/19 15:20:34 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c66dbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "parPythonData = sc.parallelize(pythonList,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "331a2c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.3, 3.4, 4.3, 2.4, 2.3, 4.0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parPythonData.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eac167",
   "metadata": {},
   "source": [
    "#### Getting the First Element\n",
    "\n",
    "The first() function can be used to get the first data out of an RDD. You might have figured out that the collect() and first() functions perform actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e914f7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parPythonData.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f438dd",
   "metadata": {},
   "source": [
    "#### Getting the First Two Elements\n",
    "\n",
    "Sometimes data analysts want to see more than one row of data. The take() function can be used to fetch more than one row from an RDD. The number of rows you want is given as an argument to the take() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4f04080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.3, 3.4, 4.3]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parPythonData.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e24c0e9",
   "metadata": {},
   "source": [
    "#### Getting the Number of Partitions in the RDD\n",
    "\n",
    "In order to optimize PySpark code, a proper distribution of data is required. The number of partitions of an RDD can be found using the getNumPartitions() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d6854e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parPythonData.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577dd095",
   "metadata": {},
   "source": [
    "Recall that we were partitioning our data into two partitions while using the parallelize() function.\n",
    "#### Convert Temperature Data\n",
    "#### Problem\n",
    "\n",
    "You want to convert temperature data by writing a temperature unit conversion program on an RDD.\n",
    "#### Solution\n",
    "You are given daily temperatures in Fahrenheit. You want to perform some analysis on that data. But your new software takes input in Celsius. Therefore, you want to convert your temperature data from Fahrenheit to Celsius. Table 4-1 shows the data you have.\n",
    "\n",
    "<img src='430628_1_En_4_Figa_HTML.gif'>\n",
    "\n",
    "You want to do the following:\n",
    "\n",
    "    Convert temperature from Fahrenheit to Celsius\n",
    "\n",
    "    Get all the temperature data points greater than 13o C\n",
    "\n",
    "We can convert temperature from Fahrenheit to Celsius by using the following mathematical formula:\n",
    "oC = (oF – 32) × 5/9\n",
    "\n",
    "We can see that in PySpark, this is a transformation problem. We can achieve this task by using the map() function on the RDD.\n",
    "\n",
    "Getting all the temperatures greater than 13o C is a filtering problem. Filtering of data can be done by using the filter() function on the RDD.\n",
    "How It Works\n",
    "\n",
    "We’ll follow the steps in this section to complete the conversion and filtering exercises.\n",
    "Step 4-2-1. Parallelizing the Data\n",
    "\n",
    "We are going to parallelize data by using our parallelize() function. We are going to distribute our data in two partitions, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b65a7226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[59, 57.2, 53.6, 55.4, 51.8, 53.6, 55.4]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempData = [59,57.2,53.6,55.4,51.8,53.6,55.4]\n",
    "parTempData = sc.parallelize(tempData)\n",
    "parTempData.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c9d0ee",
   "metadata": {},
   "source": [
    "#### Converting Temperature from Fahrenheit to Celsius\n",
    "\n",
    "Now we are going to convert our temperature in Fahrenheit to Celsius. We’ll write a fahrenheitToCentigrade function, which will take the temperature in Fahrenheit and return a temperature in Celsius for a given input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e02081a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fahrenheitToCentigrade(temperature) :\n",
    "    centigrade = (temperature-32)*5/9\n",
    "    return centigrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4676ced5",
   "metadata": {},
   "source": [
    "\n",
    "Let’s test our fahrenheitToCentigrade function:\n",
    "We are providing 59 as the input in Fahrenheit. Our function returns a Celsius value of our Fahrenheit input; 59o F is equal to 15o C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99f64645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15.0, 14.000000000000002, 12.0, 13.0, 10.999999999999998, 12.0, 13.0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parCentigradeData = parTempData.map(fahrenheitToCentigrade)\n",
    "parCentigradeData.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcc1a5b",
   "metadata": {},
   "source": [
    "We have converted the given temperature to Celsius. Now let’s filter out all the temperatures greater than or equal to 13o C.\n",
    "#### Filtering Temperatures Greater than 13o C\n",
    "\n",
    "To filter data, we can use the filter() function on the RDD. We have to provide a predicate as input to the filter() function. A predicate is a function that tests a condition and returns True or False.\n",
    "\n",
    "Let’s define the predicate tempMoreThanThirteen, which will take a temperature value and return True if input is greater than or equal to 13:\n",
    "\n",
    "We are going to send our tempMoreThanThirteen function as input to the filter() function. The filter() function will iterate over each value in the parCentigradeData RDD. For each value, the tempMoreThanThirteen function will be applied. If the value is greater than or equal to 13, True will be returned. The value for which tempMoreThanThirteen returns True will come to filteredTemprature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b5d55de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tempMoreThanThirteen(temperature): return temperature >=13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "125c8451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15.0, 14.000000000000002, 13.0, 13.0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filteredTemprature = parCentigradeData.filter(tempMoreThanThirteen)\n",
    "filteredTemprature.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f8773c",
   "metadata": {},
   "source": [
    "We can replace our predicates by using the lambda function. Using a lambda function makes the code more readable. The following code line clearly depicts that the filter() function takes a predicate as input and returns True for all the values greater than or equal to 13:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ac4ed02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15.0, 14.000000000000002, 13.0, 13.0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filteredTemprature = parCentigradeData.filter(lambda x: x>=13)\n",
    "filteredTemprature.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624d2c89",
   "metadata": {},
   "source": [
    "We finally have four elements indicating a temperature that is either greater than or equal to 13. So now you understand the way to do basic analysis on data with PySpark .\n",
    "#### Perform Basic Data Manipulation\n",
    "#### Problem\n",
    "\n",
    "You want to do data manipulation and run aggregation operations.\n",
    "#### Solution\n",
    "In this recipe, you are given data indicating student grades for a two-year (four-semester) course. Seven students are enrolled in this course. Table 4-2 depicts two years of grade data, divided into semesters, for seven enrolled students.\n",
    "\n",
    "<img src = '430628_1_En_4_Figb_HTML.gif'>\n",
    "\n",
    "                                           Table 4-2. Student Grades\n",
    "\n",
    "You want to calculate the following:\n",
    "\n",
    "    Average grades per semester, each year, for each student\n",
    "\n",
    "    Top three students who have the highest average grades in the second year\n",
    "\n",
    "    Bottom three students who have the lowest average grades in the second year\n",
    "\n",
    "    All students who have earned more than an 80% average in the second semester of the second year\n",
    "\n",
    "Using the map() function is often helpful. In this example, the average grades per semester, for each year, can be calculated using map().\n",
    "\n",
    "It is a general data science problem to get the top k elements, such as the top k highly performing bonds. The PySpark takeOrdered() function is going to take the top k or top bottom elements from our RDD.\n",
    "\n",
    "Students who have earned more than 80% averages in the second year can be filtered using the filter() function.\n",
    "How It Works\n",
    "\n",
    "Let’s solve our problem in steps. We will start with creating an RDD of our data.\n",
    "Step 4-3-1. Making a List from a Given Table\n",
    "\n",
    "In this step, we’ll create a nested list . This means that each element of the list is a record, and each record is a list in itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e9b010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "studentMarksData = [[\"si1\",\"year1\",62.08,62.4],\n",
    " [\"si1\",\"year2\",75.94,76.75],\n",
    " [\"si2\",\"year1\",68.26,72.95],\n",
    " [\"si2\",\"year2\",85.49,75.8],\n",
    " [\"si3\",\"year1\",75.08,79.84],\n",
    " [\"si3\",\"year2\",54.98,87.72],\n",
    " [\"si4\",\"year1\",50.03,66.85],\n",
    " [\"si4\",\"year2\",71.26,69.77],\n",
    " [\"si5\",\"year1\",52.74,76.27],\n",
    " [\"si5\",\"year2\",50.39,68.58],\n",
    " [\"si6\",\"year1\",74.86,60.8],\n",
    " [\"si6\",\"year2\",58.29,62.38],\n",
    " [\"si7\",\"year1\",63.95,74.51],\n",
    " [\"si7\",\"year2\",66.69,56.92]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c350f8fc",
   "metadata": {},
   "source": [
    "#### Parallelizing the Data\n",
    "\n",
    "After parallelizing the data by using the parallelize() function, we will find that we have an RDD in which each element is a list itself:\n",
    "\n",
    "As we know, the collect() function takes the whole RDD to the driver. If the RDD size is very large, the driver may face a memory issue. In order to fetch k first elements of an RDD, we can use the take() function with n as input to take(). As an example, in the following line of code, we are fetching two elements of our RDD. Remember here that take() is an action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da1b6856",
   "metadata": {},
   "outputs": [],
   "source": [
    "studentMarksDataRDD = sc.parallelize(studentMarksData,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96a388c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['si1', 'year1', 62.08, 62.4], ['si1', 'year2', 75.94, 76.75]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "studentMarksDataRDD.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f68b52",
   "metadata": {},
   "source": [
    "#### Calculating Average Semester Grades\n",
    "\n",
    "Now let me explain what I want to do in the following code. Just consider the first element of the RDD. Our first element of the RDD is ['si1', 'year1', 62.08, 62.4], which is a list of four elements. Our work is to calculate the mean of grades from two semesters. In the first element, the mean is 0.5(62.08 + 62.4). We are going to use the map() function to get our solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "564de14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['si1', 'year1', 62.239999999999995],\n",
       " ['si1', 'year2', 76.345],\n",
       " ['si2', 'year1', 70.605],\n",
       " ['si2', 'year2', 80.645],\n",
       " ['si3', 'year1', 77.46000000000001],\n",
       " ['si3', 'year2', 71.35],\n",
       " ['si4', 'year1', 58.44],\n",
       " ['si4', 'year2', 70.515],\n",
       " ['si5', 'year1', 64.505],\n",
       " ['si5', 'year2', 59.485],\n",
       " ['si6', 'year1', 67.83],\n",
       " ['si6', 'year2', 60.335],\n",
       " ['si7', 'year1', 69.23],\n",
       " ['si7', 'year2', 61.805]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "studentMarksMean = studentMarksDataRDD.map(lambda x: [x[0],x[1],(x[2] + x[3])/2])\n",
    "studentMarksMean.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bd7193",
   "metadata": {},
   "source": [
    "#### Filtering Student Average Grades in the Second Year\n",
    "\n",
    "The following line of code is going to filter out all the data of the second year. We have implemented our predicate by using a lambda function. Our predicate function checks whether year2 is in the list. If the predicate returns True, the list includes second-year grades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7f85f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['si1', 'year2', 76.345],\n",
       " ['si2', 'year2', 80.645],\n",
       " ['si3', 'year2', 71.35],\n",
       " ['si4', 'year2', 70.515],\n",
       " ['si5', 'year2', 59.485],\n",
       " ['si6', 'year2', 60.335],\n",
       " ['si7', 'year2', 61.805]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secondYearMarks = studentMarksMean.filter(lambda x: \"year2\" in x[1])\n",
    "secondYearMarks.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659e245a",
   "metadata": {},
   "source": [
    "We can clearly see that the RDD output of secondYearMarks has only second-year grades.\n",
    "#### Finding the Top Three Students\n",
    "\n",
    "We can get the top three students in two ways. The first method is to sort the full data according to grades. Obviously, we are going to sort the data in decreasing order. Sorting is done by the sortBy() function. Let’s see the implementation:\n",
    "\n",
    "In our sortBy() function, we provide the keyfunc parameter. This parameter indicates to sort the grades data in decreasing order. Now collect the output and see the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1598d0dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['si5', 'year2', 59.485],\n",
       " ['si6', 'year2', 60.335],\n",
       " ['si7', 'year2', 61.805],\n",
       " ['si4', 'year2', 70.515],\n",
       " ['si3', 'year2', 71.35],\n",
       " ['si1', 'year2', 76.345],\n",
       " ['si2', 'year2', 80.645]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortedMarksData = secondYearMarks.sortBy(keyfunc = lambda x : x[2]) #asc\n",
    "# sortedMarksData = secondYearMarks.sortBy(keyfunc = lambda x : -x[2]) desc\n",
    "sortedMarksData.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9a3d777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['si5', 'year2', 59.485], ['si6', 'year2', 60.335], ['si7', 'year2', 61.805]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortedMarksData.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476c3f98",
   "metadata": {},
   "source": [
    "We have our answer. But can we optimize it further? In order to get top-three data, we are sorting the whole list. We can optimize this by using the takeOrdered() function. This function takes two arguments: the number of elements we require, and key, which uses a lambda function to determine how to take the data out.\n",
    "\n",
    "In the preceding code, we set num to 3 for the three top elements, and lambda in key so that it can provide three top in decreasing order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f853e461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['si2', 'year2', 80.645], ['si1', 'year2', 76.345], ['si3', 'year2', 71.35]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topThreeStudents = secondYearMarks.takeOrdered(num=3, key = lambda x :-x[2])\n",
    "topThreeStudents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26c4eaa",
   "metadata": {},
   "source": [
    "In order to print the result, we are not using the collect() function to get the data. Remember that transformation creates another RDD, so we require the collect() function to collect data. But an action will directly fetch the data to the driver, and collect() is not required. So you can conclude that the takeOrdered() function is an action.\n",
    "#### Finding the Bottom Three Students\n",
    "\n",
    "We have to find the bottom three students in terms of their average grades. One way is to sort the data in increasing order and take the three on top. But that is not an efficient way, so we will use the takeOrdered() function again, but with a different key parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e724e598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['si5', 'year2', 59.485], ['si6', 'year2', 60.335], ['si7', 'year2', 61.805]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottomThreeStudents = secondYearMarks.takeOrdered(num=3, key = lambda x :x[2])\n",
    "bottomThreeStudents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307179db",
   "metadata": {},
   "source": [
    "#### Getting All Students with 80% Averages\n",
    "\n",
    "Now that you understand the filter() function, it is easy to guess that we can solve this problem by using filter(). We will have to provide a predicate, which will return True if grades are greater than 80; otherwise, it returns False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d0bf7a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['si2', 'year2', 80.645]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moreThan80Marks = secondYearMarks.filter(lambda x: x[2]>80)\n",
    "moreThan80Marks.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b44d94",
   "metadata": {},
   "source": [
    "#### Run Set Operations\n",
    "#### Problem\n",
    "\n",
    "You want to run set operations on a research company’s data.\n",
    "#### Solution\n",
    "\n",
    "XYZ Research is a company that performs research on many diversified topics. Each research project comes with a research ID. Research may come to a conclusion in one year or may take more than one year. The following data is provided, indicating the number of research projects being conducted in three years:\n",
    "\n",
    "2001: RIN1, RIN2, RIN3, RIN4, RIN5, RIN6, RIN7\n",
    "\n",
    "2002: RIN3, RIN4, RIN7, RIN8, RIN9\n",
    "\n",
    "2003: RIN4, RIN8, RIN10, RIN11, RIN12\n",
    "\n",
    "\n",
    "Now we have to answer the following questions:\n",
    "\n",
    "    How many research projects were initiated in the three years?\n",
    "\n",
    "    How many projects were completed in the first year?\n",
    "\n",
    "    How many projects were completed in the first two years?\n",
    "\n",
    "A set is collection of distinct elements. PySpark performs pseudo set operations. They are called pseudo set operations because some functions do not remove duplicate elements.\n",
    "\n",
    "Remember, the first question is not asking about completed projects. The total number of research projects initiated in three years is just the union of all three years of data. You can perform a union on two RDDs by using the union() function.\n",
    "\n",
    "The projects that have been started in the first year and not in the second year are the projects that have been completed in the first year. Every project that is started is completed. We can use the subtract() function to find all the projects that were completed in the first year.\n",
    "\n",
    "If we make a union of first-year and second-year projects and subtract third-year projects, we are going to get all the projects that have been completed in the first two years.\n",
    "How It Works\n",
    "\n",
    "Let’s solve this problem step-by-step .\n",
    "#### Creating a List of Research Data by Year\n",
    "\n",
    "Let’s start with creating a list of all the projects that the company worked on each year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae9c9a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2001 = ['RIN1', 'RIN2', 'RIN3', 'RIN4', 'RIN5', 'RIN6', 'RIN7']\n",
    "data2002 = ['RIN3', 'RIN4', 'RIN7', 'RIN8', 'RIN9']\n",
    "data2003 = ['RIN4', 'RIN8', 'RIN10', 'RIN11', 'RIN12']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bce50e7",
   "metadata": {},
   "source": [
    " data2001 is list of all the projects started in 2001. Similarly, data2002 contains all the research projects that either are continuing from 2001 or started in 2002. The data2003data list contains all the projects that the company worked on in 2003.\n",
    "#### Parallelizing the Data (Creating the RDD)\n",
    "\n",
    "After creating lists, we have to parallelize our data:\n",
    "\n",
    "\n",
    "After parallelizing, we get three RDDs. The first RDD is parData2001, the second RDD is parData2002, and the last one is parData2003.\n",
    "#### Finding Projects Initiated in Three Years\n",
    "\n",
    "The total number of projects initiated in three years is determined just by getting the union of all the data for the given three years. RDD union() takes another RDD as input and returns, merging these two RDDs. Let’s see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4922d2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "parData2001 = sc.parallelize(data2001,2)\n",
    "parData2002 = sc.parallelize(data2002,2)\n",
    "parData2003 = sc.parallelize(data2003,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441f575b",
   "metadata": {},
   "source": [
    "We have calculated the union of different research projects initiated in either the first year or the second year. We can observe that the unionized data, unionOf20012002, has duplicate values. Having duplicates values in sets is not allowed. Therefore, a set operation on an RDD is also known as a pseudo set operation. Don’t worry; we will remove these duplicates.\n",
    "\n",
    "In order to get all the research projects that have been initiated in three years, we have to get the union of parData2003 and unionOf20012002:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0b0525fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RIN1',\n",
       " 'RIN2',\n",
       " 'RIN3',\n",
       " 'RIN4',\n",
       " 'RIN5',\n",
       " 'RIN6',\n",
       " 'RIN7',\n",
       " 'RIN3',\n",
       " 'RIN4',\n",
       " 'RIN7',\n",
       " 'RIN8',\n",
       " 'RIN9']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unionof01_02 = parData2001.union(parData2002)\n",
    "unionof01_02.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7a86a4c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RIN1',\n",
       " 'RIN2',\n",
       " 'RIN3',\n",
       " 'RIN4',\n",
       " 'RIN5',\n",
       " 'RIN6',\n",
       " 'RIN7',\n",
       " 'RIN3',\n",
       " 'RIN4',\n",
       " 'RIN7',\n",
       " 'RIN8',\n",
       " 'RIN9',\n",
       " 'RIN4',\n",
       " 'RIN8',\n",
       " 'RIN10',\n",
       " 'RIN11',\n",
       " 'RIN12']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_research = unionof01_02.union(parData2003)\n",
    "all_research.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18d8632",
   "metadata": {},
   "source": [
    "#### Making Sets of Distinct Data\n",
    "\n",
    "We are going to apply the distinct() function to our RDD allResearchs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ecbcb7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "allunique_research = all_research.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e7c50ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['RIN1',\n",
       " 'RIN10',\n",
       " 'RIN12',\n",
       " 'RIN2',\n",
       " 'RIN3',\n",
       " 'RIN5',\n",
       " 'RIN8',\n",
       " 'RIN4',\n",
       " 'RIN9',\n",
       " 'RIN11',\n",
       " 'RIN6',\n",
       " 'RIN7']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allunique_research.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4383a07f",
   "metadata": {},
   "source": [
    "We can see that we have all the research projects that were initiated in the first three years.\n",
    "#### Counting Distinct Elements\n",
    "\n",
    "Now count all the distinct research projects by using the count() function on the RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1c23fc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_research.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8624dfb5",
   "metadata": {},
   "source": [
    "#### Finding Projects Completed the First Year\n",
    "\n",
    "Let’s say we have two sets, A and B. Subtracting set B from set A will give us all the elements that are members of set A but not set B. So now it is clear that, in order to know all the projects that have been completed in the first year (2001), we have to subtract the projects in year 2002 from all the projects in year 2001.\n",
    "\n",
    "Subtraction on a set can be done with the subtract() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "44147047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RIN1', 'RIN2', 'RIN5', 'RIN6']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstYearCompletion = parData2001.subtract(parData2002)\n",
    "firstYearCompletion.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132c6366",
   "metadata": {},
   "source": [
    "We have all the projects that were completed in 2001. Four projects were completed in 2001.\n",
    "#### Finding Projects Completed in the First Two Years\n",
    "\n",
    "A union of RDDs gives us all the projects started in the first two years. After getting all the projects started in the first two years, if we then subtract projects running and started in the third year, we will return all the projects completed in the first two years. The following is the implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7bc5ccec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['RIN1', 'RIN2', 'RIN3', 'RIN5', 'RIN9', 'RIN6', 'RIN7']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first2yearCompletion = parData2001.union(parData2002).subtract(parData2003)\n",
    "first2yearCompletion.distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a760af9",
   "metadata": {},
   "source": [
    "Finding Projects Started in 2001 and Continued Through 2003.\n",
    "\n",
    "This step requires using the intersection() method defined in PySpark on the RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1c0627ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['RIN3', 'RIN7']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contuning = parData2001.intersection(parData2002).subtract(parData2003)\n",
    "contuning.distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67706e5",
   "metadata": {},
   "source": [
    "#### Calculate Summary Statistics\n",
    "#### Problem\n",
    "\n",
    "You want to calculate summary statistic s on given data.\n",
    "#### Solution\n",
    "\n",
    "Renewable energy sources are gaining in popularity all over the world. The company FindEnergy wants to install windmills at a given location. For efficient operation of windmills, the air requires certain characteristics.\n",
    "Data is collected as shown in \n",
    "                                 \n",
    "                                             Table 4-3.\n",
    " \n",
    "<img src = '430628_1_En_4_Figc_HTML.gif'>\n",
    "\n",
    "\n",
    "                                   Table 4-3. Air Velocity Data\n",
    "                                   \n",
    "You, as a data scientist, want to calculate the following quantities:\n",
    "\n",
    "    Number of data points\n",
    "\n",
    "    Summation of air velocities over a day\n",
    "\n",
    "    Mean air velocity in a day\n",
    "\n",
    "    Variance of air data\n",
    "\n",
    "    Sample variance of air data\n",
    "\n",
    "    Standard deviation of air data\n",
    "\n",
    "    Sample standard deviation of air data \n",
    "\n",
    "PySpark provides many functions to summarize data on the RDD. The number of elements in an RDD can be found by using the count() function on the RDD. There are two ways to sum all the data in a given RDD. The first is to apply the sum() method to the RDD. The second is to apply the reduce() function to the RDD.\n",
    "\n",
    "The mean represents the center point of the given data, and it can be calculated in two ways too. We are going to use the mean() method and the fold() method to calculate the mean.\n",
    "\n",
    "The variance, which indicates the spread of data around the mean, can be calculated using the variance() function. Similarly, the sample variance can be calculated by using the sampleVariance() method on the RDD.\n",
    "\n",
    "Standard deviation and sample standard deviation will be calculated using the stdev() and sampleStdev() methods, respectively.\n",
    "\n",
    "PySpark provides the stats() method, which can calculate all the previously mentioned quantities in one go.\n",
    "\n",
    "#### How It Works\n",
    "\n",
    "We’ll follow the steps in this section to reach a solution.\n",
    "#### Parallelizing the Data\n",
    "\n",
    "Let’s parallelize the air velocity data from a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "614da47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "airVelocityKMPH = [12,13,15,12,11,12,11]\n",
    "parVelocityKMPH = sc.parallelize(airVelocityKMPH,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c70a706",
   "metadata": {},
   "source": [
    "The parVelocityKMPH variable is an RDD.\n",
    "#### Getting the Number of Data Points\n",
    "\n",
    "The number of data points gives us an idea of the data size. We apply the count() function to get the number of elements in the RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2d7076d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parVelocityKMPH.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e40af9",
   "metadata": {},
   "source": [
    "The total number of data points is seven.\n",
    "#### Summing Air Velocities in a Day\n",
    "\n",
    "Let’s get the summation by using the sum() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2c55f998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parVelocityKMPH.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb467629",
   "metadata": {},
   "source": [
    "Finding the Mean Air Velocity\n",
    "Figure 4-5 shows the mathematical formula for finding a mean, where x1, x2, . . . xn are n data points.\n",
    "\n",
    "<img src = '430628_1_En_4_Fig5_HTML.gif'>\n",
    "\n",
    "#### Calculating the mean\n",
    "\n",
    "We calculate the mean by using the mean() function defined on the RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "194626ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.285714285714286"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parVelocityKMPH.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b25554",
   "metadata": {},
   "source": [
    "##### Finding the Variance of Air Data\n",
    "If we have the data points x1, x2, . . . xn, then Figure 4-6 shows the mathematical formula for calculating variance. We are going to calculate the variance of the given air data by using the variance() function defined on the RDD.\n",
    "\n",
    "<img src = '430628_1_En_4_Fig6_HTML.gif'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "43bf5525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.63265306122449"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parVelocityKMPH.variance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c236883",
   "metadata": {},
   "source": [
    "#### Calculating Sample Variance\n",
    "\n",
    "The variance function calculates the population variance. In order to calculate the sample variance, we have to use sampleVariance() defined on the RDD.\n",
    "For data points x1, x2, . . . xn, the sample standard variance is defined in Figure 4-7.\n",
    "\n",
    "<img src = '430628_1_En_4_Fig7_HTML.gif'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5b928693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.904761904761905"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parVelocityKMPH.sampleVariance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ad559f",
   "metadata": {},
   "source": [
    "#### Calculating Standard Deviation\n",
    "\n",
    "The standard deviation is the square root of the variance value. Let’s calculate the standard deviation by using the stdev() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "01395e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2777531299998799"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parVelocityKMPH.stdev()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6d315f",
   "metadata": {},
   "source": [
    "#### Calculating All Values in One Step\n",
    "\n",
    "We can calculate all the values of the summary statistics in one go by using the stats() function. The StatCounter object is returned from the stats() function. Let’s use the stats() function to calculate the summary statistics of the air velocity data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6d48ca54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count: 7, mean: 12.285714285714286, stdev: 1.2777531299998799, max: 15.0, min: 11.0)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parVelocityKMPH.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "76406f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count': 7,\n",
       " 'mean': 12.285714285714286,\n",
       " 'sum': 86.0,\n",
       " 'min': 11.0,\n",
       " 'max': 15.0,\n",
       " 'stdev': 1.3801311186847085,\n",
       " 'variance': 1.904761904761905}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parVelocityKMPH.stats().asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "466f84b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parVelocityKMPH.stats().asDict()['max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "58c1503a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.285714285714286"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parVelocityKMPH.stats().asDict()['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dbdfae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5888eea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
