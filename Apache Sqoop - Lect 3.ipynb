{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0a1158a",
   "metadata": {},
   "source": [
    "## Export\n",
    "\n",
    "The previous three chapters had one thing in common: they described various use cases of transferring data from a database server to the Hadoop ecosystem. What if you have the opposite scenario and need to transfer generated, processed, or backed-up data from Hadoop to your database? Sqoop also provides facilities for this use case, and the following recipes in this chapter will help you understand how to take advantage of this feature.\n",
    "### Transferring Data from Hadoop\n",
    "### Problem\n",
    "\n",
    "You have a workflow of various Hive and MapReduce jobs that are generating data on a Hadoop cluster. You need to transfer this data to your relational database for easy querying.\n",
    "### Solution\n",
    "\n",
    "You can use Sqoopâ€™s export feature that allows you to transfer data from the Hadoop ecosystem to relational databases. For example, to export data from the export-dir directory cities (the directory in HDFS that contains the source data) into table cities (the table to populate in the database), you would use the following Sqoop command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb028c63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
